{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 5604,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.000535475234270415,
      "grad_norm": 41.014766693115234,
      "learning_rate": 0.0002,
      "loss": 9.769,
      "step": 1
    },
    {
      "epoch": 0.00107095046854083,
      "grad_norm": 29.191694259643555,
      "learning_rate": 0.00019996431120628124,
      "loss": 7.5641,
      "step": 2
    },
    {
      "epoch": 0.001606425702811245,
      "grad_norm": 15.878083229064941,
      "learning_rate": 0.00019992862241256248,
      "loss": 5.9116,
      "step": 3
    },
    {
      "epoch": 0.00214190093708166,
      "grad_norm": 24.302398681640625,
      "learning_rate": 0.00019989293361884368,
      "loss": 5.0629,
      "step": 4
    },
    {
      "epoch": 0.002677376171352075,
      "grad_norm": 22.639938354492188,
      "learning_rate": 0.00019985724482512492,
      "loss": 3.7025,
      "step": 5
    },
    {
      "epoch": 0.00321285140562249,
      "grad_norm": 14.28961181640625,
      "learning_rate": 0.00019982155603140615,
      "loss": 2.7071,
      "step": 6
    },
    {
      "epoch": 0.003748326639892905,
      "grad_norm": 8.319747924804688,
      "learning_rate": 0.00019978586723768738,
      "loss": 2.0545,
      "step": 7
    },
    {
      "epoch": 0.00428380187416332,
      "grad_norm": 9.540061950683594,
      "learning_rate": 0.0001997501784439686,
      "loss": 1.865,
      "step": 8
    },
    {
      "epoch": 0.004819277108433735,
      "grad_norm": 5.169198513031006,
      "learning_rate": 0.00019971448965024985,
      "loss": 1.0391,
      "step": 9
    },
    {
      "epoch": 0.00535475234270415,
      "grad_norm": 14.136367797851562,
      "learning_rate": 0.00019967880085653106,
      "loss": 1.4943,
      "step": 10
    },
    {
      "epoch": 0.005890227576974565,
      "grad_norm": 6.020290851593018,
      "learning_rate": 0.0001996431120628123,
      "loss": 1.082,
      "step": 11
    },
    {
      "epoch": 0.00642570281124498,
      "grad_norm": 9.847452163696289,
      "learning_rate": 0.00019960742326909352,
      "loss": 1.5256,
      "step": 12
    },
    {
      "epoch": 0.0069611780455153946,
      "grad_norm": 6.472780227661133,
      "learning_rate": 0.00019957173447537473,
      "loss": 0.5949,
      "step": 13
    },
    {
      "epoch": 0.00749665327978581,
      "grad_norm": 6.126821041107178,
      "learning_rate": 0.00019953604568165596,
      "loss": 1.0248,
      "step": 14
    },
    {
      "epoch": 0.008032128514056224,
      "grad_norm": 12.872608184814453,
      "learning_rate": 0.0001995003568879372,
      "loss": 0.8789,
      "step": 15
    },
    {
      "epoch": 0.00856760374832664,
      "grad_norm": 2.774639844894409,
      "learning_rate": 0.00019946466809421843,
      "loss": 0.519,
      "step": 16
    },
    {
      "epoch": 0.009103078982597055,
      "grad_norm": 4.934209823608398,
      "learning_rate": 0.00019942897930049966,
      "loss": 1.1991,
      "step": 17
    },
    {
      "epoch": 0.00963855421686747,
      "grad_norm": 5.9274702072143555,
      "learning_rate": 0.0001993932905067809,
      "loss": 1.2986,
      "step": 18
    },
    {
      "epoch": 0.010174029451137885,
      "grad_norm": 7.218353271484375,
      "learning_rate": 0.0001993576017130621,
      "loss": 1.0068,
      "step": 19
    },
    {
      "epoch": 0.0107095046854083,
      "grad_norm": 9.409903526306152,
      "learning_rate": 0.00019932191291934334,
      "loss": 1.1222,
      "step": 20
    },
    {
      "epoch": 0.011244979919678716,
      "grad_norm": 7.220438003540039,
      "learning_rate": 0.00019928622412562457,
      "loss": 1.3307,
      "step": 21
    },
    {
      "epoch": 0.01178045515394913,
      "grad_norm": 4.873899459838867,
      "learning_rate": 0.00019925053533190578,
      "loss": 0.4056,
      "step": 22
    },
    {
      "epoch": 0.012315930388219544,
      "grad_norm": 12.452713012695312,
      "learning_rate": 0.00019921484653818704,
      "loss": 1.8611,
      "step": 23
    },
    {
      "epoch": 0.01285140562248996,
      "grad_norm": 7.382946014404297,
      "learning_rate": 0.00019917915774446824,
      "loss": 1.4794,
      "step": 24
    },
    {
      "epoch": 0.013386880856760375,
      "grad_norm": 4.863272190093994,
      "learning_rate": 0.00019914346895074948,
      "loss": 0.7104,
      "step": 25
    },
    {
      "epoch": 0.013922356091030789,
      "grad_norm": 5.556658744812012,
      "learning_rate": 0.0001991077801570307,
      "loss": 1.096,
      "step": 26
    },
    {
      "epoch": 0.014457831325301205,
      "grad_norm": 6.035890579223633,
      "learning_rate": 0.00019907209136331194,
      "loss": 1.0703,
      "step": 27
    },
    {
      "epoch": 0.01499330655957162,
      "grad_norm": 5.26054573059082,
      "learning_rate": 0.00019903640256959315,
      "loss": 1.0032,
      "step": 28
    },
    {
      "epoch": 0.015528781793842034,
      "grad_norm": 4.624292373657227,
      "learning_rate": 0.00019900071377587438,
      "loss": 0.6286,
      "step": 29
    },
    {
      "epoch": 0.01606425702811245,
      "grad_norm": 6.327462196350098,
      "learning_rate": 0.00019896502498215562,
      "loss": 1.2061,
      "step": 30
    },
    {
      "epoch": 0.016599732262382864,
      "grad_norm": 6.557559490203857,
      "learning_rate": 0.00019892933618843682,
      "loss": 0.9512,
      "step": 31
    },
    {
      "epoch": 0.01713520749665328,
      "grad_norm": 8.886990547180176,
      "learning_rate": 0.00019889364739471808,
      "loss": 0.7686,
      "step": 32
    },
    {
      "epoch": 0.017670682730923693,
      "grad_norm": 8.525328636169434,
      "learning_rate": 0.0001988579586009993,
      "loss": 0.766,
      "step": 33
    },
    {
      "epoch": 0.01820615796519411,
      "grad_norm": 11.374279975891113,
      "learning_rate": 0.00019882226980728052,
      "loss": 0.8105,
      "step": 34
    },
    {
      "epoch": 0.018741633199464525,
      "grad_norm": 4.328817367553711,
      "learning_rate": 0.00019878658101356176,
      "loss": 0.415,
      "step": 35
    },
    {
      "epoch": 0.01927710843373494,
      "grad_norm": 10.283225059509277,
      "learning_rate": 0.000198750892219843,
      "loss": 0.8077,
      "step": 36
    },
    {
      "epoch": 0.019812583668005354,
      "grad_norm": 18.626832962036133,
      "learning_rate": 0.0001987152034261242,
      "loss": 0.693,
      "step": 37
    },
    {
      "epoch": 0.02034805890227577,
      "grad_norm": 10.726914405822754,
      "learning_rate": 0.00019867951463240543,
      "loss": 0.7839,
      "step": 38
    },
    {
      "epoch": 0.020883534136546186,
      "grad_norm": 8.109128952026367,
      "learning_rate": 0.00019864382583868666,
      "loss": 0.7596,
      "step": 39
    },
    {
      "epoch": 0.0214190093708166,
      "grad_norm": 6.792672157287598,
      "learning_rate": 0.00019860813704496787,
      "loss": 0.6953,
      "step": 40
    },
    {
      "epoch": 0.021954484605087015,
      "grad_norm": 4.5836968421936035,
      "learning_rate": 0.00019857244825124913,
      "loss": 0.4418,
      "step": 41
    },
    {
      "epoch": 0.02248995983935743,
      "grad_norm": 4.2762370109558105,
      "learning_rate": 0.00019853675945753034,
      "loss": 0.4827,
      "step": 42
    },
    {
      "epoch": 0.023025435073627844,
      "grad_norm": 4.7621331214904785,
      "learning_rate": 0.0001985010706638116,
      "loss": 0.5325,
      "step": 43
    },
    {
      "epoch": 0.02356091030789826,
      "grad_norm": 4.56088399887085,
      "learning_rate": 0.0001984653818700928,
      "loss": 0.6165,
      "step": 44
    },
    {
      "epoch": 0.024096385542168676,
      "grad_norm": 7.268270015716553,
      "learning_rate": 0.00019842969307637404,
      "loss": 0.7092,
      "step": 45
    },
    {
      "epoch": 0.02463186077643909,
      "grad_norm": 4.735079765319824,
      "learning_rate": 0.00019839400428265527,
      "loss": 0.6406,
      "step": 46
    },
    {
      "epoch": 0.025167336010709505,
      "grad_norm": 7.373002052307129,
      "learning_rate": 0.00019835831548893648,
      "loss": 0.4371,
      "step": 47
    },
    {
      "epoch": 0.02570281124497992,
      "grad_norm": 7.779286861419678,
      "learning_rate": 0.0001983226266952177,
      "loss": 0.5716,
      "step": 48
    },
    {
      "epoch": 0.026238286479250333,
      "grad_norm": 8.247961044311523,
      "learning_rate": 0.00019828693790149894,
      "loss": 0.447,
      "step": 49
    },
    {
      "epoch": 0.02677376171352075,
      "grad_norm": 8.464545249938965,
      "learning_rate": 0.00019825124910778018,
      "loss": 0.4127,
      "step": 50
    },
    {
      "epoch": 0.027309236947791166,
      "grad_norm": 12.169447898864746,
      "learning_rate": 0.00019821556031406138,
      "loss": 0.7086,
      "step": 51
    },
    {
      "epoch": 0.027844712182061578,
      "grad_norm": 5.455084800720215,
      "learning_rate": 0.00019817987152034264,
      "loss": 0.4819,
      "step": 52
    },
    {
      "epoch": 0.028380187416331994,
      "grad_norm": 4.1580400466918945,
      "learning_rate": 0.00019814418272662385,
      "loss": 0.387,
      "step": 53
    },
    {
      "epoch": 0.02891566265060241,
      "grad_norm": 1.4898654222488403,
      "learning_rate": 0.00019810849393290508,
      "loss": 0.3089,
      "step": 54
    },
    {
      "epoch": 0.029451137884872823,
      "grad_norm": 9.771139144897461,
      "learning_rate": 0.00019807280513918632,
      "loss": 0.5721,
      "step": 55
    },
    {
      "epoch": 0.02998661311914324,
      "grad_norm": 3.060391902923584,
      "learning_rate": 0.00019803711634546752,
      "loss": 0.3559,
      "step": 56
    },
    {
      "epoch": 0.030522088353413655,
      "grad_norm": 7.714673042297363,
      "learning_rate": 0.00019800142755174876,
      "loss": 0.5563,
      "step": 57
    },
    {
      "epoch": 0.031057563587684068,
      "grad_norm": 2.3893511295318604,
      "learning_rate": 0.00019796573875803,
      "loss": 0.3087,
      "step": 58
    },
    {
      "epoch": 0.03159303882195449,
      "grad_norm": 7.077948093414307,
      "learning_rate": 0.00019793004996431122,
      "loss": 0.6448,
      "step": 59
    },
    {
      "epoch": 0.0321285140562249,
      "grad_norm": 9.477276802062988,
      "learning_rate": 0.00019789436117059243,
      "loss": 0.7696,
      "step": 60
    },
    {
      "epoch": 0.03266398929049531,
      "grad_norm": 5.607622146606445,
      "learning_rate": 0.0001978586723768737,
      "loss": 0.5216,
      "step": 61
    },
    {
      "epoch": 0.03319946452476573,
      "grad_norm": 4.218380451202393,
      "learning_rate": 0.0001978229835831549,
      "loss": 0.2959,
      "step": 62
    },
    {
      "epoch": 0.033734939759036145,
      "grad_norm": 3.2535486221313477,
      "learning_rate": 0.00019778729478943613,
      "loss": 0.3449,
      "step": 63
    },
    {
      "epoch": 0.03427041499330656,
      "grad_norm": 4.321146488189697,
      "learning_rate": 0.00019775160599571736,
      "loss": 0.3955,
      "step": 64
    },
    {
      "epoch": 0.03480589022757698,
      "grad_norm": 5.134836673736572,
      "learning_rate": 0.00019771591720199857,
      "loss": 0.5678,
      "step": 65
    },
    {
      "epoch": 0.035341365461847386,
      "grad_norm": 5.620365142822266,
      "learning_rate": 0.0001976802284082798,
      "loss": 0.502,
      "step": 66
    },
    {
      "epoch": 0.0358768406961178,
      "grad_norm": 4.194943428039551,
      "learning_rate": 0.00019764453961456104,
      "loss": 0.5581,
      "step": 67
    },
    {
      "epoch": 0.03641231593038822,
      "grad_norm": 7.057179927825928,
      "learning_rate": 0.00019760885082084227,
      "loss": 0.6225,
      "step": 68
    },
    {
      "epoch": 0.036947791164658635,
      "grad_norm": 4.510729789733887,
      "learning_rate": 0.00019757316202712348,
      "loss": 0.4285,
      "step": 69
    },
    {
      "epoch": 0.03748326639892905,
      "grad_norm": 3.404266119003296,
      "learning_rate": 0.00019753747323340474,
      "loss": 0.3629,
      "step": 70
    },
    {
      "epoch": 0.03801874163319947,
      "grad_norm": 3.1404829025268555,
      "learning_rate": 0.00019750178443968594,
      "loss": 0.3579,
      "step": 71
    },
    {
      "epoch": 0.03855421686746988,
      "grad_norm": 4.9601545333862305,
      "learning_rate": 0.00019746609564596718,
      "loss": 0.3415,
      "step": 72
    },
    {
      "epoch": 0.03908969210174029,
      "grad_norm": 2.6959147453308105,
      "learning_rate": 0.0001974304068522484,
      "loss": 0.3183,
      "step": 73
    },
    {
      "epoch": 0.03962516733601071,
      "grad_norm": 6.707366943359375,
      "learning_rate": 0.00019739471805852962,
      "loss": 0.4565,
      "step": 74
    },
    {
      "epoch": 0.040160642570281124,
      "grad_norm": 5.609363079071045,
      "learning_rate": 0.00019735902926481088,
      "loss": 0.4842,
      "step": 75
    },
    {
      "epoch": 0.04069611780455154,
      "grad_norm": 3.7423200607299805,
      "learning_rate": 0.00019732334047109208,
      "loss": 0.4114,
      "step": 76
    },
    {
      "epoch": 0.041231593038821956,
      "grad_norm": 2.941751003265381,
      "learning_rate": 0.00019728765167737332,
      "loss": 0.317,
      "step": 77
    },
    {
      "epoch": 0.04176706827309237,
      "grad_norm": 2.4338932037353516,
      "learning_rate": 0.00019725196288365455,
      "loss": 0.2962,
      "step": 78
    },
    {
      "epoch": 0.04230254350736278,
      "grad_norm": 7.716624736785889,
      "learning_rate": 0.00019721627408993578,
      "loss": 0.542,
      "step": 79
    },
    {
      "epoch": 0.0428380187416332,
      "grad_norm": 9.37764835357666,
      "learning_rate": 0.000197180585296217,
      "loss": 0.4262,
      "step": 80
    },
    {
      "epoch": 0.043373493975903614,
      "grad_norm": 4.917050838470459,
      "learning_rate": 0.00019714489650249822,
      "loss": 0.4129,
      "step": 81
    },
    {
      "epoch": 0.04390896921017403,
      "grad_norm": 4.4777607917785645,
      "learning_rate": 0.00019710920770877946,
      "loss": 0.4432,
      "step": 82
    },
    {
      "epoch": 0.044444444444444446,
      "grad_norm": 3.51633358001709,
      "learning_rate": 0.00019707351891506066,
      "loss": 0.3112,
      "step": 83
    },
    {
      "epoch": 0.04497991967871486,
      "grad_norm": 5.671650409698486,
      "learning_rate": 0.00019703783012134192,
      "loss": 0.4162,
      "step": 84
    },
    {
      "epoch": 0.04551539491298527,
      "grad_norm": 5.7699055671691895,
      "learning_rate": 0.00019700214132762313,
      "loss": 0.4868,
      "step": 85
    },
    {
      "epoch": 0.04605087014725569,
      "grad_norm": 2.287041425704956,
      "learning_rate": 0.00019696645253390436,
      "loss": 0.3536,
      "step": 86
    },
    {
      "epoch": 0.046586345381526104,
      "grad_norm": 5.853799343109131,
      "learning_rate": 0.0001969307637401856,
      "loss": 0.4645,
      "step": 87
    },
    {
      "epoch": 0.04712182061579652,
      "grad_norm": 2.937577247619629,
      "learning_rate": 0.00019689507494646683,
      "loss": 0.4507,
      "step": 88
    },
    {
      "epoch": 0.047657295850066936,
      "grad_norm": 6.15720796585083,
      "learning_rate": 0.00019685938615274804,
      "loss": 0.6591,
      "step": 89
    },
    {
      "epoch": 0.04819277108433735,
      "grad_norm": 2.5189049243927,
      "learning_rate": 0.00019682369735902927,
      "loss": 0.2817,
      "step": 90
    },
    {
      "epoch": 0.04872824631860776,
      "grad_norm": 6.165538311004639,
      "learning_rate": 0.0001967880085653105,
      "loss": 0.3766,
      "step": 91
    },
    {
      "epoch": 0.04926372155287818,
      "grad_norm": 4.723466873168945,
      "learning_rate": 0.0001967523197715917,
      "loss": 0.5276,
      "step": 92
    },
    {
      "epoch": 0.04979919678714859,
      "grad_norm": 4.5985798835754395,
      "learning_rate": 0.00019671663097787297,
      "loss": 0.3238,
      "step": 93
    },
    {
      "epoch": 0.05033467202141901,
      "grad_norm": 5.399503707885742,
      "learning_rate": 0.00019668094218415418,
      "loss": 0.3493,
      "step": 94
    },
    {
      "epoch": 0.050870147255689425,
      "grad_norm": 2.825178623199463,
      "learning_rate": 0.0001966452533904354,
      "loss": 0.3224,
      "step": 95
    },
    {
      "epoch": 0.05140562248995984,
      "grad_norm": 2.5911777019500732,
      "learning_rate": 0.00019660956459671664,
      "loss": 0.2568,
      "step": 96
    },
    {
      "epoch": 0.05194109772423026,
      "grad_norm": 9.220154762268066,
      "learning_rate": 0.00019657387580299788,
      "loss": 0.3981,
      "step": 97
    },
    {
      "epoch": 0.05247657295850067,
      "grad_norm": 6.6621599197387695,
      "learning_rate": 0.0001965381870092791,
      "loss": 0.3386,
      "step": 98
    },
    {
      "epoch": 0.05301204819277108,
      "grad_norm": 2.8694424629211426,
      "learning_rate": 0.00019650249821556032,
      "loss": 0.2129,
      "step": 99
    },
    {
      "epoch": 0.0535475234270415,
      "grad_norm": 3.3722915649414062,
      "learning_rate": 0.00019646680942184155,
      "loss": 0.3392,
      "step": 100
    },
    {
      "epoch": 0.054082998661311915,
      "grad_norm": 4.9211649894714355,
      "learning_rate": 0.00019643112062812278,
      "loss": 0.4771,
      "step": 101
    },
    {
      "epoch": 0.05461847389558233,
      "grad_norm": 4.31766414642334,
      "learning_rate": 0.00019639543183440402,
      "loss": 0.2901,
      "step": 102
    },
    {
      "epoch": 0.05515394912985275,
      "grad_norm": 2.140009880065918,
      "learning_rate": 0.00019635974304068522,
      "loss": 0.3773,
      "step": 103
    },
    {
      "epoch": 0.055689424364123156,
      "grad_norm": 4.365271091461182,
      "learning_rate": 0.00019632405424696648,
      "loss": 0.2676,
      "step": 104
    },
    {
      "epoch": 0.05622489959839357,
      "grad_norm": 1.5424567461013794,
      "learning_rate": 0.0001962883654532477,
      "loss": 0.2271,
      "step": 105
    },
    {
      "epoch": 0.05676037483266399,
      "grad_norm": 4.114264965057373,
      "learning_rate": 0.00019625267665952892,
      "loss": 0.4412,
      "step": 106
    },
    {
      "epoch": 0.057295850066934405,
      "grad_norm": 2.6915442943573,
      "learning_rate": 0.00019621698786581016,
      "loss": 0.2385,
      "step": 107
    },
    {
      "epoch": 0.05783132530120482,
      "grad_norm": 1.578979253768921,
      "learning_rate": 0.00019618129907209136,
      "loss": 0.2307,
      "step": 108
    },
    {
      "epoch": 0.05836680053547524,
      "grad_norm": 5.592299938201904,
      "learning_rate": 0.0001961456102783726,
      "loss": 0.3545,
      "step": 109
    },
    {
      "epoch": 0.058902275769745646,
      "grad_norm": 10.184500694274902,
      "learning_rate": 0.00019610992148465383,
      "loss": 0.4615,
      "step": 110
    },
    {
      "epoch": 0.05943775100401606,
      "grad_norm": 4.4676642417907715,
      "learning_rate": 0.00019607423269093506,
      "loss": 0.3513,
      "step": 111
    },
    {
      "epoch": 0.05997322623828648,
      "grad_norm": 9.232636451721191,
      "learning_rate": 0.00019603854389721627,
      "loss": 0.3472,
      "step": 112
    },
    {
      "epoch": 0.060508701472556894,
      "grad_norm": 5.639286994934082,
      "learning_rate": 0.00019600285510349753,
      "loss": 0.3377,
      "step": 113
    },
    {
      "epoch": 0.06104417670682731,
      "grad_norm": 4.567000389099121,
      "learning_rate": 0.00019596716630977874,
      "loss": 0.2222,
      "step": 114
    },
    {
      "epoch": 0.06157965194109773,
      "grad_norm": 6.196760654449463,
      "learning_rate": 0.00019593147751605997,
      "loss": 0.2908,
      "step": 115
    },
    {
      "epoch": 0.062115127175368136,
      "grad_norm": 6.6702961921691895,
      "learning_rate": 0.0001958957887223412,
      "loss": 0.2462,
      "step": 116
    },
    {
      "epoch": 0.06265060240963856,
      "grad_norm": 7.649096965789795,
      "learning_rate": 0.0001958600999286224,
      "loss": 0.411,
      "step": 117
    },
    {
      "epoch": 0.06318607764390897,
      "grad_norm": 2.6001439094543457,
      "learning_rate": 0.00019582441113490364,
      "loss": 0.2823,
      "step": 118
    },
    {
      "epoch": 0.06372155287817939,
      "grad_norm": 1.7715744972229004,
      "learning_rate": 0.00019578872234118488,
      "loss": 0.2066,
      "step": 119
    },
    {
      "epoch": 0.0642570281124498,
      "grad_norm": 1.1546059846878052,
      "learning_rate": 0.0001957530335474661,
      "loss": 0.1829,
      "step": 120
    },
    {
      "epoch": 0.06479250334672021,
      "grad_norm": 4.281704425811768,
      "learning_rate": 0.00019571734475374732,
      "loss": 0.3919,
      "step": 121
    },
    {
      "epoch": 0.06532797858099063,
      "grad_norm": 9.614389419555664,
      "learning_rate": 0.00019568165596002858,
      "loss": 0.3633,
      "step": 122
    },
    {
      "epoch": 0.06586345381526104,
      "grad_norm": 2.402965784072876,
      "learning_rate": 0.00019564596716630978,
      "loss": 0.252,
      "step": 123
    },
    {
      "epoch": 0.06639892904953146,
      "grad_norm": 4.525031566619873,
      "learning_rate": 0.00019561027837259102,
      "loss": 0.3228,
      "step": 124
    },
    {
      "epoch": 0.06693440428380187,
      "grad_norm": 4.979862689971924,
      "learning_rate": 0.00019557458957887225,
      "loss": 0.243,
      "step": 125
    },
    {
      "epoch": 0.06746987951807229,
      "grad_norm": 1.6918869018554688,
      "learning_rate": 0.00019553890078515346,
      "loss": 0.2135,
      "step": 126
    },
    {
      "epoch": 0.0680053547523427,
      "grad_norm": 4.272831439971924,
      "learning_rate": 0.00019550321199143472,
      "loss": 0.2824,
      "step": 127
    },
    {
      "epoch": 0.06854082998661312,
      "grad_norm": 5.5919342041015625,
      "learning_rate": 0.00019546752319771592,
      "loss": 0.3358,
      "step": 128
    },
    {
      "epoch": 0.06907630522088354,
      "grad_norm": 1.7629406452178955,
      "learning_rate": 0.00019543183440399716,
      "loss": 0.3473,
      "step": 129
    },
    {
      "epoch": 0.06961178045515395,
      "grad_norm": 4.253555774688721,
      "learning_rate": 0.0001953961456102784,
      "loss": 0.322,
      "step": 130
    },
    {
      "epoch": 0.07014725568942437,
      "grad_norm": 1.5007517337799072,
      "learning_rate": 0.00019536045681655962,
      "loss": 0.21,
      "step": 131
    },
    {
      "epoch": 0.07068273092369477,
      "grad_norm": 2.138932943344116,
      "learning_rate": 0.00019532476802284083,
      "loss": 0.2793,
      "step": 132
    },
    {
      "epoch": 0.07121820615796519,
      "grad_norm": 3.0338151454925537,
      "learning_rate": 0.00019528907922912206,
      "loss": 0.2482,
      "step": 133
    },
    {
      "epoch": 0.0717536813922356,
      "grad_norm": 3.587989330291748,
      "learning_rate": 0.0001952533904354033,
      "loss": 0.2765,
      "step": 134
    },
    {
      "epoch": 0.07228915662650602,
      "grad_norm": 1.9889649152755737,
      "learning_rate": 0.0001952177016416845,
      "loss": 0.2521,
      "step": 135
    },
    {
      "epoch": 0.07282463186077644,
      "grad_norm": 1.2893383502960205,
      "learning_rate": 0.00019518201284796576,
      "loss": 0.1226,
      "step": 136
    },
    {
      "epoch": 0.07336010709504685,
      "grad_norm": 2.8950729370117188,
      "learning_rate": 0.00019514632405424697,
      "loss": 0.3223,
      "step": 137
    },
    {
      "epoch": 0.07389558232931727,
      "grad_norm": 4.129842281341553,
      "learning_rate": 0.0001951106352605282,
      "loss": 0.3253,
      "step": 138
    },
    {
      "epoch": 0.07443105756358769,
      "grad_norm": 6.562358856201172,
      "learning_rate": 0.00019507494646680944,
      "loss": 0.452,
      "step": 139
    },
    {
      "epoch": 0.0749665327978581,
      "grad_norm": 7.1009016036987305,
      "learning_rate": 0.00019503925767309067,
      "loss": 0.3808,
      "step": 140
    },
    {
      "epoch": 0.07550200803212852,
      "grad_norm": 5.078736305236816,
      "learning_rate": 0.00019500356887937188,
      "loss": 0.2281,
      "step": 141
    },
    {
      "epoch": 0.07603748326639893,
      "grad_norm": 3.7792906761169434,
      "learning_rate": 0.0001949678800856531,
      "loss": 0.3436,
      "step": 142
    },
    {
      "epoch": 0.07657295850066935,
      "grad_norm": 1.8976701498031616,
      "learning_rate": 0.00019493219129193434,
      "loss": 0.2248,
      "step": 143
    },
    {
      "epoch": 0.07710843373493977,
      "grad_norm": 0.9437307119369507,
      "learning_rate": 0.00019489650249821555,
      "loss": 0.1229,
      "step": 144
    },
    {
      "epoch": 0.07764390896921017,
      "grad_norm": 2.0653891563415527,
      "learning_rate": 0.0001948608137044968,
      "loss": 0.1654,
      "step": 145
    },
    {
      "epoch": 0.07817938420348058,
      "grad_norm": 10.886982917785645,
      "learning_rate": 0.00019482512491077802,
      "loss": 0.2475,
      "step": 146
    },
    {
      "epoch": 0.078714859437751,
      "grad_norm": 1.3229080438613892,
      "learning_rate": 0.00019478943611705925,
      "loss": 0.2468,
      "step": 147
    },
    {
      "epoch": 0.07925033467202142,
      "grad_norm": 5.237671852111816,
      "learning_rate": 0.00019475374732334048,
      "loss": 0.3523,
      "step": 148
    },
    {
      "epoch": 0.07978580990629183,
      "grad_norm": 1.1189746856689453,
      "learning_rate": 0.00019471805852962172,
      "loss": 0.1989,
      "step": 149
    },
    {
      "epoch": 0.08032128514056225,
      "grad_norm": 4.433987617492676,
      "learning_rate": 0.00019468236973590292,
      "loss": 0.4884,
      "step": 150
    },
    {
      "epoch": 0.08085676037483266,
      "grad_norm": 1.5742326974868774,
      "learning_rate": 0.00019464668094218416,
      "loss": 0.231,
      "step": 151
    },
    {
      "epoch": 0.08139223560910308,
      "grad_norm": 9.757259368896484,
      "learning_rate": 0.0001946109921484654,
      "loss": 0.3113,
      "step": 152
    },
    {
      "epoch": 0.0819277108433735,
      "grad_norm": 5.452183246612549,
      "learning_rate": 0.00019457530335474662,
      "loss": 0.3057,
      "step": 153
    },
    {
      "epoch": 0.08246318607764391,
      "grad_norm": 7.035062313079834,
      "learning_rate": 0.00019453961456102786,
      "loss": 0.3151,
      "step": 154
    },
    {
      "epoch": 0.08299866131191433,
      "grad_norm": 3.143690347671509,
      "learning_rate": 0.00019450392576730906,
      "loss": 0.3058,
      "step": 155
    },
    {
      "epoch": 0.08353413654618475,
      "grad_norm": 4.456253528594971,
      "learning_rate": 0.00019446823697359032,
      "loss": 0.216,
      "step": 156
    },
    {
      "epoch": 0.08406961178045515,
      "grad_norm": 12.076129913330078,
      "learning_rate": 0.00019443254817987153,
      "loss": 0.364,
      "step": 157
    },
    {
      "epoch": 0.08460508701472556,
      "grad_norm": 9.51858901977539,
      "learning_rate": 0.00019439685938615276,
      "loss": 0.2817,
      "step": 158
    },
    {
      "epoch": 0.08514056224899598,
      "grad_norm": 4.808204174041748,
      "learning_rate": 0.000194361170592434,
      "loss": 0.4007,
      "step": 159
    },
    {
      "epoch": 0.0856760374832664,
      "grad_norm": 2.557490825653076,
      "learning_rate": 0.0001943254817987152,
      "loss": 0.2852,
      "step": 160
    },
    {
      "epoch": 0.08621151271753681,
      "grad_norm": 1.6091134548187256,
      "learning_rate": 0.00019428979300499644,
      "loss": 0.3049,
      "step": 161
    },
    {
      "epoch": 0.08674698795180723,
      "grad_norm": 2.6033008098602295,
      "learning_rate": 0.00019425410421127767,
      "loss": 0.3669,
      "step": 162
    },
    {
      "epoch": 0.08728246318607764,
      "grad_norm": 1.22564697265625,
      "learning_rate": 0.0001942184154175589,
      "loss": 0.3016,
      "step": 163
    },
    {
      "epoch": 0.08781793842034806,
      "grad_norm": 2.3492391109466553,
      "learning_rate": 0.0001941827266238401,
      "loss": 0.508,
      "step": 164
    },
    {
      "epoch": 0.08835341365461848,
      "grad_norm": 4.007726669311523,
      "learning_rate": 0.00019414703783012137,
      "loss": 0.231,
      "step": 165
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 12.068930625915527,
      "learning_rate": 0.00019411134903640258,
      "loss": 0.2568,
      "step": 166
    },
    {
      "epoch": 0.08942436412315931,
      "grad_norm": 1.6324796676635742,
      "learning_rate": 0.0001940756602426838,
      "loss": 0.2905,
      "step": 167
    },
    {
      "epoch": 0.08995983935742972,
      "grad_norm": 3.4083540439605713,
      "learning_rate": 0.00019403997144896504,
      "loss": 0.3205,
      "step": 168
    },
    {
      "epoch": 0.09049531459170014,
      "grad_norm": 5.345333576202393,
      "learning_rate": 0.00019400428265524625,
      "loss": 0.4216,
      "step": 169
    },
    {
      "epoch": 0.09103078982597054,
      "grad_norm": 3.494720220565796,
      "learning_rate": 0.00019396859386152748,
      "loss": 0.3232,
      "step": 170
    },
    {
      "epoch": 0.09156626506024096,
      "grad_norm": 1.783995509147644,
      "learning_rate": 0.00019393290506780872,
      "loss": 0.2623,
      "step": 171
    },
    {
      "epoch": 0.09210174029451138,
      "grad_norm": 1.778238296508789,
      "learning_rate": 0.00019389721627408995,
      "loss": 0.2666,
      "step": 172
    },
    {
      "epoch": 0.09263721552878179,
      "grad_norm": 1.8145310878753662,
      "learning_rate": 0.00019386152748037116,
      "loss": 0.3276,
      "step": 173
    },
    {
      "epoch": 0.09317269076305221,
      "grad_norm": 4.002242088317871,
      "learning_rate": 0.00019382583868665242,
      "loss": 0.3367,
      "step": 174
    },
    {
      "epoch": 0.09370816599732262,
      "grad_norm": 3.2145535945892334,
      "learning_rate": 0.00019379014989293362,
      "loss": 0.2314,
      "step": 175
    },
    {
      "epoch": 0.09424364123159304,
      "grad_norm": 2.125304937362671,
      "learning_rate": 0.00019375446109921486,
      "loss": 0.3606,
      "step": 176
    },
    {
      "epoch": 0.09477911646586346,
      "grad_norm": 1.0651179552078247,
      "learning_rate": 0.0001937187723054961,
      "loss": 0.1474,
      "step": 177
    },
    {
      "epoch": 0.09531459170013387,
      "grad_norm": 1.9410158395767212,
      "learning_rate": 0.0001936830835117773,
      "loss": 0.2274,
      "step": 178
    },
    {
      "epoch": 0.09585006693440429,
      "grad_norm": 2.9579174518585205,
      "learning_rate": 0.00019364739471805853,
      "loss": 0.3442,
      "step": 179
    },
    {
      "epoch": 0.0963855421686747,
      "grad_norm": 3.5311245918273926,
      "learning_rate": 0.00019361170592433976,
      "loss": 0.2977,
      "step": 180
    },
    {
      "epoch": 0.09692101740294512,
      "grad_norm": 3.0928170680999756,
      "learning_rate": 0.000193576017130621,
      "loss": 0.281,
      "step": 181
    },
    {
      "epoch": 0.09745649263721552,
      "grad_norm": 3.5257391929626465,
      "learning_rate": 0.00019354032833690223,
      "loss": 0.3047,
      "step": 182
    },
    {
      "epoch": 0.09799196787148594,
      "grad_norm": 9.196916580200195,
      "learning_rate": 0.00019350463954318346,
      "loss": 0.3845,
      "step": 183
    },
    {
      "epoch": 0.09852744310575635,
      "grad_norm": 1.2250616550445557,
      "learning_rate": 0.00019346895074946467,
      "loss": 0.2515,
      "step": 184
    },
    {
      "epoch": 0.09906291834002677,
      "grad_norm": 2.3121824264526367,
      "learning_rate": 0.0001934332619557459,
      "loss": 0.2616,
      "step": 185
    },
    {
      "epoch": 0.09959839357429719,
      "grad_norm": 4.622385501861572,
      "learning_rate": 0.00019339757316202714,
      "loss": 0.2918,
      "step": 186
    },
    {
      "epoch": 0.1001338688085676,
      "grad_norm": 5.416139602661133,
      "learning_rate": 0.00019336188436830834,
      "loss": 0.4518,
      "step": 187
    },
    {
      "epoch": 0.10066934404283802,
      "grad_norm": 2.599371910095215,
      "learning_rate": 0.0001933261955745896,
      "loss": 0.1963,
      "step": 188
    },
    {
      "epoch": 0.10120481927710843,
      "grad_norm": 3.7934582233428955,
      "learning_rate": 0.0001932905067808708,
      "loss": 0.2776,
      "step": 189
    },
    {
      "epoch": 0.10174029451137885,
      "grad_norm": 3.2648823261260986,
      "learning_rate": 0.00019325481798715204,
      "loss": 0.2501,
      "step": 190
    },
    {
      "epoch": 0.10227576974564927,
      "grad_norm": 1.735913872718811,
      "learning_rate": 0.00019321912919343328,
      "loss": 0.2079,
      "step": 191
    },
    {
      "epoch": 0.10281124497991968,
      "grad_norm": 2.4986331462860107,
      "learning_rate": 0.0001931834403997145,
      "loss": 0.2313,
      "step": 192
    },
    {
      "epoch": 0.1033467202141901,
      "grad_norm": 1.7219796180725098,
      "learning_rate": 0.00019314775160599572,
      "loss": 0.1942,
      "step": 193
    },
    {
      "epoch": 0.10388219544846052,
      "grad_norm": 2.912105083465576,
      "learning_rate": 0.00019311206281227695,
      "loss": 0.2409,
      "step": 194
    },
    {
      "epoch": 0.10441767068273092,
      "grad_norm": 3.718559980392456,
      "learning_rate": 0.00019307637401855818,
      "loss": 0.3381,
      "step": 195
    },
    {
      "epoch": 0.10495314591700133,
      "grad_norm": 4.826365947723389,
      "learning_rate": 0.0001930406852248394,
      "loss": 0.2471,
      "step": 196
    },
    {
      "epoch": 0.10548862115127175,
      "grad_norm": 4.60309362411499,
      "learning_rate": 0.00019300499643112065,
      "loss": 0.1856,
      "step": 197
    },
    {
      "epoch": 0.10602409638554217,
      "grad_norm": 2.7223405838012695,
      "learning_rate": 0.00019296930763740186,
      "loss": 0.1946,
      "step": 198
    },
    {
      "epoch": 0.10655957161981258,
      "grad_norm": 5.98838996887207,
      "learning_rate": 0.0001929336188436831,
      "loss": 0.3984,
      "step": 199
    },
    {
      "epoch": 0.107095046854083,
      "grad_norm": 3.2332754135131836,
      "learning_rate": 0.00019289793004996432,
      "loss": 0.1561,
      "step": 200
    },
    {
      "epoch": 0.10763052208835341,
      "grad_norm": 3.2337565422058105,
      "learning_rate": 0.00019286224125624556,
      "loss": 0.2202,
      "step": 201
    },
    {
      "epoch": 0.10816599732262383,
      "grad_norm": 6.053712368011475,
      "learning_rate": 0.00019282655246252676,
      "loss": 0.3006,
      "step": 202
    },
    {
      "epoch": 0.10870147255689425,
      "grad_norm": 12.185135841369629,
      "learning_rate": 0.000192790863668808,
      "loss": 0.3712,
      "step": 203
    },
    {
      "epoch": 0.10923694779116466,
      "grad_norm": 5.56942081451416,
      "learning_rate": 0.00019275517487508923,
      "loss": 0.253,
      "step": 204
    },
    {
      "epoch": 0.10977242302543508,
      "grad_norm": 4.783580780029297,
      "learning_rate": 0.00019271948608137044,
      "loss": 0.3643,
      "step": 205
    },
    {
      "epoch": 0.1103078982597055,
      "grad_norm": 4.8429765701293945,
      "learning_rate": 0.0001926837972876517,
      "loss": 0.1569,
      "step": 206
    },
    {
      "epoch": 0.1108433734939759,
      "grad_norm": 2.041285276412964,
      "learning_rate": 0.0001926481084939329,
      "loss": 0.2319,
      "step": 207
    },
    {
      "epoch": 0.11137884872824631,
      "grad_norm": 1.5888172388076782,
      "learning_rate": 0.00019261241970021416,
      "loss": 0.1577,
      "step": 208
    },
    {
      "epoch": 0.11191432396251673,
      "grad_norm": 1.8198435306549072,
      "learning_rate": 0.00019257673090649537,
      "loss": 0.2279,
      "step": 209
    },
    {
      "epoch": 0.11244979919678715,
      "grad_norm": 5.2356109619140625,
      "learning_rate": 0.0001925410421127766,
      "loss": 0.2763,
      "step": 210
    },
    {
      "epoch": 0.11298527443105756,
      "grad_norm": 2.5382142066955566,
      "learning_rate": 0.00019250535331905784,
      "loss": 0.1924,
      "step": 211
    },
    {
      "epoch": 0.11352074966532798,
      "grad_norm": 3.290498733520508,
      "learning_rate": 0.00019246966452533904,
      "loss": 0.2152,
      "step": 212
    },
    {
      "epoch": 0.1140562248995984,
      "grad_norm": 1.7060770988464355,
      "learning_rate": 0.00019243397573162028,
      "loss": 0.1897,
      "step": 213
    },
    {
      "epoch": 0.11459170013386881,
      "grad_norm": 7.788384437561035,
      "learning_rate": 0.0001923982869379015,
      "loss": 0.2866,
      "step": 214
    },
    {
      "epoch": 0.11512717536813923,
      "grad_norm": 3.6939849853515625,
      "learning_rate": 0.00019236259814418274,
      "loss": 0.2898,
      "step": 215
    },
    {
      "epoch": 0.11566265060240964,
      "grad_norm": 16.86616325378418,
      "learning_rate": 0.00019232690935046395,
      "loss": 0.2298,
      "step": 216
    },
    {
      "epoch": 0.11619812583668006,
      "grad_norm": 1.6968737840652466,
      "learning_rate": 0.0001922912205567452,
      "loss": 0.1432,
      "step": 217
    },
    {
      "epoch": 0.11673360107095047,
      "grad_norm": 4.669735431671143,
      "learning_rate": 0.00019225553176302642,
      "loss": 0.1688,
      "step": 218
    },
    {
      "epoch": 0.11726907630522089,
      "grad_norm": 5.99084997177124,
      "learning_rate": 0.00019221984296930765,
      "loss": 0.2671,
      "step": 219
    },
    {
      "epoch": 0.11780455153949129,
      "grad_norm": 2.9241585731506348,
      "learning_rate": 0.00019218415417558888,
      "loss": 0.2802,
      "step": 220
    },
    {
      "epoch": 0.11834002677376171,
      "grad_norm": 5.656371116638184,
      "learning_rate": 0.0001921484653818701,
      "loss": 0.2301,
      "step": 221
    },
    {
      "epoch": 0.11887550200803212,
      "grad_norm": 3.5799553394317627,
      "learning_rate": 0.00019211277658815132,
      "loss": 0.2026,
      "step": 222
    },
    {
      "epoch": 0.11941097724230254,
      "grad_norm": 3.0222747325897217,
      "learning_rate": 0.00019207708779443256,
      "loss": 0.2917,
      "step": 223
    },
    {
      "epoch": 0.11994645247657296,
      "grad_norm": 10.181798934936523,
      "learning_rate": 0.0001920413990007138,
      "loss": 0.2439,
      "step": 224
    },
    {
      "epoch": 0.12048192771084337,
      "grad_norm": 4.252748489379883,
      "learning_rate": 0.000192005710206995,
      "loss": 0.2974,
      "step": 225
    },
    {
      "epoch": 0.12101740294511379,
      "grad_norm": 2.9725828170776367,
      "learning_rate": 0.00019197002141327626,
      "loss": 0.2697,
      "step": 226
    },
    {
      "epoch": 0.1215528781793842,
      "grad_norm": 2.083259105682373,
      "learning_rate": 0.00019193433261955746,
      "loss": 0.225,
      "step": 227
    },
    {
      "epoch": 0.12208835341365462,
      "grad_norm": 2.0369951725006104,
      "learning_rate": 0.0001918986438258387,
      "loss": 0.2804,
      "step": 228
    },
    {
      "epoch": 0.12262382864792504,
      "grad_norm": 1.5654523372650146,
      "learning_rate": 0.00019186295503211993,
      "loss": 0.1955,
      "step": 229
    },
    {
      "epoch": 0.12315930388219545,
      "grad_norm": 3.8830246925354004,
      "learning_rate": 0.00019182726623840114,
      "loss": 0.229,
      "step": 230
    },
    {
      "epoch": 0.12369477911646587,
      "grad_norm": 2.041247606277466,
      "learning_rate": 0.00019179157744468237,
      "loss": 0.2347,
      "step": 231
    },
    {
      "epoch": 0.12423025435073627,
      "grad_norm": 5.003293037414551,
      "learning_rate": 0.0001917558886509636,
      "loss": 0.2851,
      "step": 232
    },
    {
      "epoch": 0.12476572958500669,
      "grad_norm": 3.8716280460357666,
      "learning_rate": 0.00019172019985724484,
      "loss": 0.2671,
      "step": 233
    },
    {
      "epoch": 0.12530120481927712,
      "grad_norm": 2.7716422080993652,
      "learning_rate": 0.00019168451106352604,
      "loss": 0.3183,
      "step": 234
    },
    {
      "epoch": 0.12583668005354753,
      "grad_norm": 2.304382085800171,
      "learning_rate": 0.0001916488222698073,
      "loss": 0.3086,
      "step": 235
    },
    {
      "epoch": 0.12637215528781795,
      "grad_norm": 2.074761390686035,
      "learning_rate": 0.0001916131334760885,
      "loss": 0.2898,
      "step": 236
    },
    {
      "epoch": 0.12690763052208837,
      "grad_norm": 5.650290012359619,
      "learning_rate": 0.00019157744468236974,
      "loss": 0.2686,
      "step": 237
    },
    {
      "epoch": 0.12744310575635878,
      "grad_norm": 3.8286566734313965,
      "learning_rate": 0.00019154175588865098,
      "loss": 0.1877,
      "step": 238
    },
    {
      "epoch": 0.12797858099062917,
      "grad_norm": 2.0830564498901367,
      "learning_rate": 0.00019150606709493218,
      "loss": 0.1676,
      "step": 239
    },
    {
      "epoch": 0.1285140562248996,
      "grad_norm": 4.941678524017334,
      "learning_rate": 0.00019147037830121344,
      "loss": 0.3348,
      "step": 240
    },
    {
      "epoch": 0.12904953145917,
      "grad_norm": 3.295227289199829,
      "learning_rate": 0.00019143468950749465,
      "loss": 0.2595,
      "step": 241
    },
    {
      "epoch": 0.12958500669344042,
      "grad_norm": 0.8408481478691101,
      "learning_rate": 0.00019139900071377588,
      "loss": 0.1349,
      "step": 242
    },
    {
      "epoch": 0.13012048192771083,
      "grad_norm": 2.8948512077331543,
      "learning_rate": 0.00019136331192005712,
      "loss": 0.1606,
      "step": 243
    },
    {
      "epoch": 0.13065595716198125,
      "grad_norm": 2.6808576583862305,
      "learning_rate": 0.00019132762312633835,
      "loss": 0.1798,
      "step": 244
    },
    {
      "epoch": 0.13119143239625167,
      "grad_norm": 5.602467060089111,
      "learning_rate": 0.00019129193433261956,
      "loss": 0.3251,
      "step": 245
    },
    {
      "epoch": 0.13172690763052208,
      "grad_norm": 1.5883119106292725,
      "learning_rate": 0.0001912562455389008,
      "loss": 0.2588,
      "step": 246
    },
    {
      "epoch": 0.1322623828647925,
      "grad_norm": 3.996354579925537,
      "learning_rate": 0.00019122055674518202,
      "loss": 0.2631,
      "step": 247
    },
    {
      "epoch": 0.13279785809906292,
      "grad_norm": 7.210436820983887,
      "learning_rate": 0.00019118486795146323,
      "loss": 0.3361,
      "step": 248
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 2.0742099285125732,
      "learning_rate": 0.0001911491791577445,
      "loss": 0.1664,
      "step": 249
    },
    {
      "epoch": 0.13386880856760375,
      "grad_norm": 4.88632345199585,
      "learning_rate": 0.0001911134903640257,
      "loss": 0.2226,
      "step": 250
    },
    {
      "epoch": 0.13440428380187416,
      "grad_norm": 1.7384263277053833,
      "learning_rate": 0.00019107780157030693,
      "loss": 0.1286,
      "step": 251
    },
    {
      "epoch": 0.13493975903614458,
      "grad_norm": 6.1873393058776855,
      "learning_rate": 0.00019104211277658816,
      "loss": 0.4206,
      "step": 252
    },
    {
      "epoch": 0.135475234270415,
      "grad_norm": 5.499168872833252,
      "learning_rate": 0.0001910064239828694,
      "loss": 0.315,
      "step": 253
    },
    {
      "epoch": 0.1360107095046854,
      "grad_norm": 4.1553239822387695,
      "learning_rate": 0.0001909707351891506,
      "loss": 0.1801,
      "step": 254
    },
    {
      "epoch": 0.13654618473895583,
      "grad_norm": 7.146067142486572,
      "learning_rate": 0.00019093504639543184,
      "loss": 0.308,
      "step": 255
    },
    {
      "epoch": 0.13708165997322624,
      "grad_norm": 3.6156256198883057,
      "learning_rate": 0.00019089935760171307,
      "loss": 0.1463,
      "step": 256
    },
    {
      "epoch": 0.13761713520749666,
      "grad_norm": 2.2415027618408203,
      "learning_rate": 0.0001908636688079943,
      "loss": 0.2308,
      "step": 257
    },
    {
      "epoch": 0.13815261044176708,
      "grad_norm": 4.01220703125,
      "learning_rate": 0.00019082798001427554,
      "loss": 0.232,
      "step": 258
    },
    {
      "epoch": 0.1386880856760375,
      "grad_norm": 5.680580139160156,
      "learning_rate": 0.00019079229122055674,
      "loss": 0.2158,
      "step": 259
    },
    {
      "epoch": 0.1392235609103079,
      "grad_norm": 8.236593246459961,
      "learning_rate": 0.00019075660242683798,
      "loss": 0.3428,
      "step": 260
    },
    {
      "epoch": 0.13975903614457832,
      "grad_norm": 3.7375640869140625,
      "learning_rate": 0.0001907209136331192,
      "loss": 0.1943,
      "step": 261
    },
    {
      "epoch": 0.14029451137884874,
      "grad_norm": 3.321308135986328,
      "learning_rate": 0.00019068522483940044,
      "loss": 0.1745,
      "step": 262
    },
    {
      "epoch": 0.14082998661311916,
      "grad_norm": 2.639873743057251,
      "learning_rate": 0.00019064953604568168,
      "loss": 0.1999,
      "step": 263
    },
    {
      "epoch": 0.14136546184738955,
      "grad_norm": 3.075183153152466,
      "learning_rate": 0.0001906138472519629,
      "loss": 0.2924,
      "step": 264
    },
    {
      "epoch": 0.14190093708165996,
      "grad_norm": 2.4940454959869385,
      "learning_rate": 0.00019057815845824412,
      "loss": 0.2673,
      "step": 265
    },
    {
      "epoch": 0.14243641231593038,
      "grad_norm": 8.142674446105957,
      "learning_rate": 0.00019054246966452535,
      "loss": 0.2803,
      "step": 266
    },
    {
      "epoch": 0.1429718875502008,
      "grad_norm": 4.260293006896973,
      "learning_rate": 0.00019050678087080658,
      "loss": 0.2796,
      "step": 267
    },
    {
      "epoch": 0.1435073627844712,
      "grad_norm": 1.8851594924926758,
      "learning_rate": 0.0001904710920770878,
      "loss": 0.2016,
      "step": 268
    },
    {
      "epoch": 0.14404283801874163,
      "grad_norm": 3.8971810340881348,
      "learning_rate": 0.00019043540328336905,
      "loss": 0.3302,
      "step": 269
    },
    {
      "epoch": 0.14457831325301204,
      "grad_norm": 2.4696311950683594,
      "learning_rate": 0.00019039971448965026,
      "loss": 0.14,
      "step": 270
    },
    {
      "epoch": 0.14511378848728246,
      "grad_norm": 3.9050681591033936,
      "learning_rate": 0.0001903640256959315,
      "loss": 0.2085,
      "step": 271
    },
    {
      "epoch": 0.14564926372155287,
      "grad_norm": 1.4639389514923096,
      "learning_rate": 0.00019032833690221272,
      "loss": 0.1015,
      "step": 272
    },
    {
      "epoch": 0.1461847389558233,
      "grad_norm": 2.184617519378662,
      "learning_rate": 0.00019029264810849396,
      "loss": 0.1469,
      "step": 273
    },
    {
      "epoch": 0.1467202141900937,
      "grad_norm": 3.7796950340270996,
      "learning_rate": 0.00019025695931477516,
      "loss": 0.25,
      "step": 274
    },
    {
      "epoch": 0.14725568942436412,
      "grad_norm": 9.957985877990723,
      "learning_rate": 0.0001902212705210564,
      "loss": 0.3274,
      "step": 275
    },
    {
      "epoch": 0.14779116465863454,
      "grad_norm": 5.142993927001953,
      "learning_rate": 0.00019018558172733763,
      "loss": 0.3439,
      "step": 276
    },
    {
      "epoch": 0.14832663989290495,
      "grad_norm": 5.547002792358398,
      "learning_rate": 0.00019014989293361884,
      "loss": 0.2677,
      "step": 277
    },
    {
      "epoch": 0.14886211512717537,
      "grad_norm": 3.911297082901001,
      "learning_rate": 0.0001901142041399001,
      "loss": 0.2657,
      "step": 278
    },
    {
      "epoch": 0.1493975903614458,
      "grad_norm": 4.473353862762451,
      "learning_rate": 0.0001900785153461813,
      "loss": 0.2793,
      "step": 279
    },
    {
      "epoch": 0.1499330655957162,
      "grad_norm": 1.330859661102295,
      "learning_rate": 0.00019004282655246254,
      "loss": 0.1093,
      "step": 280
    },
    {
      "epoch": 0.15046854082998662,
      "grad_norm": 1.4352778196334839,
      "learning_rate": 0.00019000713775874377,
      "loss": 0.1213,
      "step": 281
    },
    {
      "epoch": 0.15100401606425704,
      "grad_norm": 2.7008330821990967,
      "learning_rate": 0.000189971448965025,
      "loss": 0.1822,
      "step": 282
    },
    {
      "epoch": 0.15153949129852745,
      "grad_norm": 2.0971086025238037,
      "learning_rate": 0.0001899357601713062,
      "loss": 0.165,
      "step": 283
    },
    {
      "epoch": 0.15207496653279787,
      "grad_norm": 2.090524435043335,
      "learning_rate": 0.00018990007137758744,
      "loss": 0.1798,
      "step": 284
    },
    {
      "epoch": 0.15261044176706828,
      "grad_norm": 2.234138250350952,
      "learning_rate": 0.00018986438258386868,
      "loss": 0.1524,
      "step": 285
    },
    {
      "epoch": 0.1531459170013387,
      "grad_norm": 3.0855119228363037,
      "learning_rate": 0.00018982869379014988,
      "loss": 0.1274,
      "step": 286
    },
    {
      "epoch": 0.15368139223560912,
      "grad_norm": 3.70585298538208,
      "learning_rate": 0.00018979300499643114,
      "loss": 0.1466,
      "step": 287
    },
    {
      "epoch": 0.15421686746987953,
      "grad_norm": 4.482032299041748,
      "learning_rate": 0.00018975731620271235,
      "loss": 0.2703,
      "step": 288
    },
    {
      "epoch": 0.15475234270414992,
      "grad_norm": 1.826433539390564,
      "learning_rate": 0.00018972162740899358,
      "loss": 0.1716,
      "step": 289
    },
    {
      "epoch": 0.15528781793842034,
      "grad_norm": 8.013652801513672,
      "learning_rate": 0.00018968593861527482,
      "loss": 0.2295,
      "step": 290
    },
    {
      "epoch": 0.15582329317269075,
      "grad_norm": 3.2245850563049316,
      "learning_rate": 0.00018965024982155605,
      "loss": 0.1623,
      "step": 291
    },
    {
      "epoch": 0.15635876840696117,
      "grad_norm": 1.8161578178405762,
      "learning_rate": 0.00018961456102783728,
      "loss": 0.1554,
      "step": 292
    },
    {
      "epoch": 0.15689424364123158,
      "grad_norm": 2.698505401611328,
      "learning_rate": 0.0001895788722341185,
      "loss": 0.1832,
      "step": 293
    },
    {
      "epoch": 0.157429718875502,
      "grad_norm": 7.298353672027588,
      "learning_rate": 0.00018954318344039972,
      "loss": 0.2312,
      "step": 294
    },
    {
      "epoch": 0.15796519410977242,
      "grad_norm": 1.6393250226974487,
      "learning_rate": 0.00018950749464668096,
      "loss": 0.1495,
      "step": 295
    },
    {
      "epoch": 0.15850066934404283,
      "grad_norm": 1.7742016315460205,
      "learning_rate": 0.0001894718058529622,
      "loss": 0.2052,
      "step": 296
    },
    {
      "epoch": 0.15903614457831325,
      "grad_norm": 1.9371387958526611,
      "learning_rate": 0.0001894361170592434,
      "loss": 0.1165,
      "step": 297
    },
    {
      "epoch": 0.15957161981258366,
      "grad_norm": 2.2205581665039062,
      "learning_rate": 0.00018940042826552466,
      "loss": 0.2773,
      "step": 298
    },
    {
      "epoch": 0.16010709504685408,
      "grad_norm": 1.5709494352340698,
      "learning_rate": 0.00018936473947180586,
      "loss": 0.1925,
      "step": 299
    },
    {
      "epoch": 0.1606425702811245,
      "grad_norm": 2.617382764816284,
      "learning_rate": 0.0001893290506780871,
      "loss": 0.1606,
      "step": 300
    },
    {
      "epoch": 0.1611780455153949,
      "grad_norm": 2.0950112342834473,
      "learning_rate": 0.00018929336188436833,
      "loss": 0.2104,
      "step": 301
    },
    {
      "epoch": 0.16171352074966533,
      "grad_norm": 5.239375114440918,
      "learning_rate": 0.00018925767309064954,
      "loss": 0.1956,
      "step": 302
    },
    {
      "epoch": 0.16224899598393575,
      "grad_norm": 1.257652759552002,
      "learning_rate": 0.00018922198429693077,
      "loss": 0.1291,
      "step": 303
    },
    {
      "epoch": 0.16278447121820616,
      "grad_norm": 4.001703262329102,
      "learning_rate": 0.000189186295503212,
      "loss": 0.1502,
      "step": 304
    },
    {
      "epoch": 0.16331994645247658,
      "grad_norm": 2.2335991859436035,
      "learning_rate": 0.00018915060670949324,
      "loss": 0.2367,
      "step": 305
    },
    {
      "epoch": 0.163855421686747,
      "grad_norm": 4.711214065551758,
      "learning_rate": 0.00018911491791577444,
      "loss": 0.206,
      "step": 306
    },
    {
      "epoch": 0.1643908969210174,
      "grad_norm": 3.9119369983673096,
      "learning_rate": 0.0001890792291220557,
      "loss": 0.1449,
      "step": 307
    },
    {
      "epoch": 0.16492637215528783,
      "grad_norm": 6.146821022033691,
      "learning_rate": 0.0001890435403283369,
      "loss": 0.3521,
      "step": 308
    },
    {
      "epoch": 0.16546184738955824,
      "grad_norm": 2.2916128635406494,
      "learning_rate": 0.00018900785153461814,
      "loss": 0.1368,
      "step": 309
    },
    {
      "epoch": 0.16599732262382866,
      "grad_norm": 2.0156478881835938,
      "learning_rate": 0.00018897216274089938,
      "loss": 0.1044,
      "step": 310
    },
    {
      "epoch": 0.16653279785809907,
      "grad_norm": 2.4998021125793457,
      "learning_rate": 0.00018893647394718058,
      "loss": 0.1786,
      "step": 311
    },
    {
      "epoch": 0.1670682730923695,
      "grad_norm": 5.418837547302246,
      "learning_rate": 0.00018890078515346182,
      "loss": 0.2794,
      "step": 312
    },
    {
      "epoch": 0.1676037483266399,
      "grad_norm": 2.6409244537353516,
      "learning_rate": 0.00018886509635974305,
      "loss": 0.1673,
      "step": 313
    },
    {
      "epoch": 0.1681392235609103,
      "grad_norm": 2.586103916168213,
      "learning_rate": 0.00018882940756602428,
      "loss": 0.2112,
      "step": 314
    },
    {
      "epoch": 0.1686746987951807,
      "grad_norm": 4.063371658325195,
      "learning_rate": 0.0001887937187723055,
      "loss": 0.219,
      "step": 315
    },
    {
      "epoch": 0.16921017402945113,
      "grad_norm": 18.216596603393555,
      "learning_rate": 0.00018875802997858675,
      "loss": 0.1923,
      "step": 316
    },
    {
      "epoch": 0.16974564926372154,
      "grad_norm": 3.208279848098755,
      "learning_rate": 0.00018872234118486796,
      "loss": 0.0919,
      "step": 317
    },
    {
      "epoch": 0.17028112449799196,
      "grad_norm": 3.194593906402588,
      "learning_rate": 0.0001886866523911492,
      "loss": 0.1559,
      "step": 318
    },
    {
      "epoch": 0.17081659973226238,
      "grad_norm": 4.952839374542236,
      "learning_rate": 0.00018865096359743042,
      "loss": 0.1651,
      "step": 319
    },
    {
      "epoch": 0.1713520749665328,
      "grad_norm": 4.571192264556885,
      "learning_rate": 0.00018861527480371163,
      "loss": 0.1988,
      "step": 320
    },
    {
      "epoch": 0.1718875502008032,
      "grad_norm": 4.192905426025391,
      "learning_rate": 0.0001885795860099929,
      "loss": 0.1955,
      "step": 321
    },
    {
      "epoch": 0.17242302543507362,
      "grad_norm": 5.246741771697998,
      "learning_rate": 0.0001885438972162741,
      "loss": 0.2114,
      "step": 322
    },
    {
      "epoch": 0.17295850066934404,
      "grad_norm": 2.7231574058532715,
      "learning_rate": 0.00018850820842255533,
      "loss": 0.0804,
      "step": 323
    },
    {
      "epoch": 0.17349397590361446,
      "grad_norm": 6.103320121765137,
      "learning_rate": 0.00018847251962883656,
      "loss": 0.2402,
      "step": 324
    },
    {
      "epoch": 0.17402945113788487,
      "grad_norm": 2.634235143661499,
      "learning_rate": 0.0001884368308351178,
      "loss": 0.1457,
      "step": 325
    },
    {
      "epoch": 0.1745649263721553,
      "grad_norm": 2.561789035797119,
      "learning_rate": 0.000188401142041399,
      "loss": 0.2311,
      "step": 326
    },
    {
      "epoch": 0.1751004016064257,
      "grad_norm": 3.710379123687744,
      "learning_rate": 0.00018836545324768024,
      "loss": 0.1727,
      "step": 327
    },
    {
      "epoch": 0.17563587684069612,
      "grad_norm": 9.259867668151855,
      "learning_rate": 0.00018832976445396147,
      "loss": 0.159,
      "step": 328
    },
    {
      "epoch": 0.17617135207496654,
      "grad_norm": 3.5976510047912598,
      "learning_rate": 0.00018829407566024268,
      "loss": 0.124,
      "step": 329
    },
    {
      "epoch": 0.17670682730923695,
      "grad_norm": 4.458890914916992,
      "learning_rate": 0.00018825838686652394,
      "loss": 0.2749,
      "step": 330
    },
    {
      "epoch": 0.17724230254350737,
      "grad_norm": 1.9956306219100952,
      "learning_rate": 0.00018822269807280514,
      "loss": 0.1804,
      "step": 331
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 2.845128297805786,
      "learning_rate": 0.00018818700927908638,
      "loss": 0.173,
      "step": 332
    },
    {
      "epoch": 0.1783132530120482,
      "grad_norm": 6.2080583572387695,
      "learning_rate": 0.0001881513204853676,
      "loss": 0.1681,
      "step": 333
    },
    {
      "epoch": 0.17884872824631862,
      "grad_norm": 1.3863463401794434,
      "learning_rate": 0.00018811563169164884,
      "loss": 0.1562,
      "step": 334
    },
    {
      "epoch": 0.17938420348058903,
      "grad_norm": 3.2025089263916016,
      "learning_rate": 0.00018807994289793005,
      "loss": 0.1722,
      "step": 335
    },
    {
      "epoch": 0.17991967871485945,
      "grad_norm": 2.1971209049224854,
      "learning_rate": 0.00018804425410421128,
      "loss": 0.1848,
      "step": 336
    },
    {
      "epoch": 0.18045515394912987,
      "grad_norm": 2.205824851989746,
      "learning_rate": 0.00018800856531049252,
      "loss": 0.159,
      "step": 337
    },
    {
      "epoch": 0.18099062918340028,
      "grad_norm": 10.88968276977539,
      "learning_rate": 0.00018797287651677372,
      "loss": 0.2607,
      "step": 338
    },
    {
      "epoch": 0.18152610441767067,
      "grad_norm": 4.6618571281433105,
      "learning_rate": 0.00018793718772305498,
      "loss": 0.2616,
      "step": 339
    },
    {
      "epoch": 0.18206157965194109,
      "grad_norm": 2.7953922748565674,
      "learning_rate": 0.0001879014989293362,
      "loss": 0.2027,
      "step": 340
    },
    {
      "epoch": 0.1825970548862115,
      "grad_norm": 20.268375396728516,
      "learning_rate": 0.00018786581013561742,
      "loss": 0.222,
      "step": 341
    },
    {
      "epoch": 0.18313253012048192,
      "grad_norm": 8.823720932006836,
      "learning_rate": 0.00018783012134189866,
      "loss": 0.1068,
      "step": 342
    },
    {
      "epoch": 0.18366800535475233,
      "grad_norm": 2.264052391052246,
      "learning_rate": 0.0001877944325481799,
      "loss": 0.1337,
      "step": 343
    },
    {
      "epoch": 0.18420348058902275,
      "grad_norm": 6.6271257400512695,
      "learning_rate": 0.0001877587437544611,
      "loss": 0.1951,
      "step": 344
    },
    {
      "epoch": 0.18473895582329317,
      "grad_norm": 2.3432788848876953,
      "learning_rate": 0.00018772305496074233,
      "loss": 0.1736,
      "step": 345
    },
    {
      "epoch": 0.18527443105756358,
      "grad_norm": 9.036993980407715,
      "learning_rate": 0.00018768736616702356,
      "loss": 0.18,
      "step": 346
    },
    {
      "epoch": 0.185809906291834,
      "grad_norm": 1.9782711267471313,
      "learning_rate": 0.0001876516773733048,
      "loss": 0.1836,
      "step": 347
    },
    {
      "epoch": 0.18634538152610441,
      "grad_norm": 4.036480903625488,
      "learning_rate": 0.00018761598857958603,
      "loss": 0.1591,
      "step": 348
    },
    {
      "epoch": 0.18688085676037483,
      "grad_norm": 4.453556537628174,
      "learning_rate": 0.00018758029978586724,
      "loss": 0.1821,
      "step": 349
    },
    {
      "epoch": 0.18741633199464525,
      "grad_norm": 4.313691139221191,
      "learning_rate": 0.0001875446109921485,
      "loss": 0.1931,
      "step": 350
    },
    {
      "epoch": 0.18795180722891566,
      "grad_norm": 3.2801530361175537,
      "learning_rate": 0.0001875089221984297,
      "loss": 0.2091,
      "step": 351
    },
    {
      "epoch": 0.18848728246318608,
      "grad_norm": 2.2774837017059326,
      "learning_rate": 0.00018747323340471094,
      "loss": 0.1718,
      "step": 352
    },
    {
      "epoch": 0.1890227576974565,
      "grad_norm": 1.617059350013733,
      "learning_rate": 0.00018743754461099217,
      "loss": 0.1637,
      "step": 353
    },
    {
      "epoch": 0.1895582329317269,
      "grad_norm": 1.2762759923934937,
      "learning_rate": 0.00018740185581727338,
      "loss": 0.0771,
      "step": 354
    },
    {
      "epoch": 0.19009370816599733,
      "grad_norm": 4.269406318664551,
      "learning_rate": 0.0001873661670235546,
      "loss": 0.1782,
      "step": 355
    },
    {
      "epoch": 0.19062918340026774,
      "grad_norm": 1.2287755012512207,
      "learning_rate": 0.00018733047822983584,
      "loss": 0.122,
      "step": 356
    },
    {
      "epoch": 0.19116465863453816,
      "grad_norm": 5.604659080505371,
      "learning_rate": 0.00018729478943611708,
      "loss": 0.1175,
      "step": 357
    },
    {
      "epoch": 0.19170013386880858,
      "grad_norm": 4.8874101638793945,
      "learning_rate": 0.00018725910064239828,
      "loss": 0.1541,
      "step": 358
    },
    {
      "epoch": 0.192235609103079,
      "grad_norm": 2.008100748062134,
      "learning_rate": 0.00018722341184867954,
      "loss": 0.0916,
      "step": 359
    },
    {
      "epoch": 0.1927710843373494,
      "grad_norm": 4.331005096435547,
      "learning_rate": 0.00018718772305496075,
      "loss": 0.1858,
      "step": 360
    },
    {
      "epoch": 0.19330655957161982,
      "grad_norm": 3.4496984481811523,
      "learning_rate": 0.00018715203426124198,
      "loss": 0.1649,
      "step": 361
    },
    {
      "epoch": 0.19384203480589024,
      "grad_norm": 2.3953745365142822,
      "learning_rate": 0.00018711634546752322,
      "loss": 0.0517,
      "step": 362
    },
    {
      "epoch": 0.19437751004016066,
      "grad_norm": 4.559731960296631,
      "learning_rate": 0.00018708065667380442,
      "loss": 0.2989,
      "step": 363
    },
    {
      "epoch": 0.19491298527443104,
      "grad_norm": 8.36156177520752,
      "learning_rate": 0.00018704496788008566,
      "loss": 0.125,
      "step": 364
    },
    {
      "epoch": 0.19544846050870146,
      "grad_norm": 6.297116756439209,
      "learning_rate": 0.0001870092790863669,
      "loss": 0.221,
      "step": 365
    },
    {
      "epoch": 0.19598393574297188,
      "grad_norm": 21.15686798095703,
      "learning_rate": 0.00018697359029264812,
      "loss": 0.0575,
      "step": 366
    },
    {
      "epoch": 0.1965194109772423,
      "grad_norm": 1.9376322031021118,
      "learning_rate": 0.00018693790149892933,
      "loss": 0.1147,
      "step": 367
    },
    {
      "epoch": 0.1970548862115127,
      "grad_norm": 4.685461521148682,
      "learning_rate": 0.0001869022127052106,
      "loss": 0.126,
      "step": 368
    },
    {
      "epoch": 0.19759036144578312,
      "grad_norm": 2.640117883682251,
      "learning_rate": 0.0001868665239114918,
      "loss": 0.1401,
      "step": 369
    },
    {
      "epoch": 0.19812583668005354,
      "grad_norm": 3.1543333530426025,
      "learning_rate": 0.00018683083511777303,
      "loss": 0.2084,
      "step": 370
    },
    {
      "epoch": 0.19866131191432396,
      "grad_norm": 10.998880386352539,
      "learning_rate": 0.00018679514632405426,
      "loss": 0.2058,
      "step": 371
    },
    {
      "epoch": 0.19919678714859437,
      "grad_norm": 2.5712876319885254,
      "learning_rate": 0.00018675945753033547,
      "loss": 0.2156,
      "step": 372
    },
    {
      "epoch": 0.1997322623828648,
      "grad_norm": 0.8988195657730103,
      "learning_rate": 0.00018672376873661673,
      "loss": 0.0562,
      "step": 373
    },
    {
      "epoch": 0.2002677376171352,
      "grad_norm": 2.1570992469787598,
      "learning_rate": 0.00018668807994289794,
      "loss": 0.1109,
      "step": 374
    },
    {
      "epoch": 0.20080321285140562,
      "grad_norm": 4.13282585144043,
      "learning_rate": 0.00018665239114917917,
      "loss": 0.1663,
      "step": 375
    },
    {
      "epoch": 0.20133868808567604,
      "grad_norm": 9.053890228271484,
      "learning_rate": 0.0001866167023554604,
      "loss": 0.1167,
      "step": 376
    },
    {
      "epoch": 0.20187416331994645,
      "grad_norm": 5.354872703552246,
      "learning_rate": 0.00018658101356174164,
      "loss": 0.3244,
      "step": 377
    },
    {
      "epoch": 0.20240963855421687,
      "grad_norm": 2.522718667984009,
      "learning_rate": 0.00018654532476802284,
      "loss": 0.1986,
      "step": 378
    },
    {
      "epoch": 0.20294511378848729,
      "grad_norm": 5.463988304138184,
      "learning_rate": 0.00018650963597430408,
      "loss": 0.1913,
      "step": 379
    },
    {
      "epoch": 0.2034805890227577,
      "grad_norm": 4.140024185180664,
      "learning_rate": 0.0001864739471805853,
      "loss": 0.1346,
      "step": 380
    },
    {
      "epoch": 0.20401606425702812,
      "grad_norm": 2.548135280609131,
      "learning_rate": 0.00018643825838686652,
      "loss": 0.1693,
      "step": 381
    },
    {
      "epoch": 0.20455153949129853,
      "grad_norm": 2.456285238265991,
      "learning_rate": 0.00018640256959314778,
      "loss": 0.1471,
      "step": 382
    },
    {
      "epoch": 0.20508701472556895,
      "grad_norm": 2.7270095348358154,
      "learning_rate": 0.00018636688079942898,
      "loss": 0.123,
      "step": 383
    },
    {
      "epoch": 0.20562248995983937,
      "grad_norm": 5.872354984283447,
      "learning_rate": 0.00018633119200571022,
      "loss": 0.1402,
      "step": 384
    },
    {
      "epoch": 0.20615796519410978,
      "grad_norm": 4.289919853210449,
      "learning_rate": 0.00018629550321199145,
      "loss": 0.2127,
      "step": 385
    },
    {
      "epoch": 0.2066934404283802,
      "grad_norm": 7.36474609375,
      "learning_rate": 0.00018625981441827268,
      "loss": 0.174,
      "step": 386
    },
    {
      "epoch": 0.20722891566265061,
      "grad_norm": 5.494461536407471,
      "learning_rate": 0.0001862241256245539,
      "loss": 0.1559,
      "step": 387
    },
    {
      "epoch": 0.20776439089692103,
      "grad_norm": 6.33180046081543,
      "learning_rate": 0.00018618843683083512,
      "loss": 0.1342,
      "step": 388
    },
    {
      "epoch": 0.20829986613119142,
      "grad_norm": 1.702929973602295,
      "learning_rate": 0.00018615274803711636,
      "loss": 0.1601,
      "step": 389
    },
    {
      "epoch": 0.20883534136546184,
      "grad_norm": 11.908486366271973,
      "learning_rate": 0.00018611705924339756,
      "loss": 0.2019,
      "step": 390
    },
    {
      "epoch": 0.20937081659973225,
      "grad_norm": 1.7441068887710571,
      "learning_rate": 0.00018608137044967882,
      "loss": 0.1742,
      "step": 391
    },
    {
      "epoch": 0.20990629183400267,
      "grad_norm": 5.6068596839904785,
      "learning_rate": 0.00018604568165596003,
      "loss": 0.1173,
      "step": 392
    },
    {
      "epoch": 0.21044176706827308,
      "grad_norm": 3.358851671218872,
      "learning_rate": 0.00018600999286224126,
      "loss": 0.2013,
      "step": 393
    },
    {
      "epoch": 0.2109772423025435,
      "grad_norm": 1.4230693578720093,
      "learning_rate": 0.0001859743040685225,
      "loss": 0.1061,
      "step": 394
    },
    {
      "epoch": 0.21151271753681392,
      "grad_norm": 2.038712978363037,
      "learning_rate": 0.00018593861527480373,
      "loss": 0.0795,
      "step": 395
    },
    {
      "epoch": 0.21204819277108433,
      "grad_norm": 2.2173569202423096,
      "learning_rate": 0.00018590292648108494,
      "loss": 0.1533,
      "step": 396
    },
    {
      "epoch": 0.21258366800535475,
      "grad_norm": 8.804583549499512,
      "learning_rate": 0.00018586723768736617,
      "loss": 0.2002,
      "step": 397
    },
    {
      "epoch": 0.21311914323962516,
      "grad_norm": 3.5151050090789795,
      "learning_rate": 0.0001858315488936474,
      "loss": 0.1324,
      "step": 398
    },
    {
      "epoch": 0.21365461847389558,
      "grad_norm": 3.0062854290008545,
      "learning_rate": 0.0001857958600999286,
      "loss": 0.2983,
      "step": 399
    },
    {
      "epoch": 0.214190093708166,
      "grad_norm": 2.1054816246032715,
      "learning_rate": 0.00018576017130620987,
      "loss": 0.0966,
      "step": 400
    },
    {
      "epoch": 0.2147255689424364,
      "grad_norm": 8.416375160217285,
      "learning_rate": 0.00018572448251249108,
      "loss": 0.1853,
      "step": 401
    },
    {
      "epoch": 0.21526104417670683,
      "grad_norm": 4.39292049407959,
      "learning_rate": 0.00018568879371877234,
      "loss": 0.1986,
      "step": 402
    },
    {
      "epoch": 0.21579651941097724,
      "grad_norm": 5.377541542053223,
      "learning_rate": 0.00018565310492505354,
      "loss": 0.1658,
      "step": 403
    },
    {
      "epoch": 0.21633199464524766,
      "grad_norm": 3.374706745147705,
      "learning_rate": 0.00018561741613133478,
      "loss": 0.1814,
      "step": 404
    },
    {
      "epoch": 0.21686746987951808,
      "grad_norm": 3.3798694610595703,
      "learning_rate": 0.000185581727337616,
      "loss": 0.195,
      "step": 405
    },
    {
      "epoch": 0.2174029451137885,
      "grad_norm": 3.283109664916992,
      "learning_rate": 0.00018554603854389722,
      "loss": 0.2098,
      "step": 406
    },
    {
      "epoch": 0.2179384203480589,
      "grad_norm": 3.514481782913208,
      "learning_rate": 0.00018551034975017845,
      "loss": 0.1405,
      "step": 407
    },
    {
      "epoch": 0.21847389558232932,
      "grad_norm": 6.065486431121826,
      "learning_rate": 0.00018547466095645968,
      "loss": 0.2205,
      "step": 408
    },
    {
      "epoch": 0.21900937081659974,
      "grad_norm": 3.459144115447998,
      "learning_rate": 0.00018543897216274092,
      "loss": 0.2476,
      "step": 409
    },
    {
      "epoch": 0.21954484605087016,
      "grad_norm": 3.772671937942505,
      "learning_rate": 0.00018540328336902212,
      "loss": 0.2194,
      "step": 410
    },
    {
      "epoch": 0.22008032128514057,
      "grad_norm": 4.319811820983887,
      "learning_rate": 0.00018536759457530338,
      "loss": 0.1799,
      "step": 411
    },
    {
      "epoch": 0.220615796519411,
      "grad_norm": 1.5041587352752686,
      "learning_rate": 0.0001853319057815846,
      "loss": 0.0992,
      "step": 412
    },
    {
      "epoch": 0.2211512717536814,
      "grad_norm": 2.135019302368164,
      "learning_rate": 0.00018529621698786582,
      "loss": 0.2334,
      "step": 413
    },
    {
      "epoch": 0.2216867469879518,
      "grad_norm": 1.7759648561477661,
      "learning_rate": 0.00018526052819414706,
      "loss": 0.1181,
      "step": 414
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 2.6035847663879395,
      "learning_rate": 0.00018522483940042826,
      "loss": 0.0851,
      "step": 415
    },
    {
      "epoch": 0.22275769745649263,
      "grad_norm": 3.763425827026367,
      "learning_rate": 0.0001851891506067095,
      "loss": 0.1683,
      "step": 416
    },
    {
      "epoch": 0.22329317269076304,
      "grad_norm": 3.6500608921051025,
      "learning_rate": 0.00018515346181299073,
      "loss": 0.1893,
      "step": 417
    },
    {
      "epoch": 0.22382864792503346,
      "grad_norm": 0.8946233987808228,
      "learning_rate": 0.00018511777301927196,
      "loss": 0.1255,
      "step": 418
    },
    {
      "epoch": 0.22436412315930387,
      "grad_norm": 2.7091102600097656,
      "learning_rate": 0.00018508208422555317,
      "loss": 0.0733,
      "step": 419
    },
    {
      "epoch": 0.2248995983935743,
      "grad_norm": 3.6680588722229004,
      "learning_rate": 0.00018504639543183443,
      "loss": 0.1348,
      "step": 420
    },
    {
      "epoch": 0.2254350736278447,
      "grad_norm": 7.1827311515808105,
      "learning_rate": 0.00018501070663811564,
      "loss": 0.0643,
      "step": 421
    },
    {
      "epoch": 0.22597054886211512,
      "grad_norm": 1.9290595054626465,
      "learning_rate": 0.00018497501784439687,
      "loss": 0.1089,
      "step": 422
    },
    {
      "epoch": 0.22650602409638554,
      "grad_norm": 2.7580502033233643,
      "learning_rate": 0.0001849393290506781,
      "loss": 0.1238,
      "step": 423
    },
    {
      "epoch": 0.22704149933065595,
      "grad_norm": 3.3824069499969482,
      "learning_rate": 0.0001849036402569593,
      "loss": 0.1567,
      "step": 424
    },
    {
      "epoch": 0.22757697456492637,
      "grad_norm": 4.562740325927734,
      "learning_rate": 0.00018486795146324054,
      "loss": 0.3409,
      "step": 425
    },
    {
      "epoch": 0.2281124497991968,
      "grad_norm": 2.751169204711914,
      "learning_rate": 0.00018483226266952178,
      "loss": 0.1217,
      "step": 426
    },
    {
      "epoch": 0.2286479250334672,
      "grad_norm": 3.789180278778076,
      "learning_rate": 0.000184796573875803,
      "loss": 0.1283,
      "step": 427
    },
    {
      "epoch": 0.22918340026773762,
      "grad_norm": 5.072024822235107,
      "learning_rate": 0.00018476088508208424,
      "loss": 0.0812,
      "step": 428
    },
    {
      "epoch": 0.22971887550200804,
      "grad_norm": 4.991504669189453,
      "learning_rate": 0.00018472519628836548,
      "loss": 0.1445,
      "step": 429
    },
    {
      "epoch": 0.23025435073627845,
      "grad_norm": 15.172174453735352,
      "learning_rate": 0.00018468950749464668,
      "loss": 0.1791,
      "step": 430
    },
    {
      "epoch": 0.23078982597054887,
      "grad_norm": 1.210308313369751,
      "learning_rate": 0.00018465381870092792,
      "loss": 0.0858,
      "step": 431
    },
    {
      "epoch": 0.23132530120481928,
      "grad_norm": 3.1862478256225586,
      "learning_rate": 0.00018461812990720915,
      "loss": 0.095,
      "step": 432
    },
    {
      "epoch": 0.2318607764390897,
      "grad_norm": 4.836492538452148,
      "learning_rate": 0.00018458244111349036,
      "loss": 0.1682,
      "step": 433
    },
    {
      "epoch": 0.23239625167336012,
      "grad_norm": 2.5082709789276123,
      "learning_rate": 0.00018454675231977162,
      "loss": 0.0908,
      "step": 434
    },
    {
      "epoch": 0.23293172690763053,
      "grad_norm": 4.102351665496826,
      "learning_rate": 0.00018451106352605282,
      "loss": 0.1281,
      "step": 435
    },
    {
      "epoch": 0.23346720214190095,
      "grad_norm": 7.990454196929932,
      "learning_rate": 0.00018447537473233406,
      "loss": 0.2383,
      "step": 436
    },
    {
      "epoch": 0.23400267737617136,
      "grad_norm": 1.7414281368255615,
      "learning_rate": 0.0001844396859386153,
      "loss": 0.1167,
      "step": 437
    },
    {
      "epoch": 0.23453815261044178,
      "grad_norm": 3.020993232727051,
      "learning_rate": 0.00018440399714489652,
      "loss": 0.1615,
      "step": 438
    },
    {
      "epoch": 0.23507362784471217,
      "grad_norm": 3.9149675369262695,
      "learning_rate": 0.00018436830835117773,
      "loss": 0.1947,
      "step": 439
    },
    {
      "epoch": 0.23560910307898258,
      "grad_norm": 2.9318249225616455,
      "learning_rate": 0.00018433261955745896,
      "loss": 0.3177,
      "step": 440
    },
    {
      "epoch": 0.236144578313253,
      "grad_norm": 3.6519219875335693,
      "learning_rate": 0.0001842969307637402,
      "loss": 0.1566,
      "step": 441
    },
    {
      "epoch": 0.23668005354752342,
      "grad_norm": 2.2365634441375732,
      "learning_rate": 0.0001842612419700214,
      "loss": 0.1767,
      "step": 442
    },
    {
      "epoch": 0.23721552878179383,
      "grad_norm": 1.7984899282455444,
      "learning_rate": 0.00018422555317630266,
      "loss": 0.1082,
      "step": 443
    },
    {
      "epoch": 0.23775100401606425,
      "grad_norm": 4.6115570068359375,
      "learning_rate": 0.00018418986438258387,
      "loss": 0.1684,
      "step": 444
    },
    {
      "epoch": 0.23828647925033467,
      "grad_norm": 2.252730131149292,
      "learning_rate": 0.0001841541755888651,
      "loss": 0.0963,
      "step": 445
    },
    {
      "epoch": 0.23882195448460508,
      "grad_norm": 1.3698829412460327,
      "learning_rate": 0.00018411848679514634,
      "loss": 0.2108,
      "step": 446
    },
    {
      "epoch": 0.2393574297188755,
      "grad_norm": 4.9425177574157715,
      "learning_rate": 0.00018408279800142757,
      "loss": 0.2396,
      "step": 447
    },
    {
      "epoch": 0.2398929049531459,
      "grad_norm": 1.152875304222107,
      "learning_rate": 0.00018404710920770878,
      "loss": 0.0983,
      "step": 448
    },
    {
      "epoch": 0.24042838018741633,
      "grad_norm": 1.395450472831726,
      "learning_rate": 0.00018401142041399,
      "loss": 0.1777,
      "step": 449
    },
    {
      "epoch": 0.24096385542168675,
      "grad_norm": 1.7617244720458984,
      "learning_rate": 0.00018397573162027124,
      "loss": 0.121,
      "step": 450
    },
    {
      "epoch": 0.24149933065595716,
      "grad_norm": 1.3120415210723877,
      "learning_rate": 0.00018394004282655245,
      "loss": 0.1231,
      "step": 451
    },
    {
      "epoch": 0.24203480589022758,
      "grad_norm": 4.50471830368042,
      "learning_rate": 0.0001839043540328337,
      "loss": 0.14,
      "step": 452
    },
    {
      "epoch": 0.242570281124498,
      "grad_norm": 1.1242735385894775,
      "learning_rate": 0.00018386866523911492,
      "loss": 0.0951,
      "step": 453
    },
    {
      "epoch": 0.2431057563587684,
      "grad_norm": 3.980975389480591,
      "learning_rate": 0.00018383297644539615,
      "loss": 0.1947,
      "step": 454
    },
    {
      "epoch": 0.24364123159303883,
      "grad_norm": 4.884604454040527,
      "learning_rate": 0.00018379728765167738,
      "loss": 0.21,
      "step": 455
    },
    {
      "epoch": 0.24417670682730924,
      "grad_norm": 4.314085960388184,
      "learning_rate": 0.00018376159885795862,
      "loss": 0.1231,
      "step": 456
    },
    {
      "epoch": 0.24471218206157966,
      "grad_norm": 4.0612053871154785,
      "learning_rate": 0.00018372591006423985,
      "loss": 0.2031,
      "step": 457
    },
    {
      "epoch": 0.24524765729585007,
      "grad_norm": 1.8418982028961182,
      "learning_rate": 0.00018369022127052106,
      "loss": 0.1389,
      "step": 458
    },
    {
      "epoch": 0.2457831325301205,
      "grad_norm": 0.8393533825874329,
      "learning_rate": 0.0001836545324768023,
      "loss": 0.0807,
      "step": 459
    },
    {
      "epoch": 0.2463186077643909,
      "grad_norm": 1.0386282205581665,
      "learning_rate": 0.00018361884368308352,
      "loss": 0.1133,
      "step": 460
    },
    {
      "epoch": 0.24685408299866132,
      "grad_norm": 1.487347960472107,
      "learning_rate": 0.00018358315488936476,
      "loss": 0.0868,
      "step": 461
    },
    {
      "epoch": 0.24738955823293174,
      "grad_norm": 1.6962848901748657,
      "learning_rate": 0.00018354746609564596,
      "loss": 0.1029,
      "step": 462
    },
    {
      "epoch": 0.24792503346720215,
      "grad_norm": 2.359797239303589,
      "learning_rate": 0.00018351177730192722,
      "loss": 0.2148,
      "step": 463
    },
    {
      "epoch": 0.24846050870147254,
      "grad_norm": 3.494431257247925,
      "learning_rate": 0.00018347608850820843,
      "loss": 0.134,
      "step": 464
    },
    {
      "epoch": 0.24899598393574296,
      "grad_norm": 0.9862854480743408,
      "learning_rate": 0.00018344039971448966,
      "loss": 0.0584,
      "step": 465
    },
    {
      "epoch": 0.24953145917001338,
      "grad_norm": 1.4223253726959229,
      "learning_rate": 0.0001834047109207709,
      "loss": 0.0986,
      "step": 466
    },
    {
      "epoch": 0.2500669344042838,
      "grad_norm": 2.1712288856506348,
      "learning_rate": 0.0001833690221270521,
      "loss": 0.1294,
      "step": 467
    },
    {
      "epoch": 0.25060240963855424,
      "grad_norm": 1.8169095516204834,
      "learning_rate": 0.00018333333333333334,
      "loss": 0.1191,
      "step": 468
    },
    {
      "epoch": 0.2511378848728246,
      "grad_norm": 4.979334831237793,
      "learning_rate": 0.00018329764453961457,
      "loss": 0.1705,
      "step": 469
    },
    {
      "epoch": 0.25167336010709507,
      "grad_norm": 5.268828868865967,
      "learning_rate": 0.0001832619557458958,
      "loss": 0.2409,
      "step": 470
    },
    {
      "epoch": 0.25220883534136546,
      "grad_norm": 1.2559993267059326,
      "learning_rate": 0.000183226266952177,
      "loss": 0.0858,
      "step": 471
    },
    {
      "epoch": 0.2527443105756359,
      "grad_norm": 5.7684831619262695,
      "learning_rate": 0.00018319057815845827,
      "loss": 0.2137,
      "step": 472
    },
    {
      "epoch": 0.2532797858099063,
      "grad_norm": 5.767971515655518,
      "learning_rate": 0.00018315488936473948,
      "loss": 0.235,
      "step": 473
    },
    {
      "epoch": 0.25381526104417673,
      "grad_norm": 4.5635881423950195,
      "learning_rate": 0.0001831192005710207,
      "loss": 0.1583,
      "step": 474
    },
    {
      "epoch": 0.2543507362784471,
      "grad_norm": 2.8249619007110596,
      "learning_rate": 0.00018308351177730194,
      "loss": 0.0908,
      "step": 475
    },
    {
      "epoch": 0.25488621151271756,
      "grad_norm": 2.3449862003326416,
      "learning_rate": 0.00018304782298358315,
      "loss": 0.2026,
      "step": 476
    },
    {
      "epoch": 0.25542168674698795,
      "grad_norm": 3.0825610160827637,
      "learning_rate": 0.00018301213418986438,
      "loss": 0.3281,
      "step": 477
    },
    {
      "epoch": 0.25595716198125834,
      "grad_norm": 1.7497680187225342,
      "learning_rate": 0.00018297644539614562,
      "loss": 0.2413,
      "step": 478
    },
    {
      "epoch": 0.2564926372155288,
      "grad_norm": 3.400172472000122,
      "learning_rate": 0.00018294075660242685,
      "loss": 0.2082,
      "step": 479
    },
    {
      "epoch": 0.2570281124497992,
      "grad_norm": 0.6890271306037903,
      "learning_rate": 0.00018290506780870806,
      "loss": 0.0854,
      "step": 480
    },
    {
      "epoch": 0.2575635876840696,
      "grad_norm": 2.956123113632202,
      "learning_rate": 0.00018286937901498932,
      "loss": 0.1362,
      "step": 481
    },
    {
      "epoch": 0.25809906291834,
      "grad_norm": 3.511720895767212,
      "learning_rate": 0.00018283369022127052,
      "loss": 0.2099,
      "step": 482
    },
    {
      "epoch": 0.25863453815261045,
      "grad_norm": 2.562514305114746,
      "learning_rate": 0.00018279800142755176,
      "loss": 0.0864,
      "step": 483
    },
    {
      "epoch": 0.25917001338688084,
      "grad_norm": 1.2970095872879028,
      "learning_rate": 0.000182762312633833,
      "loss": 0.0657,
      "step": 484
    },
    {
      "epoch": 0.2597054886211513,
      "grad_norm": 2.2726900577545166,
      "learning_rate": 0.0001827266238401142,
      "loss": 0.162,
      "step": 485
    },
    {
      "epoch": 0.26024096385542167,
      "grad_norm": 1.341857671737671,
      "learning_rate": 0.00018269093504639546,
      "loss": 0.1169,
      "step": 486
    },
    {
      "epoch": 0.2607764390896921,
      "grad_norm": 3.268895149230957,
      "learning_rate": 0.00018265524625267666,
      "loss": 0.1157,
      "step": 487
    },
    {
      "epoch": 0.2613119143239625,
      "grad_norm": 17.229047775268555,
      "learning_rate": 0.0001826195574589579,
      "loss": 0.1691,
      "step": 488
    },
    {
      "epoch": 0.26184738955823295,
      "grad_norm": 3.8889851570129395,
      "learning_rate": 0.00018258386866523913,
      "loss": 0.1327,
      "step": 489
    },
    {
      "epoch": 0.26238286479250333,
      "grad_norm": 9.422221183776855,
      "learning_rate": 0.00018254817987152036,
      "loss": 0.2354,
      "step": 490
    },
    {
      "epoch": 0.2629183400267738,
      "grad_norm": 2.8144757747650146,
      "learning_rate": 0.00018251249107780157,
      "loss": 0.2267,
      "step": 491
    },
    {
      "epoch": 0.26345381526104417,
      "grad_norm": 4.70637321472168,
      "learning_rate": 0.0001824768022840828,
      "loss": 0.1539,
      "step": 492
    },
    {
      "epoch": 0.2639892904953146,
      "grad_norm": 1.1427843570709229,
      "learning_rate": 0.00018244111349036404,
      "loss": 0.0887,
      "step": 493
    },
    {
      "epoch": 0.264524765729585,
      "grad_norm": 2.0119643211364746,
      "learning_rate": 0.00018240542469664524,
      "loss": 0.13,
      "step": 494
    },
    {
      "epoch": 0.26506024096385544,
      "grad_norm": 2.3392021656036377,
      "learning_rate": 0.0001823697359029265,
      "loss": 0.1181,
      "step": 495
    },
    {
      "epoch": 0.26559571619812583,
      "grad_norm": 0.9990190267562866,
      "learning_rate": 0.0001823340471092077,
      "loss": 0.0532,
      "step": 496
    },
    {
      "epoch": 0.2661311914323963,
      "grad_norm": 6.803038120269775,
      "learning_rate": 0.00018229835831548894,
      "loss": 0.223,
      "step": 497
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 1.4929327964782715,
      "learning_rate": 0.00018226266952177018,
      "loss": 0.1297,
      "step": 498
    },
    {
      "epoch": 0.2672021419009371,
      "grad_norm": 4.625899314880371,
      "learning_rate": 0.0001822269807280514,
      "loss": 0.178,
      "step": 499
    },
    {
      "epoch": 0.2677376171352075,
      "grad_norm": 2.266887664794922,
      "learning_rate": 0.00018219129193433262,
      "loss": 0.1173,
      "step": 500
    },
    {
      "epoch": 0.26827309236947794,
      "grad_norm": 4.76317834854126,
      "learning_rate": 0.00018215560314061385,
      "loss": 0.2428,
      "step": 501
    },
    {
      "epoch": 0.2688085676037483,
      "grad_norm": 2.7711517810821533,
      "learning_rate": 0.00018211991434689508,
      "loss": 0.117,
      "step": 502
    },
    {
      "epoch": 0.2693440428380187,
      "grad_norm": 4.517717361450195,
      "learning_rate": 0.00018208422555317632,
      "loss": 0.1569,
      "step": 503
    },
    {
      "epoch": 0.26987951807228916,
      "grad_norm": 2.5365211963653564,
      "learning_rate": 0.00018204853675945755,
      "loss": 0.1384,
      "step": 504
    },
    {
      "epoch": 0.27041499330655955,
      "grad_norm": 3.657562255859375,
      "learning_rate": 0.00018201284796573876,
      "loss": 0.2255,
      "step": 505
    },
    {
      "epoch": 0.27095046854083,
      "grad_norm": 1.4391865730285645,
      "learning_rate": 0.00018197715917202,
      "loss": 0.1819,
      "step": 506
    },
    {
      "epoch": 0.2714859437751004,
      "grad_norm": 2.5854713916778564,
      "learning_rate": 0.00018194147037830122,
      "loss": 0.1795,
      "step": 507
    },
    {
      "epoch": 0.2720214190093708,
      "grad_norm": 4.1666059494018555,
      "learning_rate": 0.00018190578158458246,
      "loss": 0.1589,
      "step": 508
    },
    {
      "epoch": 0.2725568942436412,
      "grad_norm": 1.541715145111084,
      "learning_rate": 0.00018187009279086366,
      "loss": 0.198,
      "step": 509
    },
    {
      "epoch": 0.27309236947791166,
      "grad_norm": 2.088228940963745,
      "learning_rate": 0.0001818344039971449,
      "loss": 0.1239,
      "step": 510
    },
    {
      "epoch": 0.27362784471218204,
      "grad_norm": 1.5555899143218994,
      "learning_rate": 0.00018179871520342613,
      "loss": 0.152,
      "step": 511
    },
    {
      "epoch": 0.2741633199464525,
      "grad_norm": 4.447237014770508,
      "learning_rate": 0.00018176302640970736,
      "loss": 0.1898,
      "step": 512
    },
    {
      "epoch": 0.2746987951807229,
      "grad_norm": 1.0064507722854614,
      "learning_rate": 0.0001817273376159886,
      "loss": 0.074,
      "step": 513
    },
    {
      "epoch": 0.2752342704149933,
      "grad_norm": 2.393588066101074,
      "learning_rate": 0.0001816916488222698,
      "loss": 0.1347,
      "step": 514
    },
    {
      "epoch": 0.2757697456492637,
      "grad_norm": 2.2899892330169678,
      "learning_rate": 0.00018165596002855106,
      "loss": 0.197,
      "step": 515
    },
    {
      "epoch": 0.27630522088353415,
      "grad_norm": 2.9294819831848145,
      "learning_rate": 0.00018162027123483227,
      "loss": 0.2084,
      "step": 516
    },
    {
      "epoch": 0.27684069611780454,
      "grad_norm": 1.7206472158432007,
      "learning_rate": 0.0001815845824411135,
      "loss": 0.1317,
      "step": 517
    },
    {
      "epoch": 0.277376171352075,
      "grad_norm": 1.9741417169570923,
      "learning_rate": 0.00018154889364739474,
      "loss": 0.2571,
      "step": 518
    },
    {
      "epoch": 0.2779116465863454,
      "grad_norm": 2.802377700805664,
      "learning_rate": 0.00018151320485367597,
      "loss": 0.2238,
      "step": 519
    },
    {
      "epoch": 0.2784471218206158,
      "grad_norm": 4.852760314941406,
      "learning_rate": 0.00018147751605995718,
      "loss": 0.111,
      "step": 520
    },
    {
      "epoch": 0.2789825970548862,
      "grad_norm": 0.8785884380340576,
      "learning_rate": 0.0001814418272662384,
      "loss": 0.071,
      "step": 521
    },
    {
      "epoch": 0.27951807228915665,
      "grad_norm": 2.907346725463867,
      "learning_rate": 0.00018140613847251964,
      "loss": 0.1795,
      "step": 522
    },
    {
      "epoch": 0.28005354752342704,
      "grad_norm": 2.440608263015747,
      "learning_rate": 0.00018137044967880085,
      "loss": 0.1234,
      "step": 523
    },
    {
      "epoch": 0.2805890227576975,
      "grad_norm": 5.226071357727051,
      "learning_rate": 0.0001813347608850821,
      "loss": 0.195,
      "step": 524
    },
    {
      "epoch": 0.28112449799196787,
      "grad_norm": 1.8834187984466553,
      "learning_rate": 0.00018129907209136332,
      "loss": 0.1487,
      "step": 525
    },
    {
      "epoch": 0.2816599732262383,
      "grad_norm": 5.491235256195068,
      "learning_rate": 0.00018126338329764455,
      "loss": 0.1639,
      "step": 526
    },
    {
      "epoch": 0.2821954484605087,
      "grad_norm": 0.8751970529556274,
      "learning_rate": 0.00018122769450392578,
      "loss": 0.065,
      "step": 527
    },
    {
      "epoch": 0.2827309236947791,
      "grad_norm": 1.1606117486953735,
      "learning_rate": 0.00018119200571020702,
      "loss": 0.064,
      "step": 528
    },
    {
      "epoch": 0.28326639892904953,
      "grad_norm": 4.089672088623047,
      "learning_rate": 0.00018115631691648822,
      "loss": 0.186,
      "step": 529
    },
    {
      "epoch": 0.2838018741633199,
      "grad_norm": 1.288634181022644,
      "learning_rate": 0.00018112062812276946,
      "loss": 0.0783,
      "step": 530
    },
    {
      "epoch": 0.28433734939759037,
      "grad_norm": 2.4676897525787354,
      "learning_rate": 0.0001810849393290507,
      "loss": 0.1439,
      "step": 531
    },
    {
      "epoch": 0.28487282463186075,
      "grad_norm": 2.0121395587921143,
      "learning_rate": 0.0001810492505353319,
      "loss": 0.1386,
      "step": 532
    },
    {
      "epoch": 0.2854082998661312,
      "grad_norm": 2.565175771713257,
      "learning_rate": 0.00018101356174161316,
      "loss": 0.149,
      "step": 533
    },
    {
      "epoch": 0.2859437751004016,
      "grad_norm": 0.9268935918807983,
      "learning_rate": 0.00018097787294789436,
      "loss": 0.083,
      "step": 534
    },
    {
      "epoch": 0.28647925033467203,
      "grad_norm": 1.8697760105133057,
      "learning_rate": 0.0001809421841541756,
      "loss": 0.1519,
      "step": 535
    },
    {
      "epoch": 0.2870147255689424,
      "grad_norm": 1.175548791885376,
      "learning_rate": 0.00018090649536045683,
      "loss": 0.0978,
      "step": 536
    },
    {
      "epoch": 0.28755020080321286,
      "grad_norm": 0.47530752420425415,
      "learning_rate": 0.00018087080656673806,
      "loss": 0.0207,
      "step": 537
    },
    {
      "epoch": 0.28808567603748325,
      "grad_norm": 5.7398600578308105,
      "learning_rate": 0.0001808351177730193,
      "loss": 0.1874,
      "step": 538
    },
    {
      "epoch": 0.2886211512717537,
      "grad_norm": 1.1627082824707031,
      "learning_rate": 0.0001807994289793005,
      "loss": 0.0837,
      "step": 539
    },
    {
      "epoch": 0.2891566265060241,
      "grad_norm": 5.624179840087891,
      "learning_rate": 0.00018076374018558174,
      "loss": 0.2049,
      "step": 540
    },
    {
      "epoch": 0.2896921017402945,
      "grad_norm": 5.3394856452941895,
      "learning_rate": 0.00018072805139186297,
      "loss": 0.1492,
      "step": 541
    },
    {
      "epoch": 0.2902275769745649,
      "grad_norm": 2.4435231685638428,
      "learning_rate": 0.0001806923625981442,
      "loss": 0.1846,
      "step": 542
    },
    {
      "epoch": 0.29076305220883536,
      "grad_norm": 1.8036632537841797,
      "learning_rate": 0.0001806566738044254,
      "loss": 0.1263,
      "step": 543
    },
    {
      "epoch": 0.29129852744310575,
      "grad_norm": 1.8553754091262817,
      "learning_rate": 0.00018062098501070667,
      "loss": 0.1515,
      "step": 544
    },
    {
      "epoch": 0.2918340026773762,
      "grad_norm": 1.2793807983398438,
      "learning_rate": 0.00018058529621698788,
      "loss": 0.1483,
      "step": 545
    },
    {
      "epoch": 0.2923694779116466,
      "grad_norm": 3.3432977199554443,
      "learning_rate": 0.0001805496074232691,
      "loss": 0.2589,
      "step": 546
    },
    {
      "epoch": 0.292904953145917,
      "grad_norm": 1.6647965908050537,
      "learning_rate": 0.00018051391862955034,
      "loss": 0.1348,
      "step": 547
    },
    {
      "epoch": 0.2934404283801874,
      "grad_norm": 1.64668869972229,
      "learning_rate": 0.00018047822983583155,
      "loss": 0.1246,
      "step": 548
    },
    {
      "epoch": 0.29397590361445786,
      "grad_norm": 1.084979772567749,
      "learning_rate": 0.00018044254104211278,
      "loss": 0.0977,
      "step": 549
    },
    {
      "epoch": 0.29451137884872824,
      "grad_norm": 1.4659291505813599,
      "learning_rate": 0.00018040685224839402,
      "loss": 0.1062,
      "step": 550
    },
    {
      "epoch": 0.2950468540829987,
      "grad_norm": 0.7947089076042175,
      "learning_rate": 0.00018037116345467525,
      "loss": 0.0536,
      "step": 551
    },
    {
      "epoch": 0.2955823293172691,
      "grad_norm": 0.3910219073295593,
      "learning_rate": 0.00018033547466095646,
      "loss": 0.0587,
      "step": 552
    },
    {
      "epoch": 0.29611780455153947,
      "grad_norm": 2.8467438220977783,
      "learning_rate": 0.00018029978586723772,
      "loss": 0.1491,
      "step": 553
    },
    {
      "epoch": 0.2966532797858099,
      "grad_norm": 0.7371736168861389,
      "learning_rate": 0.00018026409707351892,
      "loss": 0.0772,
      "step": 554
    },
    {
      "epoch": 0.2971887550200803,
      "grad_norm": 0.8840590715408325,
      "learning_rate": 0.00018022840827980016,
      "loss": 0.1046,
      "step": 555
    },
    {
      "epoch": 0.29772423025435074,
      "grad_norm": 1.9205163717269897,
      "learning_rate": 0.0001801927194860814,
      "loss": 0.2225,
      "step": 556
    },
    {
      "epoch": 0.29825970548862113,
      "grad_norm": 2.0675618648529053,
      "learning_rate": 0.0001801570306923626,
      "loss": 0.0908,
      "step": 557
    },
    {
      "epoch": 0.2987951807228916,
      "grad_norm": 2.325988531112671,
      "learning_rate": 0.00018012134189864383,
      "loss": 0.1272,
      "step": 558
    },
    {
      "epoch": 0.29933065595716196,
      "grad_norm": 1.262589931488037,
      "learning_rate": 0.00018008565310492506,
      "loss": 0.1032,
      "step": 559
    },
    {
      "epoch": 0.2998661311914324,
      "grad_norm": 8.234755516052246,
      "learning_rate": 0.0001800499643112063,
      "loss": 0.1438,
      "step": 560
    },
    {
      "epoch": 0.3004016064257028,
      "grad_norm": 1.4870657920837402,
      "learning_rate": 0.0001800142755174875,
      "loss": 0.1083,
      "step": 561
    },
    {
      "epoch": 0.30093708165997324,
      "grad_norm": 6.083077430725098,
      "learning_rate": 0.00017997858672376876,
      "loss": 0.2724,
      "step": 562
    },
    {
      "epoch": 0.3014725568942436,
      "grad_norm": 6.448523044586182,
      "learning_rate": 0.00017994289793004997,
      "loss": 0.1845,
      "step": 563
    },
    {
      "epoch": 0.30200803212851407,
      "grad_norm": 10.341413497924805,
      "learning_rate": 0.0001799072091363312,
      "loss": 0.3113,
      "step": 564
    },
    {
      "epoch": 0.30254350736278446,
      "grad_norm": 2.375318765640259,
      "learning_rate": 0.00017987152034261244,
      "loss": 0.127,
      "step": 565
    },
    {
      "epoch": 0.3030789825970549,
      "grad_norm": 7.137829303741455,
      "learning_rate": 0.00017983583154889364,
      "loss": 0.3226,
      "step": 566
    },
    {
      "epoch": 0.3036144578313253,
      "grad_norm": 4.422981262207031,
      "learning_rate": 0.0001798001427551749,
      "loss": 0.1773,
      "step": 567
    },
    {
      "epoch": 0.30414993306559573,
      "grad_norm": 3.673701524734497,
      "learning_rate": 0.0001797644539614561,
      "loss": 0.1322,
      "step": 568
    },
    {
      "epoch": 0.3046854082998661,
      "grad_norm": 2.0161616802215576,
      "learning_rate": 0.00017972876516773734,
      "loss": 0.1693,
      "step": 569
    },
    {
      "epoch": 0.30522088353413657,
      "grad_norm": 1.3152488470077515,
      "learning_rate": 0.00017969307637401858,
      "loss": 0.0802,
      "step": 570
    },
    {
      "epoch": 0.30575635876840696,
      "grad_norm": 1.0280810594558716,
      "learning_rate": 0.0001796573875802998,
      "loss": 0.1324,
      "step": 571
    },
    {
      "epoch": 0.3062918340026774,
      "grad_norm": 1.242689847946167,
      "learning_rate": 0.00017962169878658102,
      "loss": 0.1371,
      "step": 572
    },
    {
      "epoch": 0.3068273092369478,
      "grad_norm": 3.602652072906494,
      "learning_rate": 0.00017958600999286225,
      "loss": 0.2246,
      "step": 573
    },
    {
      "epoch": 0.30736278447121823,
      "grad_norm": 1.1043459177017212,
      "learning_rate": 0.00017955032119914348,
      "loss": 0.1989,
      "step": 574
    },
    {
      "epoch": 0.3078982597054886,
      "grad_norm": 3.655620813369751,
      "learning_rate": 0.0001795146324054247,
      "loss": 0.2423,
      "step": 575
    },
    {
      "epoch": 0.30843373493975906,
      "grad_norm": 0.9310197830200195,
      "learning_rate": 0.00017947894361170595,
      "loss": 0.1358,
      "step": 576
    },
    {
      "epoch": 0.30896921017402945,
      "grad_norm": 1.3246936798095703,
      "learning_rate": 0.00017944325481798716,
      "loss": 0.1189,
      "step": 577
    },
    {
      "epoch": 0.30950468540829984,
      "grad_norm": 1.2040759325027466,
      "learning_rate": 0.0001794075660242684,
      "loss": 0.1733,
      "step": 578
    },
    {
      "epoch": 0.3100401606425703,
      "grad_norm": 2.334850788116455,
      "learning_rate": 0.00017937187723054962,
      "loss": 0.1472,
      "step": 579
    },
    {
      "epoch": 0.31057563587684067,
      "grad_norm": 3.3913607597351074,
      "learning_rate": 0.00017933618843683086,
      "loss": 0.1548,
      "step": 580
    },
    {
      "epoch": 0.3111111111111111,
      "grad_norm": 2.5346522331237793,
      "learning_rate": 0.00017930049964311206,
      "loss": 0.2131,
      "step": 581
    },
    {
      "epoch": 0.3116465863453815,
      "grad_norm": 2.0826406478881836,
      "learning_rate": 0.0001792648108493933,
      "loss": 0.1263,
      "step": 582
    },
    {
      "epoch": 0.31218206157965195,
      "grad_norm": 2.8843162059783936,
      "learning_rate": 0.00017922912205567453,
      "loss": 0.1041,
      "step": 583
    },
    {
      "epoch": 0.31271753681392234,
      "grad_norm": 5.833225727081299,
      "learning_rate": 0.00017919343326195573,
      "loss": 0.1214,
      "step": 584
    },
    {
      "epoch": 0.3132530120481928,
      "grad_norm": 1.5789378881454468,
      "learning_rate": 0.000179157744468237,
      "loss": 0.0768,
      "step": 585
    },
    {
      "epoch": 0.31378848728246317,
      "grad_norm": 1.8438360691070557,
      "learning_rate": 0.0001791220556745182,
      "loss": 0.1583,
      "step": 586
    },
    {
      "epoch": 0.3143239625167336,
      "grad_norm": 4.757740020751953,
      "learning_rate": 0.00017908636688079944,
      "loss": 0.2193,
      "step": 587
    },
    {
      "epoch": 0.314859437751004,
      "grad_norm": 7.258530139923096,
      "learning_rate": 0.00017905067808708067,
      "loss": 0.1812,
      "step": 588
    },
    {
      "epoch": 0.31539491298527444,
      "grad_norm": 1.5534452199935913,
      "learning_rate": 0.0001790149892933619,
      "loss": 0.1572,
      "step": 589
    },
    {
      "epoch": 0.31593038821954483,
      "grad_norm": 5.69083833694458,
      "learning_rate": 0.0001789793004996431,
      "loss": 0.2045,
      "step": 590
    },
    {
      "epoch": 0.3164658634538153,
      "grad_norm": 18.57505226135254,
      "learning_rate": 0.00017894361170592434,
      "loss": 0.2205,
      "step": 591
    },
    {
      "epoch": 0.31700133868808567,
      "grad_norm": 2.4306247234344482,
      "learning_rate": 0.00017890792291220558,
      "loss": 0.1524,
      "step": 592
    },
    {
      "epoch": 0.3175368139223561,
      "grad_norm": 2.127985954284668,
      "learning_rate": 0.0001788722341184868,
      "loss": 0.0723,
      "step": 593
    },
    {
      "epoch": 0.3180722891566265,
      "grad_norm": 13.854286193847656,
      "learning_rate": 0.00017883654532476804,
      "loss": 0.2116,
      "step": 594
    },
    {
      "epoch": 0.31860776439089694,
      "grad_norm": 2.6052682399749756,
      "learning_rate": 0.00017880085653104925,
      "loss": 0.1427,
      "step": 595
    },
    {
      "epoch": 0.31914323962516733,
      "grad_norm": 1.4454121589660645,
      "learning_rate": 0.0001787651677373305,
      "loss": 0.0649,
      "step": 596
    },
    {
      "epoch": 0.3196787148594378,
      "grad_norm": 4.66534948348999,
      "learning_rate": 0.00017872947894361172,
      "loss": 0.126,
      "step": 597
    },
    {
      "epoch": 0.32021419009370816,
      "grad_norm": 1.958592414855957,
      "learning_rate": 0.00017869379014989295,
      "loss": 0.1501,
      "step": 598
    },
    {
      "epoch": 0.3207496653279786,
      "grad_norm": 4.14042329788208,
      "learning_rate": 0.00017865810135617418,
      "loss": 0.1047,
      "step": 599
    },
    {
      "epoch": 0.321285140562249,
      "grad_norm": 3.018862009048462,
      "learning_rate": 0.0001786224125624554,
      "loss": 0.0618,
      "step": 600
    },
    {
      "epoch": 0.32182061579651944,
      "grad_norm": 3.215614080429077,
      "learning_rate": 0.00017858672376873662,
      "loss": 0.1167,
      "step": 601
    },
    {
      "epoch": 0.3223560910307898,
      "grad_norm": 1.394884705543518,
      "learning_rate": 0.00017855103497501786,
      "loss": 0.0708,
      "step": 602
    },
    {
      "epoch": 0.3228915662650602,
      "grad_norm": 2.211144208908081,
      "learning_rate": 0.0001785153461812991,
      "loss": 0.218,
      "step": 603
    },
    {
      "epoch": 0.32342704149933066,
      "grad_norm": 3.282731533050537,
      "learning_rate": 0.0001784796573875803,
      "loss": 0.2319,
      "step": 604
    },
    {
      "epoch": 0.32396251673360105,
      "grad_norm": 14.646381378173828,
      "learning_rate": 0.00017844396859386156,
      "loss": 0.3189,
      "step": 605
    },
    {
      "epoch": 0.3244979919678715,
      "grad_norm": 1.9372625350952148,
      "learning_rate": 0.00017840827980014276,
      "loss": 0.1549,
      "step": 606
    },
    {
      "epoch": 0.3250334672021419,
      "grad_norm": 1.2684221267700195,
      "learning_rate": 0.000178372591006424,
      "loss": 0.191,
      "step": 607
    },
    {
      "epoch": 0.3255689424364123,
      "grad_norm": 1.9410338401794434,
      "learning_rate": 0.00017833690221270523,
      "loss": 0.2107,
      "step": 608
    },
    {
      "epoch": 0.3261044176706827,
      "grad_norm": 2.4903061389923096,
      "learning_rate": 0.00017830121341898644,
      "loss": 0.1383,
      "step": 609
    },
    {
      "epoch": 0.32663989290495316,
      "grad_norm": 2.8646342754364014,
      "learning_rate": 0.00017826552462526767,
      "loss": 0.1515,
      "step": 610
    },
    {
      "epoch": 0.32717536813922354,
      "grad_norm": 1.9160404205322266,
      "learning_rate": 0.0001782298358315489,
      "loss": 0.1939,
      "step": 611
    },
    {
      "epoch": 0.327710843373494,
      "grad_norm": 0.6348035335540771,
      "learning_rate": 0.00017819414703783014,
      "loss": 0.1191,
      "step": 612
    },
    {
      "epoch": 0.3282463186077644,
      "grad_norm": 0.9433413147926331,
      "learning_rate": 0.00017815845824411134,
      "loss": 0.1166,
      "step": 613
    },
    {
      "epoch": 0.3287817938420348,
      "grad_norm": 0.9080603718757629,
      "learning_rate": 0.0001781227694503926,
      "loss": 0.1755,
      "step": 614
    },
    {
      "epoch": 0.3293172690763052,
      "grad_norm": 1.7093925476074219,
      "learning_rate": 0.0001780870806566738,
      "loss": 0.1219,
      "step": 615
    },
    {
      "epoch": 0.32985274431057565,
      "grad_norm": 1.5755239725112915,
      "learning_rate": 0.00017805139186295504,
      "loss": 0.1577,
      "step": 616
    },
    {
      "epoch": 0.33038821954484604,
      "grad_norm": 0.9120901823043823,
      "learning_rate": 0.00017801570306923628,
      "loss": 0.1295,
      "step": 617
    },
    {
      "epoch": 0.3309236947791165,
      "grad_norm": 1.7609049081802368,
      "learning_rate": 0.00017798001427551748,
      "loss": 0.0912,
      "step": 618
    },
    {
      "epoch": 0.3314591700133869,
      "grad_norm": 1.0396456718444824,
      "learning_rate": 0.00017794432548179872,
      "loss": 0.1556,
      "step": 619
    },
    {
      "epoch": 0.3319946452476573,
      "grad_norm": 0.6116577982902527,
      "learning_rate": 0.00017790863668807995,
      "loss": 0.0899,
      "step": 620
    },
    {
      "epoch": 0.3325301204819277,
      "grad_norm": 0.9467599987983704,
      "learning_rate": 0.00017787294789436118,
      "loss": 0.1731,
      "step": 621
    },
    {
      "epoch": 0.33306559571619815,
      "grad_norm": 1.384941816329956,
      "learning_rate": 0.00017783725910064242,
      "loss": 0.1698,
      "step": 622
    },
    {
      "epoch": 0.33360107095046854,
      "grad_norm": 0.655586302280426,
      "learning_rate": 0.00017780157030692365,
      "loss": 0.1006,
      "step": 623
    },
    {
      "epoch": 0.334136546184739,
      "grad_norm": 0.6051927208900452,
      "learning_rate": 0.00017776588151320486,
      "loss": 0.0735,
      "step": 624
    },
    {
      "epoch": 0.33467202141900937,
      "grad_norm": 5.896318435668945,
      "learning_rate": 0.0001777301927194861,
      "loss": 0.1587,
      "step": 625
    },
    {
      "epoch": 0.3352074966532798,
      "grad_norm": 0.8048863410949707,
      "learning_rate": 0.00017769450392576732,
      "loss": 0.0642,
      "step": 626
    },
    {
      "epoch": 0.3357429718875502,
      "grad_norm": 0.9707651734352112,
      "learning_rate": 0.00017765881513204853,
      "loss": 0.0857,
      "step": 627
    },
    {
      "epoch": 0.3362784471218206,
      "grad_norm": 3.5701613426208496,
      "learning_rate": 0.0001776231263383298,
      "loss": 0.1215,
      "step": 628
    },
    {
      "epoch": 0.33681392235609103,
      "grad_norm": 1.821756362915039,
      "learning_rate": 0.000177587437544611,
      "loss": 0.227,
      "step": 629
    },
    {
      "epoch": 0.3373493975903614,
      "grad_norm": 4.739150047302246,
      "learning_rate": 0.00017755174875089223,
      "loss": 0.1652,
      "step": 630
    },
    {
      "epoch": 0.33788487282463187,
      "grad_norm": 1.913421630859375,
      "learning_rate": 0.00017751605995717346,
      "loss": 0.1172,
      "step": 631
    },
    {
      "epoch": 0.33842034805890225,
      "grad_norm": 3.535491466522217,
      "learning_rate": 0.0001774803711634547,
      "loss": 0.1347,
      "step": 632
    },
    {
      "epoch": 0.3389558232931727,
      "grad_norm": 4.2155375480651855,
      "learning_rate": 0.0001774446823697359,
      "loss": 0.0958,
      "step": 633
    },
    {
      "epoch": 0.3394912985274431,
      "grad_norm": 1.238518476486206,
      "learning_rate": 0.00017740899357601714,
      "loss": 0.1062,
      "step": 634
    },
    {
      "epoch": 0.34002677376171353,
      "grad_norm": 5.229771137237549,
      "learning_rate": 0.00017737330478229837,
      "loss": 0.1375,
      "step": 635
    },
    {
      "epoch": 0.3405622489959839,
      "grad_norm": 2.726046323776245,
      "learning_rate": 0.00017733761598857957,
      "loss": 0.254,
      "step": 636
    },
    {
      "epoch": 0.34109772423025436,
      "grad_norm": 3.974184989929199,
      "learning_rate": 0.00017730192719486084,
      "loss": 0.164,
      "step": 637
    },
    {
      "epoch": 0.34163319946452475,
      "grad_norm": 2.5472123622894287,
      "learning_rate": 0.00017726623840114204,
      "loss": 0.2111,
      "step": 638
    },
    {
      "epoch": 0.3421686746987952,
      "grad_norm": 8.759563446044922,
      "learning_rate": 0.00017723054960742328,
      "loss": 0.3075,
      "step": 639
    },
    {
      "epoch": 0.3427041499330656,
      "grad_norm": 5.322810173034668,
      "learning_rate": 0.0001771948608137045,
      "loss": 0.2487,
      "step": 640
    },
    {
      "epoch": 0.343239625167336,
      "grad_norm": 1.571706771850586,
      "learning_rate": 0.00017715917201998574,
      "loss": 0.2189,
      "step": 641
    },
    {
      "epoch": 0.3437751004016064,
      "grad_norm": 12.417301177978516,
      "learning_rate": 0.00017712348322626695,
      "loss": 0.2849,
      "step": 642
    },
    {
      "epoch": 0.34431057563587686,
      "grad_norm": 1.7710883617401123,
      "learning_rate": 0.00017708779443254818,
      "loss": 0.1588,
      "step": 643
    },
    {
      "epoch": 0.34484605087014725,
      "grad_norm": 13.12744140625,
      "learning_rate": 0.00017705210563882942,
      "loss": 0.0872,
      "step": 644
    },
    {
      "epoch": 0.3453815261044177,
      "grad_norm": 0.8657239079475403,
      "learning_rate": 0.00017701641684511062,
      "loss": 0.1454,
      "step": 645
    },
    {
      "epoch": 0.3459170013386881,
      "grad_norm": 1.259037733078003,
      "learning_rate": 0.00017698072805139188,
      "loss": 0.1647,
      "step": 646
    },
    {
      "epoch": 0.3464524765729585,
      "grad_norm": 2.7826836109161377,
      "learning_rate": 0.0001769450392576731,
      "loss": 0.1737,
      "step": 647
    },
    {
      "epoch": 0.3469879518072289,
      "grad_norm": 3.1919493675231934,
      "learning_rate": 0.00017690935046395435,
      "loss": 0.1814,
      "step": 648
    },
    {
      "epoch": 0.34752342704149936,
      "grad_norm": 4.905024528503418,
      "learning_rate": 0.00017687366167023556,
      "loss": 0.0612,
      "step": 649
    },
    {
      "epoch": 0.34805890227576974,
      "grad_norm": 5.589320182800293,
      "learning_rate": 0.0001768379728765168,
      "loss": 0.3526,
      "step": 650
    },
    {
      "epoch": 0.3485943775100402,
      "grad_norm": 2.001788854598999,
      "learning_rate": 0.00017680228408279802,
      "loss": 0.1199,
      "step": 651
    },
    {
      "epoch": 0.3491298527443106,
      "grad_norm": 2.1918673515319824,
      "learning_rate": 0.00017676659528907923,
      "loss": 0.064,
      "step": 652
    },
    {
      "epoch": 0.34966532797858096,
      "grad_norm": 0.9506222009658813,
      "learning_rate": 0.00017673090649536046,
      "loss": 0.1827,
      "step": 653
    },
    {
      "epoch": 0.3502008032128514,
      "grad_norm": 0.8994435667991638,
      "learning_rate": 0.0001766952177016417,
      "loss": 0.1957,
      "step": 654
    },
    {
      "epoch": 0.3507362784471218,
      "grad_norm": 1.348791241645813,
      "learning_rate": 0.00017665952890792293,
      "loss": 0.0802,
      "step": 655
    },
    {
      "epoch": 0.35127175368139224,
      "grad_norm": 0.7296037077903748,
      "learning_rate": 0.00017662384011420413,
      "loss": 0.1389,
      "step": 656
    },
    {
      "epoch": 0.35180722891566263,
      "grad_norm": 11.214712142944336,
      "learning_rate": 0.0001765881513204854,
      "loss": 0.1726,
      "step": 657
    },
    {
      "epoch": 0.3523427041499331,
      "grad_norm": 0.8398928046226501,
      "learning_rate": 0.0001765524625267666,
      "loss": 0.1835,
      "step": 658
    },
    {
      "epoch": 0.35287817938420346,
      "grad_norm": 1.3154840469360352,
      "learning_rate": 0.00017651677373304784,
      "loss": 0.1294,
      "step": 659
    },
    {
      "epoch": 0.3534136546184739,
      "grad_norm": 0.6297398209571838,
      "learning_rate": 0.00017648108493932907,
      "loss": 0.1215,
      "step": 660
    },
    {
      "epoch": 0.3539491298527443,
      "grad_norm": 0.6292582154273987,
      "learning_rate": 0.00017644539614561027,
      "loss": 0.0734,
      "step": 661
    },
    {
      "epoch": 0.35448460508701474,
      "grad_norm": 5.946977138519287,
      "learning_rate": 0.0001764097073518915,
      "loss": 0.236,
      "step": 662
    },
    {
      "epoch": 0.3550200803212851,
      "grad_norm": 4.620030403137207,
      "learning_rate": 0.00017637401855817274,
      "loss": 0.1877,
      "step": 663
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 1.8523378372192383,
      "learning_rate": 0.00017633832976445398,
      "loss": 0.1219,
      "step": 664
    },
    {
      "epoch": 0.35609103078982596,
      "grad_norm": 2.2492082118988037,
      "learning_rate": 0.00017630264097073518,
      "loss": 0.141,
      "step": 665
    },
    {
      "epoch": 0.3566265060240964,
      "grad_norm": 1.977793574333191,
      "learning_rate": 0.00017626695217701644,
      "loss": 0.2239,
      "step": 666
    },
    {
      "epoch": 0.3571619812583668,
      "grad_norm": 2.5403552055358887,
      "learning_rate": 0.00017623126338329765,
      "loss": 0.0942,
      "step": 667
    },
    {
      "epoch": 0.35769745649263723,
      "grad_norm": 4.170276165008545,
      "learning_rate": 0.00017619557458957888,
      "loss": 0.127,
      "step": 668
    },
    {
      "epoch": 0.3582329317269076,
      "grad_norm": 0.7379599213600159,
      "learning_rate": 0.00017615988579586012,
      "loss": 0.1293,
      "step": 669
    },
    {
      "epoch": 0.35876840696117807,
      "grad_norm": 0.9787030816078186,
      "learning_rate": 0.00017612419700214132,
      "loss": 0.1672,
      "step": 670
    },
    {
      "epoch": 0.35930388219544845,
      "grad_norm": 0.6885220408439636,
      "learning_rate": 0.00017608850820842255,
      "loss": 0.1342,
      "step": 671
    },
    {
      "epoch": 0.3598393574297189,
      "grad_norm": 1.1937448978424072,
      "learning_rate": 0.0001760528194147038,
      "loss": 0.1666,
      "step": 672
    },
    {
      "epoch": 0.3603748326639893,
      "grad_norm": 1.0320473909378052,
      "learning_rate": 0.00017601713062098502,
      "loss": 0.107,
      "step": 673
    },
    {
      "epoch": 0.36091030789825973,
      "grad_norm": 1.9903218746185303,
      "learning_rate": 0.00017598144182726623,
      "loss": 0.158,
      "step": 674
    },
    {
      "epoch": 0.3614457831325301,
      "grad_norm": 0.7199269533157349,
      "learning_rate": 0.0001759457530335475,
      "loss": 0.1833,
      "step": 675
    },
    {
      "epoch": 0.36198125836680056,
      "grad_norm": 3.513608694076538,
      "learning_rate": 0.0001759100642398287,
      "loss": 0.2517,
      "step": 676
    },
    {
      "epoch": 0.36251673360107095,
      "grad_norm": 0.5055010914802551,
      "learning_rate": 0.00017587437544610993,
      "loss": 0.1073,
      "step": 677
    },
    {
      "epoch": 0.36305220883534134,
      "grad_norm": 0.4480116069316864,
      "learning_rate": 0.00017583868665239116,
      "loss": 0.0727,
      "step": 678
    },
    {
      "epoch": 0.3635876840696118,
      "grad_norm": 5.699085712432861,
      "learning_rate": 0.00017580299785867237,
      "loss": 0.1065,
      "step": 679
    },
    {
      "epoch": 0.36412315930388217,
      "grad_norm": 0.6674807667732239,
      "learning_rate": 0.00017576730906495363,
      "loss": 0.1388,
      "step": 680
    },
    {
      "epoch": 0.3646586345381526,
      "grad_norm": 1.289737343788147,
      "learning_rate": 0.00017573162027123483,
      "loss": 0.1925,
      "step": 681
    },
    {
      "epoch": 0.365194109772423,
      "grad_norm": 5.509997367858887,
      "learning_rate": 0.00017569593147751607,
      "loss": 0.1813,
      "step": 682
    },
    {
      "epoch": 0.36572958500669345,
      "grad_norm": 0.594009518623352,
      "learning_rate": 0.0001756602426837973,
      "loss": 0.0939,
      "step": 683
    },
    {
      "epoch": 0.36626506024096384,
      "grad_norm": 0.9984255433082581,
      "learning_rate": 0.00017562455389007854,
      "loss": 0.1668,
      "step": 684
    },
    {
      "epoch": 0.3668005354752343,
      "grad_norm": 5.079586982727051,
      "learning_rate": 0.00017558886509635974,
      "loss": 0.1868,
      "step": 685
    },
    {
      "epoch": 0.36733601070950467,
      "grad_norm": 0.8583349585533142,
      "learning_rate": 0.00017555317630264098,
      "loss": 0.0951,
      "step": 686
    },
    {
      "epoch": 0.3678714859437751,
      "grad_norm": 3.0019021034240723,
      "learning_rate": 0.0001755174875089222,
      "loss": 0.1357,
      "step": 687
    },
    {
      "epoch": 0.3684069611780455,
      "grad_norm": 0.6659912467002869,
      "learning_rate": 0.00017548179871520341,
      "loss": 0.0628,
      "step": 688
    },
    {
      "epoch": 0.36894243641231594,
      "grad_norm": 1.505089521408081,
      "learning_rate": 0.00017544610992148468,
      "loss": 0.1084,
      "step": 689
    },
    {
      "epoch": 0.36947791164658633,
      "grad_norm": 0.9050391912460327,
      "learning_rate": 0.00017541042112776588,
      "loss": 0.1289,
      "step": 690
    },
    {
      "epoch": 0.3700133868808568,
      "grad_norm": 0.5997455716133118,
      "learning_rate": 0.00017537473233404712,
      "loss": 0.1691,
      "step": 691
    },
    {
      "epoch": 0.37054886211512716,
      "grad_norm": 1.3244518041610718,
      "learning_rate": 0.00017533904354032835,
      "loss": 0.134,
      "step": 692
    },
    {
      "epoch": 0.3710843373493976,
      "grad_norm": 0.8110635280609131,
      "learning_rate": 0.00017530335474660958,
      "loss": 0.1582,
      "step": 693
    },
    {
      "epoch": 0.371619812583668,
      "grad_norm": 1.067142367362976,
      "learning_rate": 0.0001752676659528908,
      "loss": 0.1636,
      "step": 694
    },
    {
      "epoch": 0.37215528781793844,
      "grad_norm": 0.6556412577629089,
      "learning_rate": 0.00017523197715917202,
      "loss": 0.0823,
      "step": 695
    },
    {
      "epoch": 0.37269076305220883,
      "grad_norm": 0.37140917778015137,
      "learning_rate": 0.00017519628836545326,
      "loss": 0.0657,
      "step": 696
    },
    {
      "epoch": 0.3732262382864793,
      "grad_norm": 3.192108154296875,
      "learning_rate": 0.00017516059957173446,
      "loss": 0.1255,
      "step": 697
    },
    {
      "epoch": 0.37376171352074966,
      "grad_norm": 0.4737117290496826,
      "learning_rate": 0.00017512491077801572,
      "loss": 0.0376,
      "step": 698
    },
    {
      "epoch": 0.3742971887550201,
      "grad_norm": 4.153564929962158,
      "learning_rate": 0.00017508922198429693,
      "loss": 0.2452,
      "step": 699
    },
    {
      "epoch": 0.3748326639892905,
      "grad_norm": 2.663621187210083,
      "learning_rate": 0.00017505353319057816,
      "loss": 0.0694,
      "step": 700
    },
    {
      "epoch": 0.37536813922356094,
      "grad_norm": 3.9853785037994385,
      "learning_rate": 0.0001750178443968594,
      "loss": 0.3479,
      "step": 701
    },
    {
      "epoch": 0.3759036144578313,
      "grad_norm": 10.378788948059082,
      "learning_rate": 0.00017498215560314063,
      "loss": 0.3812,
      "step": 702
    },
    {
      "epoch": 0.3764390896921017,
      "grad_norm": 2.481506109237671,
      "learning_rate": 0.00017494646680942186,
      "loss": 0.1217,
      "step": 703
    },
    {
      "epoch": 0.37697456492637216,
      "grad_norm": 0.8363668322563171,
      "learning_rate": 0.00017491077801570307,
      "loss": 0.087,
      "step": 704
    },
    {
      "epoch": 0.37751004016064255,
      "grad_norm": 1.747667670249939,
      "learning_rate": 0.0001748750892219843,
      "loss": 0.1405,
      "step": 705
    },
    {
      "epoch": 0.378045515394913,
      "grad_norm": 1.4387741088867188,
      "learning_rate": 0.00017483940042826554,
      "loss": 0.1295,
      "step": 706
    },
    {
      "epoch": 0.3785809906291834,
      "grad_norm": 3.896448850631714,
      "learning_rate": 0.00017480371163454677,
      "loss": 0.1746,
      "step": 707
    },
    {
      "epoch": 0.3791164658634538,
      "grad_norm": 1.850196123123169,
      "learning_rate": 0.00017476802284082797,
      "loss": 0.145,
      "step": 708
    },
    {
      "epoch": 0.3796519410977242,
      "grad_norm": 0.963660478591919,
      "learning_rate": 0.00017473233404710924,
      "loss": 0.1273,
      "step": 709
    },
    {
      "epoch": 0.38018741633199465,
      "grad_norm": 4.404514312744141,
      "learning_rate": 0.00017469664525339044,
      "loss": 0.1184,
      "step": 710
    },
    {
      "epoch": 0.38072289156626504,
      "grad_norm": 1.3137016296386719,
      "learning_rate": 0.00017466095645967168,
      "loss": 0.0878,
      "step": 711
    },
    {
      "epoch": 0.3812583668005355,
      "grad_norm": 1.56630539894104,
      "learning_rate": 0.0001746252676659529,
      "loss": 0.1602,
      "step": 712
    },
    {
      "epoch": 0.3817938420348059,
      "grad_norm": 0.4891592860221863,
      "learning_rate": 0.00017458957887223411,
      "loss": 0.1012,
      "step": 713
    },
    {
      "epoch": 0.3823293172690763,
      "grad_norm": 2.7051382064819336,
      "learning_rate": 0.00017455389007851535,
      "loss": 0.1387,
      "step": 714
    },
    {
      "epoch": 0.3828647925033467,
      "grad_norm": 0.4023452401161194,
      "learning_rate": 0.00017451820128479658,
      "loss": 0.1283,
      "step": 715
    },
    {
      "epoch": 0.38340026773761715,
      "grad_norm": 1.3173460960388184,
      "learning_rate": 0.00017448251249107782,
      "loss": 0.1841,
      "step": 716
    },
    {
      "epoch": 0.38393574297188754,
      "grad_norm": 0.4401835799217224,
      "learning_rate": 0.00017444682369735902,
      "loss": 0.0492,
      "step": 717
    },
    {
      "epoch": 0.384471218206158,
      "grad_norm": 3.152376651763916,
      "learning_rate": 0.00017441113490364028,
      "loss": 0.1217,
      "step": 718
    },
    {
      "epoch": 0.38500669344042837,
      "grad_norm": 3.0957915782928467,
      "learning_rate": 0.0001743754461099215,
      "loss": 0.1931,
      "step": 719
    },
    {
      "epoch": 0.3855421686746988,
      "grad_norm": 1.1770853996276855,
      "learning_rate": 0.00017433975731620272,
      "loss": 0.1223,
      "step": 720
    },
    {
      "epoch": 0.3860776439089692,
      "grad_norm": 2.547041177749634,
      "learning_rate": 0.00017430406852248396,
      "loss": 0.0869,
      "step": 721
    },
    {
      "epoch": 0.38661311914323965,
      "grad_norm": 1.722217082977295,
      "learning_rate": 0.00017426837972876516,
      "loss": 0.2044,
      "step": 722
    },
    {
      "epoch": 0.38714859437751004,
      "grad_norm": 6.043761730194092,
      "learning_rate": 0.0001742326909350464,
      "loss": 0.0967,
      "step": 723
    },
    {
      "epoch": 0.3876840696117805,
      "grad_norm": 1.952458381652832,
      "learning_rate": 0.00017419700214132763,
      "loss": 0.2926,
      "step": 724
    },
    {
      "epoch": 0.38821954484605087,
      "grad_norm": 1.4970613718032837,
      "learning_rate": 0.00017416131334760886,
      "loss": 0.1342,
      "step": 725
    },
    {
      "epoch": 0.3887550200803213,
      "grad_norm": 2.0344786643981934,
      "learning_rate": 0.00017412562455389007,
      "loss": 0.0865,
      "step": 726
    },
    {
      "epoch": 0.3892904953145917,
      "grad_norm": 4.284550189971924,
      "learning_rate": 0.00017408993576017133,
      "loss": 0.1251,
      "step": 727
    },
    {
      "epoch": 0.3898259705488621,
      "grad_norm": 3.933586597442627,
      "learning_rate": 0.00017405424696645253,
      "loss": 0.0818,
      "step": 728
    },
    {
      "epoch": 0.39036144578313253,
      "grad_norm": 2.3806169033050537,
      "learning_rate": 0.00017401855817273377,
      "loss": 0.1715,
      "step": 729
    },
    {
      "epoch": 0.3908969210174029,
      "grad_norm": 10.426216125488281,
      "learning_rate": 0.000173982869379015,
      "loss": 0.2509,
      "step": 730
    },
    {
      "epoch": 0.39143239625167336,
      "grad_norm": 1.3079510927200317,
      "learning_rate": 0.0001739471805852962,
      "loss": 0.1014,
      "step": 731
    },
    {
      "epoch": 0.39196787148594375,
      "grad_norm": 4.391970157623291,
      "learning_rate": 0.00017391149179157747,
      "loss": 0.1837,
      "step": 732
    },
    {
      "epoch": 0.3925033467202142,
      "grad_norm": 3.2608327865600586,
      "learning_rate": 0.00017387580299785867,
      "loss": 0.1714,
      "step": 733
    },
    {
      "epoch": 0.3930388219544846,
      "grad_norm": 0.8762997984886169,
      "learning_rate": 0.0001738401142041399,
      "loss": 0.0983,
      "step": 734
    },
    {
      "epoch": 0.39357429718875503,
      "grad_norm": 0.27880212664604187,
      "learning_rate": 0.00017380442541042114,
      "loss": 0.0375,
      "step": 735
    },
    {
      "epoch": 0.3941097724230254,
      "grad_norm": 8.114669799804688,
      "learning_rate": 0.00017376873661670238,
      "loss": 0.167,
      "step": 736
    },
    {
      "epoch": 0.39464524765729586,
      "grad_norm": 1.5902076959609985,
      "learning_rate": 0.00017373304782298358,
      "loss": 0.1544,
      "step": 737
    },
    {
      "epoch": 0.39518072289156625,
      "grad_norm": 0.4042101502418518,
      "learning_rate": 0.00017369735902926481,
      "loss": 0.0594,
      "step": 738
    },
    {
      "epoch": 0.3957161981258367,
      "grad_norm": 2.608623743057251,
      "learning_rate": 0.00017366167023554605,
      "loss": 0.1117,
      "step": 739
    },
    {
      "epoch": 0.3962516733601071,
      "grad_norm": 0.9007276892662048,
      "learning_rate": 0.00017362598144182725,
      "loss": 0.1458,
      "step": 740
    },
    {
      "epoch": 0.3967871485943775,
      "grad_norm": 2.8510794639587402,
      "learning_rate": 0.00017359029264810852,
      "loss": 0.0915,
      "step": 741
    },
    {
      "epoch": 0.3973226238286479,
      "grad_norm": 0.838503360748291,
      "learning_rate": 0.00017355460385438972,
      "loss": 0.0672,
      "step": 742
    },
    {
      "epoch": 0.39785809906291836,
      "grad_norm": 1.4079549312591553,
      "learning_rate": 0.00017351891506067095,
      "loss": 0.067,
      "step": 743
    },
    {
      "epoch": 0.39839357429718875,
      "grad_norm": 0.4061524569988251,
      "learning_rate": 0.0001734832262669522,
      "loss": 0.098,
      "step": 744
    },
    {
      "epoch": 0.3989290495314592,
      "grad_norm": 1.8438242673873901,
      "learning_rate": 0.00017344753747323342,
      "loss": 0.096,
      "step": 745
    },
    {
      "epoch": 0.3994645247657296,
      "grad_norm": 1.6496548652648926,
      "learning_rate": 0.00017341184867951463,
      "loss": 0.1544,
      "step": 746
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.5197573304176331,
      "learning_rate": 0.00017337615988579586,
      "loss": 0.0778,
      "step": 747
    },
    {
      "epoch": 0.4005354752342704,
      "grad_norm": 4.808170318603516,
      "learning_rate": 0.0001733404710920771,
      "loss": 0.1524,
      "step": 748
    },
    {
      "epoch": 0.40107095046854085,
      "grad_norm": 0.6897441148757935,
      "learning_rate": 0.0001733047822983583,
      "loss": 0.0758,
      "step": 749
    },
    {
      "epoch": 0.40160642570281124,
      "grad_norm": 0.5945813655853271,
      "learning_rate": 0.00017326909350463956,
      "loss": 0.1067,
      "step": 750
    },
    {
      "epoch": 0.4021419009370817,
      "grad_norm": 1.272286295890808,
      "learning_rate": 0.00017323340471092077,
      "loss": 0.1464,
      "step": 751
    },
    {
      "epoch": 0.4026773761713521,
      "grad_norm": 0.9177097678184509,
      "learning_rate": 0.000173197715917202,
      "loss": 0.133,
      "step": 752
    },
    {
      "epoch": 0.40321285140562246,
      "grad_norm": 0.26387494802474976,
      "learning_rate": 0.00017316202712348323,
      "loss": 0.0228,
      "step": 753
    },
    {
      "epoch": 0.4037483266398929,
      "grad_norm": 0.7774226665496826,
      "learning_rate": 0.00017312633832976447,
      "loss": 0.0992,
      "step": 754
    },
    {
      "epoch": 0.4042838018741633,
      "grad_norm": 0.9774786233901978,
      "learning_rate": 0.00017309064953604567,
      "loss": 0.0406,
      "step": 755
    },
    {
      "epoch": 0.40481927710843374,
      "grad_norm": 0.5911362767219543,
      "learning_rate": 0.0001730549607423269,
      "loss": 0.0716,
      "step": 756
    },
    {
      "epoch": 0.4053547523427041,
      "grad_norm": 0.7810758948326111,
      "learning_rate": 0.00017301927194860814,
      "loss": 0.0949,
      "step": 757
    },
    {
      "epoch": 0.40589022757697457,
      "grad_norm": 3.7125043869018555,
      "learning_rate": 0.00017298358315488937,
      "loss": 0.1871,
      "step": 758
    },
    {
      "epoch": 0.40642570281124496,
      "grad_norm": 7.119696617126465,
      "learning_rate": 0.0001729478943611706,
      "loss": 0.1984,
      "step": 759
    },
    {
      "epoch": 0.4069611780455154,
      "grad_norm": 2.4235379695892334,
      "learning_rate": 0.00017291220556745181,
      "loss": 0.1474,
      "step": 760
    },
    {
      "epoch": 0.4074966532797858,
      "grad_norm": 2.1948485374450684,
      "learning_rate": 0.00017287651677373308,
      "loss": 0.0969,
      "step": 761
    },
    {
      "epoch": 0.40803212851405624,
      "grad_norm": 2.0244107246398926,
      "learning_rate": 0.00017284082798001428,
      "loss": 0.1475,
      "step": 762
    },
    {
      "epoch": 0.4085676037483266,
      "grad_norm": 3.4916751384735107,
      "learning_rate": 0.00017280513918629552,
      "loss": 0.2104,
      "step": 763
    },
    {
      "epoch": 0.40910307898259707,
      "grad_norm": 1.8688026666641235,
      "learning_rate": 0.00017276945039257675,
      "loss": 0.1203,
      "step": 764
    },
    {
      "epoch": 0.40963855421686746,
      "grad_norm": 1.429023265838623,
      "learning_rate": 0.00017273376159885795,
      "loss": 0.1337,
      "step": 765
    },
    {
      "epoch": 0.4101740294511379,
      "grad_norm": 1.0333517789840698,
      "learning_rate": 0.0001726980728051392,
      "loss": 0.1125,
      "step": 766
    },
    {
      "epoch": 0.4107095046854083,
      "grad_norm": 4.869702339172363,
      "learning_rate": 0.00017266238401142042,
      "loss": 0.2522,
      "step": 767
    },
    {
      "epoch": 0.41124497991967873,
      "grad_norm": 3.622077226638794,
      "learning_rate": 0.00017262669521770166,
      "loss": 0.0817,
      "step": 768
    },
    {
      "epoch": 0.4117804551539491,
      "grad_norm": 2.3154358863830566,
      "learning_rate": 0.00017259100642398286,
      "loss": 0.1359,
      "step": 769
    },
    {
      "epoch": 0.41231593038821956,
      "grad_norm": 4.628772735595703,
      "learning_rate": 0.00017255531763026412,
      "loss": 0.2058,
      "step": 770
    },
    {
      "epoch": 0.41285140562248995,
      "grad_norm": 1.031954050064087,
      "learning_rate": 0.00017251962883654533,
      "loss": 0.1197,
      "step": 771
    },
    {
      "epoch": 0.4133868808567604,
      "grad_norm": 0.9409729838371277,
      "learning_rate": 0.00017248394004282656,
      "loss": 0.1121,
      "step": 772
    },
    {
      "epoch": 0.4139223560910308,
      "grad_norm": 1.3995572328567505,
      "learning_rate": 0.0001724482512491078,
      "loss": 0.0797,
      "step": 773
    },
    {
      "epoch": 0.41445783132530123,
      "grad_norm": 1.6793413162231445,
      "learning_rate": 0.00017241256245538903,
      "loss": 0.1477,
      "step": 774
    },
    {
      "epoch": 0.4149933065595716,
      "grad_norm": 0.7362723350524902,
      "learning_rate": 0.00017237687366167023,
      "loss": 0.131,
      "step": 775
    },
    {
      "epoch": 0.41552878179384206,
      "grad_norm": 0.7505921721458435,
      "learning_rate": 0.00017234118486795147,
      "loss": 0.0839,
      "step": 776
    },
    {
      "epoch": 0.41606425702811245,
      "grad_norm": 3.427536725997925,
      "learning_rate": 0.0001723054960742327,
      "loss": 0.1061,
      "step": 777
    },
    {
      "epoch": 0.41659973226238284,
      "grad_norm": 1.4422333240509033,
      "learning_rate": 0.0001722698072805139,
      "loss": 0.1695,
      "step": 778
    },
    {
      "epoch": 0.4171352074966533,
      "grad_norm": 4.072596549987793,
      "learning_rate": 0.00017223411848679517,
      "loss": 0.228,
      "step": 779
    },
    {
      "epoch": 0.41767068273092367,
      "grad_norm": 1.823643445968628,
      "learning_rate": 0.00017219842969307637,
      "loss": 0.1934,
      "step": 780
    },
    {
      "epoch": 0.4182061579651941,
      "grad_norm": 1.6379116773605347,
      "learning_rate": 0.0001721627408993576,
      "loss": 0.0589,
      "step": 781
    },
    {
      "epoch": 0.4187416331994645,
      "grad_norm": 1.9762417078018188,
      "learning_rate": 0.00017212705210563884,
      "loss": 0.1551,
      "step": 782
    },
    {
      "epoch": 0.41927710843373495,
      "grad_norm": 3.5922980308532715,
      "learning_rate": 0.00017209136331192008,
      "loss": 0.1451,
      "step": 783
    },
    {
      "epoch": 0.41981258366800533,
      "grad_norm": 0.8924561142921448,
      "learning_rate": 0.0001720556745182013,
      "loss": 0.166,
      "step": 784
    },
    {
      "epoch": 0.4203480589022758,
      "grad_norm": 0.7753691673278809,
      "learning_rate": 0.00017201998572448251,
      "loss": 0.0712,
      "step": 785
    },
    {
      "epoch": 0.42088353413654617,
      "grad_norm": 0.5888636708259583,
      "learning_rate": 0.00017198429693076375,
      "loss": 0.0472,
      "step": 786
    },
    {
      "epoch": 0.4214190093708166,
      "grad_norm": 0.8191279768943787,
      "learning_rate": 0.00017194860813704498,
      "loss": 0.1021,
      "step": 787
    },
    {
      "epoch": 0.421954484605087,
      "grad_norm": 0.4499284625053406,
      "learning_rate": 0.00017191291934332622,
      "loss": 0.0454,
      "step": 788
    },
    {
      "epoch": 0.42248995983935744,
      "grad_norm": 2.046386957168579,
      "learning_rate": 0.00017187723054960742,
      "loss": 0.2054,
      "step": 789
    },
    {
      "epoch": 0.42302543507362783,
      "grad_norm": 0.968421995639801,
      "learning_rate": 0.00017184154175588868,
      "loss": 0.1252,
      "step": 790
    },
    {
      "epoch": 0.4235609103078983,
      "grad_norm": 0.3952607214450836,
      "learning_rate": 0.0001718058529621699,
      "loss": 0.0629,
      "step": 791
    },
    {
      "epoch": 0.42409638554216866,
      "grad_norm": 20.829418182373047,
      "learning_rate": 0.00017177016416845112,
      "loss": 0.2121,
      "step": 792
    },
    {
      "epoch": 0.4246318607764391,
      "grad_norm": 3.6366419792175293,
      "learning_rate": 0.00017173447537473236,
      "loss": 0.2947,
      "step": 793
    },
    {
      "epoch": 0.4251673360107095,
      "grad_norm": 2.147228240966797,
      "learning_rate": 0.00017169878658101356,
      "loss": 0.1721,
      "step": 794
    },
    {
      "epoch": 0.42570281124497994,
      "grad_norm": 0.5299621224403381,
      "learning_rate": 0.0001716630977872948,
      "loss": 0.1333,
      "step": 795
    },
    {
      "epoch": 0.42623828647925033,
      "grad_norm": 0.9329144358634949,
      "learning_rate": 0.00017162740899357603,
      "loss": 0.0885,
      "step": 796
    },
    {
      "epoch": 0.42677376171352077,
      "grad_norm": 1.1471900939941406,
      "learning_rate": 0.00017159172019985726,
      "loss": 0.0404,
      "step": 797
    },
    {
      "epoch": 0.42730923694779116,
      "grad_norm": 4.9719157218933105,
      "learning_rate": 0.00017155603140613847,
      "loss": 0.2199,
      "step": 798
    },
    {
      "epoch": 0.4278447121820616,
      "grad_norm": 1.2442282438278198,
      "learning_rate": 0.00017152034261241973,
      "loss": 0.1996,
      "step": 799
    },
    {
      "epoch": 0.428380187416332,
      "grad_norm": 0.839857280254364,
      "learning_rate": 0.00017148465381870093,
      "loss": 0.0821,
      "step": 800
    },
    {
      "epoch": 0.42891566265060244,
      "grad_norm": 9.98162841796875,
      "learning_rate": 0.00017144896502498217,
      "loss": 0.2085,
      "step": 801
    },
    {
      "epoch": 0.4294511378848728,
      "grad_norm": 1.5822111368179321,
      "learning_rate": 0.0001714132762312634,
      "loss": 0.1475,
      "step": 802
    },
    {
      "epoch": 0.4299866131191432,
      "grad_norm": 1.3628437519073486,
      "learning_rate": 0.0001713775874375446,
      "loss": 0.1523,
      "step": 803
    },
    {
      "epoch": 0.43052208835341366,
      "grad_norm": 1.1582667827606201,
      "learning_rate": 0.00017134189864382584,
      "loss": 0.0598,
      "step": 804
    },
    {
      "epoch": 0.43105756358768405,
      "grad_norm": 0.9502947926521301,
      "learning_rate": 0.00017130620985010707,
      "loss": 0.084,
      "step": 805
    },
    {
      "epoch": 0.4315930388219545,
      "grad_norm": 0.9126331806182861,
      "learning_rate": 0.0001712705210563883,
      "loss": 0.0627,
      "step": 806
    },
    {
      "epoch": 0.4321285140562249,
      "grad_norm": 4.491268634796143,
      "learning_rate": 0.00017123483226266951,
      "loss": 0.352,
      "step": 807
    },
    {
      "epoch": 0.4326639892904953,
      "grad_norm": 2.2848007678985596,
      "learning_rate": 0.00017119914346895078,
      "loss": 0.1129,
      "step": 808
    },
    {
      "epoch": 0.4331994645247657,
      "grad_norm": 2.040295124053955,
      "learning_rate": 0.00017116345467523198,
      "loss": 0.1861,
      "step": 809
    },
    {
      "epoch": 0.43373493975903615,
      "grad_norm": 1.7476600408554077,
      "learning_rate": 0.00017112776588151321,
      "loss": 0.1492,
      "step": 810
    },
    {
      "epoch": 0.43427041499330654,
      "grad_norm": 1.4168205261230469,
      "learning_rate": 0.00017109207708779445,
      "loss": 0.0802,
      "step": 811
    },
    {
      "epoch": 0.434805890227577,
      "grad_norm": 5.124238967895508,
      "learning_rate": 0.00017105638829407565,
      "loss": 0.1737,
      "step": 812
    },
    {
      "epoch": 0.4353413654618474,
      "grad_norm": 1.0725140571594238,
      "learning_rate": 0.00017102069950035692,
      "loss": 0.1166,
      "step": 813
    },
    {
      "epoch": 0.4358768406961178,
      "grad_norm": 0.8404147028923035,
      "learning_rate": 0.00017098501070663812,
      "loss": 0.1507,
      "step": 814
    },
    {
      "epoch": 0.4364123159303882,
      "grad_norm": 2.2930774688720703,
      "learning_rate": 0.00017094932191291935,
      "loss": 0.1402,
      "step": 815
    },
    {
      "epoch": 0.43694779116465865,
      "grad_norm": 3.1657426357269287,
      "learning_rate": 0.0001709136331192006,
      "loss": 0.2237,
      "step": 816
    },
    {
      "epoch": 0.43748326639892904,
      "grad_norm": 3.7619173526763916,
      "learning_rate": 0.00017087794432548182,
      "loss": 0.0858,
      "step": 817
    },
    {
      "epoch": 0.4380187416331995,
      "grad_norm": 0.6575967669487,
      "learning_rate": 0.00017084225553176303,
      "loss": 0.0707,
      "step": 818
    },
    {
      "epoch": 0.43855421686746987,
      "grad_norm": 0.8565871119499207,
      "learning_rate": 0.00017080656673804426,
      "loss": 0.1747,
      "step": 819
    },
    {
      "epoch": 0.4390896921017403,
      "grad_norm": 2.0415804386138916,
      "learning_rate": 0.0001707708779443255,
      "loss": 0.1351,
      "step": 820
    },
    {
      "epoch": 0.4396251673360107,
      "grad_norm": 0.7897778153419495,
      "learning_rate": 0.0001707351891506067,
      "loss": 0.0607,
      "step": 821
    },
    {
      "epoch": 0.44016064257028115,
      "grad_norm": 3.6752636432647705,
      "learning_rate": 0.00017069950035688796,
      "loss": 0.1971,
      "step": 822
    },
    {
      "epoch": 0.44069611780455153,
      "grad_norm": 0.8940658569335938,
      "learning_rate": 0.00017066381156316917,
      "loss": 0.1204,
      "step": 823
    },
    {
      "epoch": 0.441231593038822,
      "grad_norm": 0.5804619789123535,
      "learning_rate": 0.0001706281227694504,
      "loss": 0.1669,
      "step": 824
    },
    {
      "epoch": 0.44176706827309237,
      "grad_norm": 2.339433431625366,
      "learning_rate": 0.00017059243397573163,
      "loss": 0.1375,
      "step": 825
    },
    {
      "epoch": 0.4423025435073628,
      "grad_norm": 3.1991724967956543,
      "learning_rate": 0.00017055674518201287,
      "loss": 0.1443,
      "step": 826
    },
    {
      "epoch": 0.4428380187416332,
      "grad_norm": 4.9033074378967285,
      "learning_rate": 0.00017052105638829407,
      "loss": 0.182,
      "step": 827
    },
    {
      "epoch": 0.4433734939759036,
      "grad_norm": 2.5469963550567627,
      "learning_rate": 0.0001704853675945753,
      "loss": 0.1416,
      "step": 828
    },
    {
      "epoch": 0.44390896921017403,
      "grad_norm": 0.6241342425346375,
      "learning_rate": 0.00017044967880085654,
      "loss": 0.0801,
      "step": 829
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 3.8357467651367188,
      "learning_rate": 0.00017041399000713775,
      "loss": 0.126,
      "step": 830
    },
    {
      "epoch": 0.44497991967871486,
      "grad_norm": 0.9574161171913147,
      "learning_rate": 0.000170378301213419,
      "loss": 0.1653,
      "step": 831
    },
    {
      "epoch": 0.44551539491298525,
      "grad_norm": 0.7363661527633667,
      "learning_rate": 0.00017034261241970021,
      "loss": 0.1329,
      "step": 832
    },
    {
      "epoch": 0.4460508701472557,
      "grad_norm": 3.756117820739746,
      "learning_rate": 0.00017030692362598145,
      "loss": 0.2088,
      "step": 833
    },
    {
      "epoch": 0.4465863453815261,
      "grad_norm": 0.8535877466201782,
      "learning_rate": 0.00017027123483226268,
      "loss": 0.0739,
      "step": 834
    },
    {
      "epoch": 0.44712182061579653,
      "grad_norm": 6.069597244262695,
      "learning_rate": 0.00017023554603854391,
      "loss": 0.1678,
      "step": 835
    },
    {
      "epoch": 0.4476572958500669,
      "grad_norm": 0.3366171419620514,
      "learning_rate": 0.00017019985724482512,
      "loss": 0.0911,
      "step": 836
    },
    {
      "epoch": 0.44819277108433736,
      "grad_norm": 1.1742569208145142,
      "learning_rate": 0.00017016416845110635,
      "loss": 0.1851,
      "step": 837
    },
    {
      "epoch": 0.44872824631860775,
      "grad_norm": 1.2694716453552246,
      "learning_rate": 0.0001701284796573876,
      "loss": 0.1992,
      "step": 838
    },
    {
      "epoch": 0.4492637215528782,
      "grad_norm": 1.6419410705566406,
      "learning_rate": 0.00017009279086366882,
      "loss": 0.1251,
      "step": 839
    },
    {
      "epoch": 0.4497991967871486,
      "grad_norm": 1.66146981716156,
      "learning_rate": 0.00017005710206995005,
      "loss": 0.0986,
      "step": 840
    },
    {
      "epoch": 0.450334672021419,
      "grad_norm": 0.45368069410324097,
      "learning_rate": 0.00017002141327623126,
      "loss": 0.0668,
      "step": 841
    },
    {
      "epoch": 0.4508701472556894,
      "grad_norm": 1.4447990655899048,
      "learning_rate": 0.00016998572448251252,
      "loss": 0.136,
      "step": 842
    },
    {
      "epoch": 0.45140562248995986,
      "grad_norm": 0.5698815584182739,
      "learning_rate": 0.00016995003568879373,
      "loss": 0.1109,
      "step": 843
    },
    {
      "epoch": 0.45194109772423025,
      "grad_norm": 1.1180577278137207,
      "learning_rate": 0.00016991434689507496,
      "loss": 0.0545,
      "step": 844
    },
    {
      "epoch": 0.4524765729585007,
      "grad_norm": 2.667578935623169,
      "learning_rate": 0.0001698786581013562,
      "loss": 0.1338,
      "step": 845
    },
    {
      "epoch": 0.4530120481927711,
      "grad_norm": 5.126291751861572,
      "learning_rate": 0.0001698429693076374,
      "loss": 0.2163,
      "step": 846
    },
    {
      "epoch": 0.4535475234270415,
      "grad_norm": 1.258212924003601,
      "learning_rate": 0.00016980728051391863,
      "loss": 0.125,
      "step": 847
    },
    {
      "epoch": 0.4540829986613119,
      "grad_norm": 0.701442539691925,
      "learning_rate": 0.00016977159172019987,
      "loss": 0.1471,
      "step": 848
    },
    {
      "epoch": 0.45461847389558235,
      "grad_norm": 1.7260555028915405,
      "learning_rate": 0.0001697359029264811,
      "loss": 0.1408,
      "step": 849
    },
    {
      "epoch": 0.45515394912985274,
      "grad_norm": 9.208974838256836,
      "learning_rate": 0.0001697002141327623,
      "loss": 0.1162,
      "step": 850
    },
    {
      "epoch": 0.4556894243641232,
      "grad_norm": 1.7576147317886353,
      "learning_rate": 0.00016966452533904357,
      "loss": 0.1274,
      "step": 851
    },
    {
      "epoch": 0.4562248995983936,
      "grad_norm": 2.4475667476654053,
      "learning_rate": 0.00016962883654532477,
      "loss": 0.1882,
      "step": 852
    },
    {
      "epoch": 0.45676037483266396,
      "grad_norm": 1.5290387868881226,
      "learning_rate": 0.000169593147751606,
      "loss": 0.1413,
      "step": 853
    },
    {
      "epoch": 0.4572958500669344,
      "grad_norm": 0.1868620067834854,
      "learning_rate": 0.00016955745895788724,
      "loss": 0.0122,
      "step": 854
    },
    {
      "epoch": 0.4578313253012048,
      "grad_norm": 0.9953826069831848,
      "learning_rate": 0.00016952177016416845,
      "loss": 0.0464,
      "step": 855
    },
    {
      "epoch": 0.45836680053547524,
      "grad_norm": 5.179459571838379,
      "learning_rate": 0.00016948608137044968,
      "loss": 0.1316,
      "step": 856
    },
    {
      "epoch": 0.4589022757697456,
      "grad_norm": 4.8534836769104,
      "learning_rate": 0.00016945039257673091,
      "loss": 0.1524,
      "step": 857
    },
    {
      "epoch": 0.45943775100401607,
      "grad_norm": 4.629547595977783,
      "learning_rate": 0.00016941470378301215,
      "loss": 0.1544,
      "step": 858
    },
    {
      "epoch": 0.45997322623828646,
      "grad_norm": 2.0009961128234863,
      "learning_rate": 0.00016937901498929335,
      "loss": 0.1289,
      "step": 859
    },
    {
      "epoch": 0.4605087014725569,
      "grad_norm": 0.6106370091438293,
      "learning_rate": 0.00016934332619557462,
      "loss": 0.0763,
      "step": 860
    },
    {
      "epoch": 0.4610441767068273,
      "grad_norm": 0.5087170004844666,
      "learning_rate": 0.00016930763740185582,
      "loss": 0.0896,
      "step": 861
    },
    {
      "epoch": 0.46157965194109773,
      "grad_norm": 1.0835260152816772,
      "learning_rate": 0.00016927194860813705,
      "loss": 0.1075,
      "step": 862
    },
    {
      "epoch": 0.4621151271753681,
      "grad_norm": 1.0088896751403809,
      "learning_rate": 0.0001692362598144183,
      "loss": 0.1523,
      "step": 863
    },
    {
      "epoch": 0.46265060240963857,
      "grad_norm": 2.7298381328582764,
      "learning_rate": 0.0001692005710206995,
      "loss": 0.0811,
      "step": 864
    },
    {
      "epoch": 0.46318607764390896,
      "grad_norm": 0.618996262550354,
      "learning_rate": 0.00016916488222698073,
      "loss": 0.0609,
      "step": 865
    },
    {
      "epoch": 0.4637215528781794,
      "grad_norm": 1.0519369840621948,
      "learning_rate": 0.00016912919343326196,
      "loss": 0.0665,
      "step": 866
    },
    {
      "epoch": 0.4642570281124498,
      "grad_norm": 0.6780396103858948,
      "learning_rate": 0.0001690935046395432,
      "loss": 0.0433,
      "step": 867
    },
    {
      "epoch": 0.46479250334672023,
      "grad_norm": 0.9725970029830933,
      "learning_rate": 0.00016905781584582443,
      "loss": 0.0955,
      "step": 868
    },
    {
      "epoch": 0.4653279785809906,
      "grad_norm": 1.636364459991455,
      "learning_rate": 0.00016902212705210566,
      "loss": 0.0522,
      "step": 869
    },
    {
      "epoch": 0.46586345381526106,
      "grad_norm": 0.6905325055122375,
      "learning_rate": 0.00016898643825838687,
      "loss": 0.0753,
      "step": 870
    },
    {
      "epoch": 0.46639892904953145,
      "grad_norm": 5.894781589508057,
      "learning_rate": 0.0001689507494646681,
      "loss": 0.1872,
      "step": 871
    },
    {
      "epoch": 0.4669344042838019,
      "grad_norm": 0.8954302072525024,
      "learning_rate": 0.00016891506067094933,
      "loss": 0.0837,
      "step": 872
    },
    {
      "epoch": 0.4674698795180723,
      "grad_norm": 1.105028748512268,
      "learning_rate": 0.00016887937187723054,
      "loss": 0.0868,
      "step": 873
    },
    {
      "epoch": 0.46800535475234273,
      "grad_norm": 0.9763562083244324,
      "learning_rate": 0.0001688436830835118,
      "loss": 0.1709,
      "step": 874
    },
    {
      "epoch": 0.4685408299866131,
      "grad_norm": 0.960938036441803,
      "learning_rate": 0.000168807994289793,
      "loss": 0.1356,
      "step": 875
    },
    {
      "epoch": 0.46907630522088356,
      "grad_norm": 0.7247408032417297,
      "learning_rate": 0.00016877230549607424,
      "loss": 0.1098,
      "step": 876
    },
    {
      "epoch": 0.46961178045515395,
      "grad_norm": 2.081531524658203,
      "learning_rate": 0.00016873661670235547,
      "loss": 0.1847,
      "step": 877
    },
    {
      "epoch": 0.47014725568942434,
      "grad_norm": 1.5341994762420654,
      "learning_rate": 0.0001687009279086367,
      "loss": 0.1162,
      "step": 878
    },
    {
      "epoch": 0.4706827309236948,
      "grad_norm": 15.946063995361328,
      "learning_rate": 0.00016866523911491791,
      "loss": 0.1485,
      "step": 879
    },
    {
      "epoch": 0.47121820615796517,
      "grad_norm": 0.5145222544670105,
      "learning_rate": 0.00016862955032119915,
      "loss": 0.0861,
      "step": 880
    },
    {
      "epoch": 0.4717536813922356,
      "grad_norm": 0.6386191248893738,
      "learning_rate": 0.00016859386152748038,
      "loss": 0.1146,
      "step": 881
    },
    {
      "epoch": 0.472289156626506,
      "grad_norm": 1.3227131366729736,
      "learning_rate": 0.0001685581727337616,
      "loss": 0.0987,
      "step": 882
    },
    {
      "epoch": 0.47282463186077645,
      "grad_norm": 2.1069447994232178,
      "learning_rate": 0.00016852248394004285,
      "loss": 0.1066,
      "step": 883
    },
    {
      "epoch": 0.47336010709504683,
      "grad_norm": 1.9677506685256958,
      "learning_rate": 0.00016848679514632405,
      "loss": 0.2221,
      "step": 884
    },
    {
      "epoch": 0.4738955823293173,
      "grad_norm": 1.0555124282836914,
      "learning_rate": 0.0001684511063526053,
      "loss": 0.1383,
      "step": 885
    },
    {
      "epoch": 0.47443105756358767,
      "grad_norm": 0.6623280048370361,
      "learning_rate": 0.00016841541755888652,
      "loss": 0.1679,
      "step": 886
    },
    {
      "epoch": 0.4749665327978581,
      "grad_norm": 0.8986216187477112,
      "learning_rate": 0.00016837972876516775,
      "loss": 0.0285,
      "step": 887
    },
    {
      "epoch": 0.4755020080321285,
      "grad_norm": 0.3538794219493866,
      "learning_rate": 0.00016834403997144896,
      "loss": 0.0604,
      "step": 888
    },
    {
      "epoch": 0.47603748326639894,
      "grad_norm": 1.325219750404358,
      "learning_rate": 0.0001683083511777302,
      "loss": 0.2047,
      "step": 889
    },
    {
      "epoch": 0.47657295850066933,
      "grad_norm": 0.595245897769928,
      "learning_rate": 0.00016827266238401143,
      "loss": 0.0443,
      "step": 890
    },
    {
      "epoch": 0.4771084337349398,
      "grad_norm": 0.9496751427650452,
      "learning_rate": 0.00016823697359029263,
      "loss": 0.2316,
      "step": 891
    },
    {
      "epoch": 0.47764390896921016,
      "grad_norm": 0.5797752737998962,
      "learning_rate": 0.0001682012847965739,
      "loss": 0.1035,
      "step": 892
    },
    {
      "epoch": 0.4781793842034806,
      "grad_norm": 0.38897839188575745,
      "learning_rate": 0.0001681655960028551,
      "loss": 0.108,
      "step": 893
    },
    {
      "epoch": 0.478714859437751,
      "grad_norm": 0.7267316579818726,
      "learning_rate": 0.00016812990720913636,
      "loss": 0.0657,
      "step": 894
    },
    {
      "epoch": 0.47925033467202144,
      "grad_norm": 3.9051589965820312,
      "learning_rate": 0.00016809421841541757,
      "loss": 0.1188,
      "step": 895
    },
    {
      "epoch": 0.4797858099062918,
      "grad_norm": 1.927896499633789,
      "learning_rate": 0.0001680585296216988,
      "loss": 0.0322,
      "step": 896
    },
    {
      "epoch": 0.48032128514056227,
      "grad_norm": 0.899686336517334,
      "learning_rate": 0.00016802284082798003,
      "loss": 0.1624,
      "step": 897
    },
    {
      "epoch": 0.48085676037483266,
      "grad_norm": 0.7648844122886658,
      "learning_rate": 0.00016798715203426124,
      "loss": 0.1283,
      "step": 898
    },
    {
      "epoch": 0.4813922356091031,
      "grad_norm": 1.2664846181869507,
      "learning_rate": 0.00016795146324054247,
      "loss": 0.1729,
      "step": 899
    },
    {
      "epoch": 0.4819277108433735,
      "grad_norm": 1.064732313156128,
      "learning_rate": 0.0001679157744468237,
      "loss": 0.0675,
      "step": 900
    },
    {
      "epoch": 0.48246318607764394,
      "grad_norm": 1.236166000366211,
      "learning_rate": 0.00016788008565310494,
      "loss": 0.1378,
      "step": 901
    },
    {
      "epoch": 0.4829986613119143,
      "grad_norm": 0.7640823125839233,
      "learning_rate": 0.00016784439685938615,
      "loss": 0.0694,
      "step": 902
    },
    {
      "epoch": 0.4835341365461847,
      "grad_norm": 0.6268173456192017,
      "learning_rate": 0.0001678087080656674,
      "loss": 0.0814,
      "step": 903
    },
    {
      "epoch": 0.48406961178045516,
      "grad_norm": 1.4490728378295898,
      "learning_rate": 0.00016777301927194861,
      "loss": 0.0802,
      "step": 904
    },
    {
      "epoch": 0.48460508701472554,
      "grad_norm": 3.347382068634033,
      "learning_rate": 0.00016773733047822985,
      "loss": 0.0862,
      "step": 905
    },
    {
      "epoch": 0.485140562248996,
      "grad_norm": 0.6562581062316895,
      "learning_rate": 0.00016770164168451108,
      "loss": 0.0915,
      "step": 906
    },
    {
      "epoch": 0.4856760374832664,
      "grad_norm": 0.5988513231277466,
      "learning_rate": 0.0001676659528907923,
      "loss": 0.1167,
      "step": 907
    },
    {
      "epoch": 0.4862115127175368,
      "grad_norm": 1.3125429153442383,
      "learning_rate": 0.00016763026409707352,
      "loss": 0.158,
      "step": 908
    },
    {
      "epoch": 0.4867469879518072,
      "grad_norm": 0.5957254767417908,
      "learning_rate": 0.00016759457530335475,
      "loss": 0.2044,
      "step": 909
    },
    {
      "epoch": 0.48728246318607765,
      "grad_norm": 4.748824596405029,
      "learning_rate": 0.000167558886509636,
      "loss": 0.2052,
      "step": 910
    },
    {
      "epoch": 0.48781793842034804,
      "grad_norm": 0.7658165097236633,
      "learning_rate": 0.0001675231977159172,
      "loss": 0.0858,
      "step": 911
    },
    {
      "epoch": 0.4883534136546185,
      "grad_norm": 1.2344821691513062,
      "learning_rate": 0.00016748750892219845,
      "loss": 0.1279,
      "step": 912
    },
    {
      "epoch": 0.4888888888888889,
      "grad_norm": 5.869093894958496,
      "learning_rate": 0.00016745182012847966,
      "loss": 0.1751,
      "step": 913
    },
    {
      "epoch": 0.4894243641231593,
      "grad_norm": 0.3110002875328064,
      "learning_rate": 0.0001674161313347609,
      "loss": 0.0727,
      "step": 914
    },
    {
      "epoch": 0.4899598393574297,
      "grad_norm": 1.4300521612167358,
      "learning_rate": 0.00016738044254104213,
      "loss": 0.1331,
      "step": 915
    },
    {
      "epoch": 0.49049531459170015,
      "grad_norm": 0.39627647399902344,
      "learning_rate": 0.00016734475374732333,
      "loss": 0.0922,
      "step": 916
    },
    {
      "epoch": 0.49103078982597054,
      "grad_norm": 0.3972318470478058,
      "learning_rate": 0.00016730906495360457,
      "loss": 0.0496,
      "step": 917
    },
    {
      "epoch": 0.491566265060241,
      "grad_norm": 0.8453952074050903,
      "learning_rate": 0.0001672733761598858,
      "loss": 0.0746,
      "step": 918
    },
    {
      "epoch": 0.49210174029451137,
      "grad_norm": 1.384499192237854,
      "learning_rate": 0.00016723768736616703,
      "loss": 0.1265,
      "step": 919
    },
    {
      "epoch": 0.4926372155287818,
      "grad_norm": 2.0616490840911865,
      "learning_rate": 0.00016720199857244824,
      "loss": 0.1196,
      "step": 920
    },
    {
      "epoch": 0.4931726907630522,
      "grad_norm": 1.181036353111267,
      "learning_rate": 0.0001671663097787295,
      "loss": 0.1444,
      "step": 921
    },
    {
      "epoch": 0.49370816599732265,
      "grad_norm": 3.2399752140045166,
      "learning_rate": 0.0001671306209850107,
      "loss": 0.1189,
      "step": 922
    },
    {
      "epoch": 0.49424364123159303,
      "grad_norm": 0.7354649305343628,
      "learning_rate": 0.00016709493219129194,
      "loss": 0.1119,
      "step": 923
    },
    {
      "epoch": 0.4947791164658635,
      "grad_norm": 0.5356097221374512,
      "learning_rate": 0.00016705924339757317,
      "loss": 0.0647,
      "step": 924
    },
    {
      "epoch": 0.49531459170013387,
      "grad_norm": 0.3486495614051819,
      "learning_rate": 0.00016702355460385438,
      "loss": 0.089,
      "step": 925
    },
    {
      "epoch": 0.4958500669344043,
      "grad_norm": 0.4142836332321167,
      "learning_rate": 0.00016698786581013564,
      "loss": 0.084,
      "step": 926
    },
    {
      "epoch": 0.4963855421686747,
      "grad_norm": 0.7489537596702576,
      "learning_rate": 0.00016695217701641685,
      "loss": 0.0892,
      "step": 927
    },
    {
      "epoch": 0.4969210174029451,
      "grad_norm": 0.8550006747245789,
      "learning_rate": 0.00016691648822269808,
      "loss": 0.1416,
      "step": 928
    },
    {
      "epoch": 0.49745649263721553,
      "grad_norm": 1.3229460716247559,
      "learning_rate": 0.00016688079942897931,
      "loss": 0.0977,
      "step": 929
    },
    {
      "epoch": 0.4979919678714859,
      "grad_norm": 0.4421398639678955,
      "learning_rate": 0.00016684511063526055,
      "loss": 0.072,
      "step": 930
    },
    {
      "epoch": 0.49852744310575636,
      "grad_norm": 0.6940962672233582,
      "learning_rate": 0.00016680942184154175,
      "loss": 0.1412,
      "step": 931
    },
    {
      "epoch": 0.49906291834002675,
      "grad_norm": 4.140282154083252,
      "learning_rate": 0.000166773733047823,
      "loss": 0.1748,
      "step": 932
    },
    {
      "epoch": 0.4995983935742972,
      "grad_norm": 2.767347574234009,
      "learning_rate": 0.00016673804425410422,
      "loss": 0.1843,
      "step": 933
    },
    {
      "epoch": 0.5001338688085676,
      "grad_norm": 1.58122718334198,
      "learning_rate": 0.00016670235546038543,
      "loss": 0.1727,
      "step": 934
    },
    {
      "epoch": 0.500669344042838,
      "grad_norm": 0.6352542042732239,
      "learning_rate": 0.0001666666666666667,
      "loss": 0.1511,
      "step": 935
    },
    {
      "epoch": 0.5012048192771085,
      "grad_norm": 0.6589781641960144,
      "learning_rate": 0.0001666309778729479,
      "loss": 0.127,
      "step": 936
    },
    {
      "epoch": 0.5017402945113788,
      "grad_norm": 0.6735444068908691,
      "learning_rate": 0.00016659528907922913,
      "loss": 0.1526,
      "step": 937
    },
    {
      "epoch": 0.5022757697456492,
      "grad_norm": 0.47596874833106995,
      "learning_rate": 0.00016655960028551036,
      "loss": 0.1545,
      "step": 938
    },
    {
      "epoch": 0.5028112449799197,
      "grad_norm": 0.4031012952327728,
      "learning_rate": 0.0001665239114917916,
      "loss": 0.1417,
      "step": 939
    },
    {
      "epoch": 0.5033467202141901,
      "grad_norm": 0.4479946792125702,
      "learning_rate": 0.0001664882226980728,
      "loss": 0.1316,
      "step": 940
    },
    {
      "epoch": 0.5038821954484605,
      "grad_norm": 0.5119642615318298,
      "learning_rate": 0.00016645253390435403,
      "loss": 0.1435,
      "step": 941
    },
    {
      "epoch": 0.5044176706827309,
      "grad_norm": 2.2766714096069336,
      "learning_rate": 0.00016641684511063527,
      "loss": 0.0767,
      "step": 942
    },
    {
      "epoch": 0.5049531459170014,
      "grad_norm": 0.3986321985721588,
      "learning_rate": 0.00016638115631691647,
      "loss": 0.0494,
      "step": 943
    },
    {
      "epoch": 0.5054886211512718,
      "grad_norm": 0.33560827374458313,
      "learning_rate": 0.00016634546752319773,
      "loss": 0.0433,
      "step": 944
    },
    {
      "epoch": 0.5060240963855421,
      "grad_norm": 0.917237401008606,
      "learning_rate": 0.00016630977872947894,
      "loss": 0.033,
      "step": 945
    },
    {
      "epoch": 0.5065595716198126,
      "grad_norm": 0.4965859353542328,
      "learning_rate": 0.00016627408993576017,
      "loss": 0.1055,
      "step": 946
    },
    {
      "epoch": 0.507095046854083,
      "grad_norm": 0.6686761379241943,
      "learning_rate": 0.0001662384011420414,
      "loss": 0.0395,
      "step": 947
    },
    {
      "epoch": 0.5076305220883535,
      "grad_norm": 1.7520886659622192,
      "learning_rate": 0.00016620271234832264,
      "loss": 0.0954,
      "step": 948
    },
    {
      "epoch": 0.5081659973226238,
      "grad_norm": 0.9660711288452148,
      "learning_rate": 0.00016616702355460387,
      "loss": 0.2124,
      "step": 949
    },
    {
      "epoch": 0.5087014725568942,
      "grad_norm": 1.8260260820388794,
      "learning_rate": 0.00016613133476088508,
      "loss": 0.1491,
      "step": 950
    },
    {
      "epoch": 0.5092369477911647,
      "grad_norm": 0.3050624430179596,
      "learning_rate": 0.00016609564596716631,
      "loss": 0.0691,
      "step": 951
    },
    {
      "epoch": 0.5097724230254351,
      "grad_norm": 0.403007447719574,
      "learning_rate": 0.00016605995717344755,
      "loss": 0.0813,
      "step": 952
    },
    {
      "epoch": 0.5103078982597055,
      "grad_norm": 0.6007986664772034,
      "learning_rate": 0.00016602426837972878,
      "loss": 0.1728,
      "step": 953
    },
    {
      "epoch": 0.5108433734939759,
      "grad_norm": 0.5783186554908752,
      "learning_rate": 0.00016598857958601,
      "loss": 0.1958,
      "step": 954
    },
    {
      "epoch": 0.5113788487282463,
      "grad_norm": 0.6108044981956482,
      "learning_rate": 0.00016595289079229125,
      "loss": 0.1448,
      "step": 955
    },
    {
      "epoch": 0.5119143239625167,
      "grad_norm": 1.9348721504211426,
      "learning_rate": 0.00016591720199857245,
      "loss": 0.1693,
      "step": 956
    },
    {
      "epoch": 0.5124497991967871,
      "grad_norm": 2.8011603355407715,
      "learning_rate": 0.0001658815132048537,
      "loss": 0.1829,
      "step": 957
    },
    {
      "epoch": 0.5129852744310576,
      "grad_norm": 1.1573902368545532,
      "learning_rate": 0.00016584582441113492,
      "loss": 0.1316,
      "step": 958
    },
    {
      "epoch": 0.513520749665328,
      "grad_norm": 2.354884147644043,
      "learning_rate": 0.00016581013561741613,
      "loss": 0.2065,
      "step": 959
    },
    {
      "epoch": 0.5140562248995983,
      "grad_norm": 0.6938173174858093,
      "learning_rate": 0.00016577444682369736,
      "loss": 0.2156,
      "step": 960
    },
    {
      "epoch": 0.5145917001338688,
      "grad_norm": 0.4222778379917145,
      "learning_rate": 0.0001657387580299786,
      "loss": 0.1018,
      "step": 961
    },
    {
      "epoch": 0.5151271753681392,
      "grad_norm": 1.1048564910888672,
      "learning_rate": 0.00016570306923625983,
      "loss": 0.0655,
      "step": 962
    },
    {
      "epoch": 0.5156626506024097,
      "grad_norm": 0.9227818250656128,
      "learning_rate": 0.00016566738044254103,
      "loss": 0.183,
      "step": 963
    },
    {
      "epoch": 0.51619812583668,
      "grad_norm": 0.4764789342880249,
      "learning_rate": 0.0001656316916488223,
      "loss": 0.0709,
      "step": 964
    },
    {
      "epoch": 0.5167336010709505,
      "grad_norm": 1.8972727060317993,
      "learning_rate": 0.0001655960028551035,
      "loss": 0.1945,
      "step": 965
    },
    {
      "epoch": 0.5172690763052209,
      "grad_norm": 0.5935355424880981,
      "learning_rate": 0.00016556031406138473,
      "loss": 0.0601,
      "step": 966
    },
    {
      "epoch": 0.5178045515394913,
      "grad_norm": 1.5379613637924194,
      "learning_rate": 0.00016552462526766597,
      "loss": 0.1734,
      "step": 967
    },
    {
      "epoch": 0.5183400267737617,
      "grad_norm": 7.034226417541504,
      "learning_rate": 0.00016548893647394717,
      "loss": 0.1729,
      "step": 968
    },
    {
      "epoch": 0.5188755020080321,
      "grad_norm": 0.7264751195907593,
      "learning_rate": 0.0001654532476802284,
      "loss": 0.1142,
      "step": 969
    },
    {
      "epoch": 0.5194109772423026,
      "grad_norm": 4.729879856109619,
      "learning_rate": 0.00016541755888650964,
      "loss": 0.2165,
      "step": 970
    },
    {
      "epoch": 0.519946452476573,
      "grad_norm": 1.489786148071289,
      "learning_rate": 0.00016538187009279087,
      "loss": 0.1361,
      "step": 971
    },
    {
      "epoch": 0.5204819277108433,
      "grad_norm": 0.8328312039375305,
      "learning_rate": 0.00016534618129907208,
      "loss": 0.0313,
      "step": 972
    },
    {
      "epoch": 0.5210174029451138,
      "grad_norm": 0.779801070690155,
      "learning_rate": 0.00016531049250535334,
      "loss": 0.168,
      "step": 973
    },
    {
      "epoch": 0.5215528781793842,
      "grad_norm": 0.4841500222682953,
      "learning_rate": 0.00016527480371163455,
      "loss": 0.0722,
      "step": 974
    },
    {
      "epoch": 0.5220883534136547,
      "grad_norm": 1.0214602947235107,
      "learning_rate": 0.00016523911491791578,
      "loss": 0.1555,
      "step": 975
    },
    {
      "epoch": 0.522623828647925,
      "grad_norm": 1.2947357892990112,
      "learning_rate": 0.00016520342612419701,
      "loss": 0.1221,
      "step": 976
    },
    {
      "epoch": 0.5231593038821954,
      "grad_norm": 2.9155306816101074,
      "learning_rate": 0.00016516773733047822,
      "loss": 0.192,
      "step": 977
    },
    {
      "epoch": 0.5236947791164659,
      "grad_norm": 0.961550772190094,
      "learning_rate": 0.00016513204853675948,
      "loss": 0.1073,
      "step": 978
    },
    {
      "epoch": 0.5242302543507362,
      "grad_norm": 1.0657962560653687,
      "learning_rate": 0.0001650963597430407,
      "loss": 0.066,
      "step": 979
    },
    {
      "epoch": 0.5247657295850067,
      "grad_norm": 1.5668772459030151,
      "learning_rate": 0.00016506067094932192,
      "loss": 0.1108,
      "step": 980
    },
    {
      "epoch": 0.5253012048192771,
      "grad_norm": 5.281162261962891,
      "learning_rate": 0.00016502498215560315,
      "loss": 0.2125,
      "step": 981
    },
    {
      "epoch": 0.5258366800535476,
      "grad_norm": 0.7623181939125061,
      "learning_rate": 0.0001649892933618844,
      "loss": 0.095,
      "step": 982
    },
    {
      "epoch": 0.5263721552878179,
      "grad_norm": 1.5052192211151123,
      "learning_rate": 0.0001649536045681656,
      "loss": 0.163,
      "step": 983
    },
    {
      "epoch": 0.5269076305220883,
      "grad_norm": 3.7685773372650146,
      "learning_rate": 0.00016491791577444683,
      "loss": 0.1205,
      "step": 984
    },
    {
      "epoch": 0.5274431057563588,
      "grad_norm": 0.8651344180107117,
      "learning_rate": 0.00016488222698072806,
      "loss": 0.1151,
      "step": 985
    },
    {
      "epoch": 0.5279785809906292,
      "grad_norm": 0.7981150150299072,
      "learning_rate": 0.00016484653818700927,
      "loss": 0.1278,
      "step": 986
    },
    {
      "epoch": 0.5285140562248996,
      "grad_norm": 9.109444618225098,
      "learning_rate": 0.00016481084939329053,
      "loss": 0.1735,
      "step": 987
    },
    {
      "epoch": 0.52904953145917,
      "grad_norm": 6.448757171630859,
      "learning_rate": 0.00016477516059957173,
      "loss": 0.1208,
      "step": 988
    },
    {
      "epoch": 0.5295850066934404,
      "grad_norm": 0.4603216350078583,
      "learning_rate": 0.00016473947180585297,
      "loss": 0.0436,
      "step": 989
    },
    {
      "epoch": 0.5301204819277109,
      "grad_norm": 1.9161945581436157,
      "learning_rate": 0.0001647037830121342,
      "loss": 0.085,
      "step": 990
    },
    {
      "epoch": 0.5306559571619812,
      "grad_norm": 1.5899455547332764,
      "learning_rate": 0.00016466809421841543,
      "loss": 0.1414,
      "step": 991
    },
    {
      "epoch": 0.5311914323962517,
      "grad_norm": 0.3502117097377777,
      "learning_rate": 0.00016463240542469664,
      "loss": 0.0611,
      "step": 992
    },
    {
      "epoch": 0.5317269076305221,
      "grad_norm": 6.392978191375732,
      "learning_rate": 0.00016459671663097787,
      "loss": 0.0927,
      "step": 993
    },
    {
      "epoch": 0.5322623828647925,
      "grad_norm": 0.37062668800354004,
      "learning_rate": 0.0001645610278372591,
      "loss": 0.0655,
      "step": 994
    },
    {
      "epoch": 0.5327978580990629,
      "grad_norm": 1.6053423881530762,
      "learning_rate": 0.00016452533904354031,
      "loss": 0.177,
      "step": 995
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 2.545614004135132,
      "learning_rate": 0.00016448965024982157,
      "loss": 0.2043,
      "step": 996
    },
    {
      "epoch": 0.5338688085676038,
      "grad_norm": 5.023622989654541,
      "learning_rate": 0.00016445396145610278,
      "loss": 0.2175,
      "step": 997
    },
    {
      "epoch": 0.5344042838018742,
      "grad_norm": 0.358530193567276,
      "learning_rate": 0.00016441827266238401,
      "loss": 0.0701,
      "step": 998
    },
    {
      "epoch": 0.5349397590361445,
      "grad_norm": 3.561486005783081,
      "learning_rate": 0.00016438258386866525,
      "loss": 0.2338,
      "step": 999
    },
    {
      "epoch": 0.535475234270415,
      "grad_norm": 0.5639414191246033,
      "learning_rate": 0.00016434689507494648,
      "loss": 0.0549,
      "step": 1000
    },
    {
      "epoch": 0.5360107095046854,
      "grad_norm": 1.4838703870773315,
      "learning_rate": 0.0001643112062812277,
      "loss": 0.1203,
      "step": 1001
    },
    {
      "epoch": 0.5365461847389559,
      "grad_norm": 1.0800349712371826,
      "learning_rate": 0.00016427551748750892,
      "loss": 0.0302,
      "step": 1002
    },
    {
      "epoch": 0.5370816599732262,
      "grad_norm": 0.4071290194988251,
      "learning_rate": 0.00016423982869379015,
      "loss": 0.0872,
      "step": 1003
    },
    {
      "epoch": 0.5376171352074967,
      "grad_norm": 0.8564655184745789,
      "learning_rate": 0.0001642041399000714,
      "loss": 0.1468,
      "step": 1004
    },
    {
      "epoch": 0.5381526104417671,
      "grad_norm": 7.502778053283691,
      "learning_rate": 0.00016416845110635262,
      "loss": 0.1045,
      "step": 1005
    },
    {
      "epoch": 0.5386880856760374,
      "grad_norm": 2.10137677192688,
      "learning_rate": 0.00016413276231263383,
      "loss": 0.1476,
      "step": 1006
    },
    {
      "epoch": 0.5392235609103079,
      "grad_norm": 0.3387542963027954,
      "learning_rate": 0.0001640970735189151,
      "loss": 0.0749,
      "step": 1007
    },
    {
      "epoch": 0.5397590361445783,
      "grad_norm": 0.42090821266174316,
      "learning_rate": 0.0001640613847251963,
      "loss": 0.0758,
      "step": 1008
    },
    {
      "epoch": 0.5402945113788488,
      "grad_norm": 0.2769998610019684,
      "learning_rate": 0.00016402569593147753,
      "loss": 0.0667,
      "step": 1009
    },
    {
      "epoch": 0.5408299866131191,
      "grad_norm": 0.3877495527267456,
      "learning_rate": 0.00016399000713775876,
      "loss": 0.1043,
      "step": 1010
    },
    {
      "epoch": 0.5413654618473895,
      "grad_norm": 0.5638071894645691,
      "learning_rate": 0.00016395431834403997,
      "loss": 0.1114,
      "step": 1011
    },
    {
      "epoch": 0.54190093708166,
      "grad_norm": 0.9426453709602356,
      "learning_rate": 0.0001639186295503212,
      "loss": 0.1252,
      "step": 1012
    },
    {
      "epoch": 0.5424364123159304,
      "grad_norm": 0.6188669204711914,
      "learning_rate": 0.00016388294075660243,
      "loss": 0.1462,
      "step": 1013
    },
    {
      "epoch": 0.5429718875502008,
      "grad_norm": 0.46783098578453064,
      "learning_rate": 0.00016384725196288367,
      "loss": 0.0461,
      "step": 1014
    },
    {
      "epoch": 0.5435073627844712,
      "grad_norm": 1.7959011793136597,
      "learning_rate": 0.00016381156316916487,
      "loss": 0.1618,
      "step": 1015
    },
    {
      "epoch": 0.5440428380187416,
      "grad_norm": 0.49064844846725464,
      "learning_rate": 0.00016377587437544613,
      "loss": 0.0799,
      "step": 1016
    },
    {
      "epoch": 0.5445783132530121,
      "grad_norm": 1.480100393295288,
      "learning_rate": 0.00016374018558172734,
      "loss": 0.114,
      "step": 1017
    },
    {
      "epoch": 0.5451137884872824,
      "grad_norm": 0.5355251431465149,
      "learning_rate": 0.00016370449678800857,
      "loss": 0.0642,
      "step": 1018
    },
    {
      "epoch": 0.5456492637215529,
      "grad_norm": 1.520056128501892,
      "learning_rate": 0.0001636688079942898,
      "loss": 0.1177,
      "step": 1019
    },
    {
      "epoch": 0.5461847389558233,
      "grad_norm": 0.5888595581054688,
      "learning_rate": 0.00016363311920057104,
      "loss": 0.0446,
      "step": 1020
    },
    {
      "epoch": 0.5467202141900938,
      "grad_norm": 2.7005491256713867,
      "learning_rate": 0.00016359743040685225,
      "loss": 0.1077,
      "step": 1021
    },
    {
      "epoch": 0.5472556894243641,
      "grad_norm": 1.1214834451675415,
      "learning_rate": 0.00016356174161313348,
      "loss": 0.1348,
      "step": 1022
    },
    {
      "epoch": 0.5477911646586345,
      "grad_norm": 1.0572848320007324,
      "learning_rate": 0.00016352605281941471,
      "loss": 0.1144,
      "step": 1023
    },
    {
      "epoch": 0.548326639892905,
      "grad_norm": 0.7521976828575134,
      "learning_rate": 0.00016349036402569592,
      "loss": 0.1398,
      "step": 1024
    },
    {
      "epoch": 0.5488621151271754,
      "grad_norm": 2.632585287094116,
      "learning_rate": 0.00016345467523197718,
      "loss": 0.1592,
      "step": 1025
    },
    {
      "epoch": 0.5493975903614458,
      "grad_norm": 1.6326842308044434,
      "learning_rate": 0.0001634189864382584,
      "loss": 0.1057,
      "step": 1026
    },
    {
      "epoch": 0.5499330655957162,
      "grad_norm": 5.564823150634766,
      "learning_rate": 0.00016338329764453962,
      "loss": 0.1251,
      "step": 1027
    },
    {
      "epoch": 0.5504685408299866,
      "grad_norm": 0.9239187836647034,
      "learning_rate": 0.00016334760885082085,
      "loss": 0.1214,
      "step": 1028
    },
    {
      "epoch": 0.551004016064257,
      "grad_norm": 0.5147428512573242,
      "learning_rate": 0.0001633119200571021,
      "loss": 0.0883,
      "step": 1029
    },
    {
      "epoch": 0.5515394912985274,
      "grad_norm": 3.651618480682373,
      "learning_rate": 0.0001632762312633833,
      "loss": 0.1442,
      "step": 1030
    },
    {
      "epoch": 0.5520749665327979,
      "grad_norm": 3.830301523208618,
      "learning_rate": 0.00016324054246966453,
      "loss": 0.1166,
      "step": 1031
    },
    {
      "epoch": 0.5526104417670683,
      "grad_norm": 0.696421205997467,
      "learning_rate": 0.00016320485367594576,
      "loss": 0.1672,
      "step": 1032
    },
    {
      "epoch": 0.5531459170013386,
      "grad_norm": 0.35744354128837585,
      "learning_rate": 0.000163169164882227,
      "loss": 0.0923,
      "step": 1033
    },
    {
      "epoch": 0.5536813922356091,
      "grad_norm": 0.6559953093528748,
      "learning_rate": 0.00016313347608850823,
      "loss": 0.102,
      "step": 1034
    },
    {
      "epoch": 0.5542168674698795,
      "grad_norm": 4.332624912261963,
      "learning_rate": 0.00016309778729478943,
      "loss": 0.1865,
      "step": 1035
    },
    {
      "epoch": 0.55475234270415,
      "grad_norm": 0.6762245297431946,
      "learning_rate": 0.0001630620985010707,
      "loss": 0.1164,
      "step": 1036
    },
    {
      "epoch": 0.5552878179384203,
      "grad_norm": 0.4000713527202606,
      "learning_rate": 0.0001630264097073519,
      "loss": 0.0614,
      "step": 1037
    },
    {
      "epoch": 0.5558232931726907,
      "grad_norm": 0.7753476500511169,
      "learning_rate": 0.00016299072091363313,
      "loss": 0.1121,
      "step": 1038
    },
    {
      "epoch": 0.5563587684069612,
      "grad_norm": 0.48461419343948364,
      "learning_rate": 0.00016295503211991437,
      "loss": 0.056,
      "step": 1039
    },
    {
      "epoch": 0.5568942436412316,
      "grad_norm": 0.4579918086528778,
      "learning_rate": 0.00016291934332619557,
      "loss": 0.0743,
      "step": 1040
    },
    {
      "epoch": 0.557429718875502,
      "grad_norm": 1.2398624420166016,
      "learning_rate": 0.0001628836545324768,
      "loss": 0.0792,
      "step": 1041
    },
    {
      "epoch": 0.5579651941097724,
      "grad_norm": 0.8316831588745117,
      "learning_rate": 0.00016284796573875804,
      "loss": 0.1341,
      "step": 1042
    },
    {
      "epoch": 0.5585006693440429,
      "grad_norm": 0.46030136942863464,
      "learning_rate": 0.00016281227694503927,
      "loss": 0.0973,
      "step": 1043
    },
    {
      "epoch": 0.5590361445783133,
      "grad_norm": 0.5481733083724976,
      "learning_rate": 0.00016277658815132048,
      "loss": 0.1375,
      "step": 1044
    },
    {
      "epoch": 0.5595716198125836,
      "grad_norm": 0.9615541696548462,
      "learning_rate": 0.00016274089935760174,
      "loss": 0.0723,
      "step": 1045
    },
    {
      "epoch": 0.5601070950468541,
      "grad_norm": 1.7656632661819458,
      "learning_rate": 0.00016270521056388295,
      "loss": 0.2343,
      "step": 1046
    },
    {
      "epoch": 0.5606425702811245,
      "grad_norm": 0.5327270030975342,
      "learning_rate": 0.00016266952177016418,
      "loss": 0.1103,
      "step": 1047
    },
    {
      "epoch": 0.561178045515395,
      "grad_norm": 0.6291062235832214,
      "learning_rate": 0.00016263383297644541,
      "loss": 0.0952,
      "step": 1048
    },
    {
      "epoch": 0.5617135207496653,
      "grad_norm": 1.2122116088867188,
      "learning_rate": 0.00016259814418272662,
      "loss": 0.0809,
      "step": 1049
    },
    {
      "epoch": 0.5622489959839357,
      "grad_norm": 0.36389443278312683,
      "learning_rate": 0.00016256245538900785,
      "loss": 0.0285,
      "step": 1050
    },
    {
      "epoch": 0.5627844712182062,
      "grad_norm": 1.0874981880187988,
      "learning_rate": 0.0001625267665952891,
      "loss": 0.2105,
      "step": 1051
    },
    {
      "epoch": 0.5633199464524766,
      "grad_norm": 0.7206059694290161,
      "learning_rate": 0.00016249107780157032,
      "loss": 0.1018,
      "step": 1052
    },
    {
      "epoch": 0.563855421686747,
      "grad_norm": 0.5091257691383362,
      "learning_rate": 0.00016245538900785153,
      "loss": 0.1279,
      "step": 1053
    },
    {
      "epoch": 0.5643908969210174,
      "grad_norm": 0.3843994438648224,
      "learning_rate": 0.0001624197002141328,
      "loss": 0.1008,
      "step": 1054
    },
    {
      "epoch": 0.5649263721552878,
      "grad_norm": 0.8729114532470703,
      "learning_rate": 0.000162384011420414,
      "loss": 0.0787,
      "step": 1055
    },
    {
      "epoch": 0.5654618473895582,
      "grad_norm": 1.9016172885894775,
      "learning_rate": 0.00016234832262669523,
      "loss": 0.1269,
      "step": 1056
    },
    {
      "epoch": 0.5659973226238286,
      "grad_norm": 0.9043424129486084,
      "learning_rate": 0.00016231263383297646,
      "loss": 0.1216,
      "step": 1057
    },
    {
      "epoch": 0.5665327978580991,
      "grad_norm": 0.3967498540878296,
      "learning_rate": 0.00016227694503925767,
      "loss": 0.04,
      "step": 1058
    },
    {
      "epoch": 0.5670682730923695,
      "grad_norm": 0.4581890404224396,
      "learning_rate": 0.00016224125624553893,
      "loss": 0.1579,
      "step": 1059
    },
    {
      "epoch": 0.5676037483266398,
      "grad_norm": 0.5624915361404419,
      "learning_rate": 0.00016220556745182013,
      "loss": 0.0875,
      "step": 1060
    },
    {
      "epoch": 0.5681392235609103,
      "grad_norm": 0.43745461106300354,
      "learning_rate": 0.00016216987865810137,
      "loss": 0.086,
      "step": 1061
    },
    {
      "epoch": 0.5686746987951807,
      "grad_norm": 0.7061601877212524,
      "learning_rate": 0.0001621341898643826,
      "loss": 0.0767,
      "step": 1062
    },
    {
      "epoch": 0.5692101740294512,
      "grad_norm": 4.015410900115967,
      "learning_rate": 0.00016209850107066383,
      "loss": 0.1599,
      "step": 1063
    },
    {
      "epoch": 0.5697456492637215,
      "grad_norm": 0.47642865777015686,
      "learning_rate": 0.00016206281227694504,
      "loss": 0.0959,
      "step": 1064
    },
    {
      "epoch": 0.570281124497992,
      "grad_norm": 0.7110857963562012,
      "learning_rate": 0.00016202712348322627,
      "loss": 0.0919,
      "step": 1065
    },
    {
      "epoch": 0.5708165997322624,
      "grad_norm": 1.4328749179840088,
      "learning_rate": 0.0001619914346895075,
      "loss": 0.1895,
      "step": 1066
    },
    {
      "epoch": 0.5713520749665328,
      "grad_norm": 0.4639781415462494,
      "learning_rate": 0.00016195574589578871,
      "loss": 0.0812,
      "step": 1067
    },
    {
      "epoch": 0.5718875502008032,
      "grad_norm": 0.3296053409576416,
      "learning_rate": 0.00016192005710206997,
      "loss": 0.0362,
      "step": 1068
    },
    {
      "epoch": 0.5724230254350736,
      "grad_norm": 1.285343050956726,
      "learning_rate": 0.00016188436830835118,
      "loss": 0.1853,
      "step": 1069
    },
    {
      "epoch": 0.5729585006693441,
      "grad_norm": 0.439846932888031,
      "learning_rate": 0.00016184867951463241,
      "loss": 0.0545,
      "step": 1070
    },
    {
      "epoch": 0.5734939759036145,
      "grad_norm": 0.1327056884765625,
      "learning_rate": 0.00016181299072091365,
      "loss": 0.0073,
      "step": 1071
    },
    {
      "epoch": 0.5740294511378848,
      "grad_norm": 0.5093830227851868,
      "learning_rate": 0.00016177730192719488,
      "loss": 0.0632,
      "step": 1072
    },
    {
      "epoch": 0.5745649263721553,
      "grad_norm": 0.8819800019264221,
      "learning_rate": 0.0001617416131334761,
      "loss": 0.1135,
      "step": 1073
    },
    {
      "epoch": 0.5751004016064257,
      "grad_norm": 0.37213924527168274,
      "learning_rate": 0.00016170592433975732,
      "loss": 0.0808,
      "step": 1074
    },
    {
      "epoch": 0.5756358768406962,
      "grad_norm": 0.30591294169425964,
      "learning_rate": 0.00016167023554603855,
      "loss": 0.065,
      "step": 1075
    },
    {
      "epoch": 0.5761713520749665,
      "grad_norm": 1.3792190551757812,
      "learning_rate": 0.00016163454675231976,
      "loss": 0.1559,
      "step": 1076
    },
    {
      "epoch": 0.576706827309237,
      "grad_norm": 0.6780982613563538,
      "learning_rate": 0.00016159885795860102,
      "loss": 0.0476,
      "step": 1077
    },
    {
      "epoch": 0.5772423025435074,
      "grad_norm": 4.020517349243164,
      "learning_rate": 0.00016156316916488223,
      "loss": 0.4229,
      "step": 1078
    },
    {
      "epoch": 0.5777777777777777,
      "grad_norm": 4.333398818969727,
      "learning_rate": 0.00016152748037116346,
      "loss": 0.2085,
      "step": 1079
    },
    {
      "epoch": 0.5783132530120482,
      "grad_norm": 0.3511611819267273,
      "learning_rate": 0.0001614917915774447,
      "loss": 0.0926,
      "step": 1080
    },
    {
      "epoch": 0.5788487282463186,
      "grad_norm": 0.46796220541000366,
      "learning_rate": 0.00016145610278372593,
      "loss": 0.1506,
      "step": 1081
    },
    {
      "epoch": 0.579384203480589,
      "grad_norm": 1.520146369934082,
      "learning_rate": 0.00016142041399000713,
      "loss": 0.0975,
      "step": 1082
    },
    {
      "epoch": 0.5799196787148594,
      "grad_norm": 0.9591032266616821,
      "learning_rate": 0.00016138472519628837,
      "loss": 0.0578,
      "step": 1083
    },
    {
      "epoch": 0.5804551539491298,
      "grad_norm": 0.9449133276939392,
      "learning_rate": 0.0001613490364025696,
      "loss": 0.2685,
      "step": 1084
    },
    {
      "epoch": 0.5809906291834003,
      "grad_norm": 0.8291305899620056,
      "learning_rate": 0.0001613133476088508,
      "loss": 0.2173,
      "step": 1085
    },
    {
      "epoch": 0.5815261044176707,
      "grad_norm": 0.29731762409210205,
      "learning_rate": 0.00016127765881513207,
      "loss": 0.0298,
      "step": 1086
    },
    {
      "epoch": 0.582061579651941,
      "grad_norm": 0.5323173999786377,
      "learning_rate": 0.00016124197002141327,
      "loss": 0.0754,
      "step": 1087
    },
    {
      "epoch": 0.5825970548862115,
      "grad_norm": 2.935760259628296,
      "learning_rate": 0.00016120628122769453,
      "loss": 0.2058,
      "step": 1088
    },
    {
      "epoch": 0.5831325301204819,
      "grad_norm": 5.703106880187988,
      "learning_rate": 0.00016117059243397574,
      "loss": 0.1637,
      "step": 1089
    },
    {
      "epoch": 0.5836680053547524,
      "grad_norm": 0.6914380788803101,
      "learning_rate": 0.00016113490364025697,
      "loss": 0.0715,
      "step": 1090
    },
    {
      "epoch": 0.5842034805890227,
      "grad_norm": 0.7923921346664429,
      "learning_rate": 0.0001610992148465382,
      "loss": 0.1914,
      "step": 1091
    },
    {
      "epoch": 0.5847389558232932,
      "grad_norm": 1.0245693922042847,
      "learning_rate": 0.00016106352605281941,
      "loss": 0.1577,
      "step": 1092
    },
    {
      "epoch": 0.5852744310575636,
      "grad_norm": 0.9734961986541748,
      "learning_rate": 0.00016102783725910065,
      "loss": 0.1698,
      "step": 1093
    },
    {
      "epoch": 0.585809906291834,
      "grad_norm": 0.5559284687042236,
      "learning_rate": 0.00016099214846538188,
      "loss": 0.1305,
      "step": 1094
    },
    {
      "epoch": 0.5863453815261044,
      "grad_norm": 0.4856400489807129,
      "learning_rate": 0.00016095645967166311,
      "loss": 0.0777,
      "step": 1095
    },
    {
      "epoch": 0.5868808567603748,
      "grad_norm": 0.7676771283149719,
      "learning_rate": 0.00016092077087794432,
      "loss": 0.1738,
      "step": 1096
    },
    {
      "epoch": 0.5874163319946453,
      "grad_norm": 0.4194076359272003,
      "learning_rate": 0.00016088508208422558,
      "loss": 0.0829,
      "step": 1097
    },
    {
      "epoch": 0.5879518072289157,
      "grad_norm": 7.280713081359863,
      "learning_rate": 0.0001608493932905068,
      "loss": 0.2154,
      "step": 1098
    },
    {
      "epoch": 0.588487282463186,
      "grad_norm": 0.32585230469703674,
      "learning_rate": 0.00016081370449678802,
      "loss": 0.1225,
      "step": 1099
    },
    {
      "epoch": 0.5890227576974565,
      "grad_norm": 0.42827799916267395,
      "learning_rate": 0.00016077801570306925,
      "loss": 0.119,
      "step": 1100
    },
    {
      "epoch": 0.5895582329317269,
      "grad_norm": 0.763253390789032,
      "learning_rate": 0.00016074232690935046,
      "loss": 0.1564,
      "step": 1101
    },
    {
      "epoch": 0.5900937081659974,
      "grad_norm": 0.2925461530685425,
      "learning_rate": 0.0001607066381156317,
      "loss": 0.0933,
      "step": 1102
    },
    {
      "epoch": 0.5906291834002677,
      "grad_norm": 0.28433462977409363,
      "learning_rate": 0.00016067094932191293,
      "loss": 0.0475,
      "step": 1103
    },
    {
      "epoch": 0.5911646586345382,
      "grad_norm": 2.7176175117492676,
      "learning_rate": 0.00016063526052819416,
      "loss": 0.0816,
      "step": 1104
    },
    {
      "epoch": 0.5917001338688086,
      "grad_norm": 0.23676712810993195,
      "learning_rate": 0.00016059957173447537,
      "loss": 0.0601,
      "step": 1105
    },
    {
      "epoch": 0.5922356091030789,
      "grad_norm": 2.0561845302581787,
      "learning_rate": 0.00016056388294075663,
      "loss": 0.1733,
      "step": 1106
    },
    {
      "epoch": 0.5927710843373494,
      "grad_norm": 0.2601170837879181,
      "learning_rate": 0.00016052819414703783,
      "loss": 0.091,
      "step": 1107
    },
    {
      "epoch": 0.5933065595716198,
      "grad_norm": 1.1648273468017578,
      "learning_rate": 0.00016049250535331907,
      "loss": 0.1225,
      "step": 1108
    },
    {
      "epoch": 0.5938420348058903,
      "grad_norm": 0.2695721983909607,
      "learning_rate": 0.0001604568165596003,
      "loss": 0.0481,
      "step": 1109
    },
    {
      "epoch": 0.5943775100401606,
      "grad_norm": 0.276215136051178,
      "learning_rate": 0.0001604211277658815,
      "loss": 0.0682,
      "step": 1110
    },
    {
      "epoch": 0.594912985274431,
      "grad_norm": 3.6175448894500732,
      "learning_rate": 0.00016038543897216274,
      "loss": 0.2155,
      "step": 1111
    },
    {
      "epoch": 0.5954484605087015,
      "grad_norm": 0.25759613513946533,
      "learning_rate": 0.00016034975017844397,
      "loss": 0.0228,
      "step": 1112
    },
    {
      "epoch": 0.5959839357429719,
      "grad_norm": 0.3498515486717224,
      "learning_rate": 0.0001603140613847252,
      "loss": 0.096,
      "step": 1113
    },
    {
      "epoch": 0.5965194109772423,
      "grad_norm": 0.43180546164512634,
      "learning_rate": 0.00016027837259100644,
      "loss": 0.0618,
      "step": 1114
    },
    {
      "epoch": 0.5970548862115127,
      "grad_norm": 0.8095619678497314,
      "learning_rate": 0.00016024268379728767,
      "loss": 0.1167,
      "step": 1115
    },
    {
      "epoch": 0.5975903614457831,
      "grad_norm": 0.9156239032745361,
      "learning_rate": 0.00016020699500356888,
      "loss": 0.1348,
      "step": 1116
    },
    {
      "epoch": 0.5981258366800536,
      "grad_norm": 0.41668567061424255,
      "learning_rate": 0.00016017130620985011,
      "loss": 0.1702,
      "step": 1117
    },
    {
      "epoch": 0.5986613119143239,
      "grad_norm": 0.5243321061134338,
      "learning_rate": 0.00016013561741613135,
      "loss": 0.0614,
      "step": 1118
    },
    {
      "epoch": 0.5991967871485944,
      "grad_norm": 2.0659868717193604,
      "learning_rate": 0.00016009992862241255,
      "loss": 0.0721,
      "step": 1119
    },
    {
      "epoch": 0.5997322623828648,
      "grad_norm": 0.5451425909996033,
      "learning_rate": 0.00016006423982869381,
      "loss": 0.1226,
      "step": 1120
    },
    {
      "epoch": 0.6002677376171353,
      "grad_norm": 0.7478479743003845,
      "learning_rate": 0.00016002855103497502,
      "loss": 0.1031,
      "step": 1121
    },
    {
      "epoch": 0.6008032128514056,
      "grad_norm": 0.915259599685669,
      "learning_rate": 0.00015999286224125625,
      "loss": 0.1171,
      "step": 1122
    },
    {
      "epoch": 0.601338688085676,
      "grad_norm": 0.529069721698761,
      "learning_rate": 0.0001599571734475375,
      "loss": 0.0679,
      "step": 1123
    },
    {
      "epoch": 0.6018741633199465,
      "grad_norm": 1.7636595964431763,
      "learning_rate": 0.00015992148465381872,
      "loss": 0.1242,
      "step": 1124
    },
    {
      "epoch": 0.6024096385542169,
      "grad_norm": 0.21709324419498444,
      "learning_rate": 0.00015988579586009993,
      "loss": 0.0144,
      "step": 1125
    },
    {
      "epoch": 0.6029451137884873,
      "grad_norm": 1.593658447265625,
      "learning_rate": 0.00015985010706638116,
      "loss": 0.151,
      "step": 1126
    },
    {
      "epoch": 0.6034805890227577,
      "grad_norm": 0.5317323207855225,
      "learning_rate": 0.0001598144182726624,
      "loss": 0.0716,
      "step": 1127
    },
    {
      "epoch": 0.6040160642570281,
      "grad_norm": 0.7331922054290771,
      "learning_rate": 0.0001597787294789436,
      "loss": 0.1159,
      "step": 1128
    },
    {
      "epoch": 0.6045515394912985,
      "grad_norm": 0.6016252040863037,
      "learning_rate": 0.00015974304068522486,
      "loss": 0.0843,
      "step": 1129
    },
    {
      "epoch": 0.6050870147255689,
      "grad_norm": 1.1368978023529053,
      "learning_rate": 0.00015970735189150607,
      "loss": 0.1749,
      "step": 1130
    },
    {
      "epoch": 0.6056224899598394,
      "grad_norm": 1.2576186656951904,
      "learning_rate": 0.0001596716630977873,
      "loss": 0.094,
      "step": 1131
    },
    {
      "epoch": 0.6061579651941098,
      "grad_norm": 2.6964714527130127,
      "learning_rate": 0.00015963597430406853,
      "loss": 0.1193,
      "step": 1132
    },
    {
      "epoch": 0.6066934404283801,
      "grad_norm": 2.376732110977173,
      "learning_rate": 0.00015960028551034977,
      "loss": 0.1276,
      "step": 1133
    },
    {
      "epoch": 0.6072289156626506,
      "grad_norm": 0.863174319267273,
      "learning_rate": 0.00015956459671663097,
      "loss": 0.0916,
      "step": 1134
    },
    {
      "epoch": 0.607764390896921,
      "grad_norm": 0.7414057850837708,
      "learning_rate": 0.0001595289079229122,
      "loss": 0.0969,
      "step": 1135
    },
    {
      "epoch": 0.6082998661311915,
      "grad_norm": 0.6617963314056396,
      "learning_rate": 0.00015949321912919344,
      "loss": 0.1033,
      "step": 1136
    },
    {
      "epoch": 0.6088353413654618,
      "grad_norm": 0.5145907998085022,
      "learning_rate": 0.00015945753033547465,
      "loss": 0.0503,
      "step": 1137
    },
    {
      "epoch": 0.6093708165997322,
      "grad_norm": 1.2415049076080322,
      "learning_rate": 0.0001594218415417559,
      "loss": 0.0806,
      "step": 1138
    },
    {
      "epoch": 0.6099062918340027,
      "grad_norm": 3.0790610313415527,
      "learning_rate": 0.00015938615274803711,
      "loss": 0.187,
      "step": 1139
    },
    {
      "epoch": 0.6104417670682731,
      "grad_norm": 0.33112239837646484,
      "learning_rate": 0.00015935046395431835,
      "loss": 0.0592,
      "step": 1140
    },
    {
      "epoch": 0.6109772423025435,
      "grad_norm": 0.5719637274742126,
      "learning_rate": 0.00015931477516059958,
      "loss": 0.0468,
      "step": 1141
    },
    {
      "epoch": 0.6115127175368139,
      "grad_norm": 0.42614489793777466,
      "learning_rate": 0.00015927908636688081,
      "loss": 0.0806,
      "step": 1142
    },
    {
      "epoch": 0.6120481927710844,
      "grad_norm": 0.8642913103103638,
      "learning_rate": 0.00015924339757316205,
      "loss": 0.1326,
      "step": 1143
    },
    {
      "epoch": 0.6125836680053548,
      "grad_norm": 1.6844958066940308,
      "learning_rate": 0.00015920770877944325,
      "loss": 0.0755,
      "step": 1144
    },
    {
      "epoch": 0.6131191432396251,
      "grad_norm": 0.5875852108001709,
      "learning_rate": 0.0001591720199857245,
      "loss": 0.1542,
      "step": 1145
    },
    {
      "epoch": 0.6136546184738956,
      "grad_norm": 1.1081348657608032,
      "learning_rate": 0.00015913633119200572,
      "loss": 0.1632,
      "step": 1146
    },
    {
      "epoch": 0.614190093708166,
      "grad_norm": 5.955461502075195,
      "learning_rate": 0.00015910064239828695,
      "loss": 0.1651,
      "step": 1147
    },
    {
      "epoch": 0.6147255689424365,
      "grad_norm": 0.29161950945854187,
      "learning_rate": 0.00015906495360456816,
      "loss": 0.0821,
      "step": 1148
    },
    {
      "epoch": 0.6152610441767068,
      "grad_norm": 0.2902638614177704,
      "learning_rate": 0.00015902926481084942,
      "loss": 0.0819,
      "step": 1149
    },
    {
      "epoch": 0.6157965194109772,
      "grad_norm": 0.8976240754127502,
      "learning_rate": 0.00015899357601713063,
      "loss": 0.1647,
      "step": 1150
    },
    {
      "epoch": 0.6163319946452477,
      "grad_norm": 0.24861477315425873,
      "learning_rate": 0.00015895788722341186,
      "loss": 0.0453,
      "step": 1151
    },
    {
      "epoch": 0.6168674698795181,
      "grad_norm": 0.31046339869499207,
      "learning_rate": 0.0001589221984296931,
      "loss": 0.0308,
      "step": 1152
    },
    {
      "epoch": 0.6174029451137885,
      "grad_norm": 8.930975914001465,
      "learning_rate": 0.0001588865096359743,
      "loss": 0.1888,
      "step": 1153
    },
    {
      "epoch": 0.6179384203480589,
      "grad_norm": 0.5989522933959961,
      "learning_rate": 0.00015885082084225553,
      "loss": 0.1719,
      "step": 1154
    },
    {
      "epoch": 0.6184738955823293,
      "grad_norm": 2.3524560928344727,
      "learning_rate": 0.00015881513204853677,
      "loss": 0.1085,
      "step": 1155
    },
    {
      "epoch": 0.6190093708165997,
      "grad_norm": 0.7130085229873657,
      "learning_rate": 0.000158779443254818,
      "loss": 0.0601,
      "step": 1156
    },
    {
      "epoch": 0.6195448460508701,
      "grad_norm": 0.4969918727874756,
      "learning_rate": 0.0001587437544610992,
      "loss": 0.0903,
      "step": 1157
    },
    {
      "epoch": 0.6200803212851406,
      "grad_norm": 0.35712018609046936,
      "learning_rate": 0.00015870806566738047,
      "loss": 0.1226,
      "step": 1158
    },
    {
      "epoch": 0.620615796519411,
      "grad_norm": 0.46527063846588135,
      "learning_rate": 0.00015867237687366167,
      "loss": 0.0608,
      "step": 1159
    },
    {
      "epoch": 0.6211512717536813,
      "grad_norm": 0.44251367449760437,
      "learning_rate": 0.0001586366880799429,
      "loss": 0.0576,
      "step": 1160
    },
    {
      "epoch": 0.6216867469879518,
      "grad_norm": 1.816096305847168,
      "learning_rate": 0.00015860099928622414,
      "loss": 0.0997,
      "step": 1161
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 2.3161144256591797,
      "learning_rate": 0.00015856531049250535,
      "loss": 0.213,
      "step": 1162
    },
    {
      "epoch": 0.6227576974564927,
      "grad_norm": 1.4397693872451782,
      "learning_rate": 0.00015852962169878658,
      "loss": 0.1645,
      "step": 1163
    },
    {
      "epoch": 0.623293172690763,
      "grad_norm": 5.370241641998291,
      "learning_rate": 0.00015849393290506781,
      "loss": 0.1763,
      "step": 1164
    },
    {
      "epoch": 0.6238286479250335,
      "grad_norm": 0.2880564033985138,
      "learning_rate": 0.00015845824411134905,
      "loss": 0.0397,
      "step": 1165
    },
    {
      "epoch": 0.6243641231593039,
      "grad_norm": 0.3691098988056183,
      "learning_rate": 0.00015842255531763025,
      "loss": 0.1209,
      "step": 1166
    },
    {
      "epoch": 0.6248995983935743,
      "grad_norm": 0.36377426981925964,
      "learning_rate": 0.00015838686652391151,
      "loss": 0.0366,
      "step": 1167
    },
    {
      "epoch": 0.6254350736278447,
      "grad_norm": 0.5690774917602539,
      "learning_rate": 0.00015835117773019272,
      "loss": 0.1107,
      "step": 1168
    },
    {
      "epoch": 0.6259705488621151,
      "grad_norm": 0.3017585873603821,
      "learning_rate": 0.00015831548893647395,
      "loss": 0.1147,
      "step": 1169
    },
    {
      "epoch": 0.6265060240963856,
      "grad_norm": 0.3468663692474365,
      "learning_rate": 0.0001582798001427552,
      "loss": 0.1233,
      "step": 1170
    },
    {
      "epoch": 0.627041499330656,
      "grad_norm": 1.1667778491973877,
      "learning_rate": 0.0001582441113490364,
      "loss": 0.0875,
      "step": 1171
    },
    {
      "epoch": 0.6275769745649263,
      "grad_norm": 4.147261142730713,
      "learning_rate": 0.00015820842255531765,
      "loss": 0.1033,
      "step": 1172
    },
    {
      "epoch": 0.6281124497991968,
      "grad_norm": 0.24429699778556824,
      "learning_rate": 0.00015817273376159886,
      "loss": 0.0107,
      "step": 1173
    },
    {
      "epoch": 0.6286479250334672,
      "grad_norm": 2.811349391937256,
      "learning_rate": 0.0001581370449678801,
      "loss": 0.0868,
      "step": 1174
    },
    {
      "epoch": 0.6291834002677377,
      "grad_norm": 0.36844345927238464,
      "learning_rate": 0.00015810135617416133,
      "loss": 0.1313,
      "step": 1175
    },
    {
      "epoch": 0.629718875502008,
      "grad_norm": 1.083749532699585,
      "learning_rate": 0.00015806566738044256,
      "loss": 0.0996,
      "step": 1176
    },
    {
      "epoch": 0.6302543507362784,
      "grad_norm": 0.32646846771240234,
      "learning_rate": 0.00015802997858672377,
      "loss": 0.0379,
      "step": 1177
    },
    {
      "epoch": 0.6307898259705489,
      "grad_norm": 0.41194671392440796,
      "learning_rate": 0.000157994289793005,
      "loss": 0.0825,
      "step": 1178
    },
    {
      "epoch": 0.6313253012048192,
      "grad_norm": 0.4092070460319519,
      "learning_rate": 0.00015795860099928623,
      "loss": 0.1481,
      "step": 1179
    },
    {
      "epoch": 0.6318607764390897,
      "grad_norm": 0.21150551736354828,
      "learning_rate": 0.00015792291220556744,
      "loss": 0.0517,
      "step": 1180
    },
    {
      "epoch": 0.6323962516733601,
      "grad_norm": 1.1680457592010498,
      "learning_rate": 0.0001578872234118487,
      "loss": 0.1471,
      "step": 1181
    },
    {
      "epoch": 0.6329317269076306,
      "grad_norm": 0.41016143560409546,
      "learning_rate": 0.0001578515346181299,
      "loss": 0.1553,
      "step": 1182
    },
    {
      "epoch": 0.6334672021419009,
      "grad_norm": 0.758675217628479,
      "learning_rate": 0.00015781584582441114,
      "loss": 0.2046,
      "step": 1183
    },
    {
      "epoch": 0.6340026773761713,
      "grad_norm": 0.6227810382843018,
      "learning_rate": 0.00015778015703069237,
      "loss": 0.0888,
      "step": 1184
    },
    {
      "epoch": 0.6345381526104418,
      "grad_norm": 0.44328564405441284,
      "learning_rate": 0.0001577444682369736,
      "loss": 0.0998,
      "step": 1185
    },
    {
      "epoch": 0.6350736278447122,
      "grad_norm": 0.5558716654777527,
      "learning_rate": 0.00015770877944325481,
      "loss": 0.0657,
      "step": 1186
    },
    {
      "epoch": 0.6356091030789826,
      "grad_norm": 4.048933029174805,
      "learning_rate": 0.00015767309064953605,
      "loss": 0.1671,
      "step": 1187
    },
    {
      "epoch": 0.636144578313253,
      "grad_norm": 1.9107133150100708,
      "learning_rate": 0.00015763740185581728,
      "loss": 0.1484,
      "step": 1188
    },
    {
      "epoch": 0.6366800535475234,
      "grad_norm": 0.29770419001579285,
      "learning_rate": 0.0001576017130620985,
      "loss": 0.1036,
      "step": 1189
    },
    {
      "epoch": 0.6372155287817939,
      "grad_norm": 0.4005570411682129,
      "learning_rate": 0.00015756602426837975,
      "loss": 0.077,
      "step": 1190
    },
    {
      "epoch": 0.6377510040160642,
      "grad_norm": 2.1686620712280273,
      "learning_rate": 0.00015753033547466095,
      "loss": 0.1233,
      "step": 1191
    },
    {
      "epoch": 0.6382864792503347,
      "grad_norm": 1.8114712238311768,
      "learning_rate": 0.0001574946466809422,
      "loss": 0.1604,
      "step": 1192
    },
    {
      "epoch": 0.6388219544846051,
      "grad_norm": 2.618074655532837,
      "learning_rate": 0.00015745895788722342,
      "loss": 0.128,
      "step": 1193
    },
    {
      "epoch": 0.6393574297188755,
      "grad_norm": 5.579498767852783,
      "learning_rate": 0.00015742326909350465,
      "loss": 0.0815,
      "step": 1194
    },
    {
      "epoch": 0.6398929049531459,
      "grad_norm": 0.38678663969039917,
      "learning_rate": 0.00015738758029978586,
      "loss": 0.0835,
      "step": 1195
    },
    {
      "epoch": 0.6404283801874163,
      "grad_norm": 0.8092575073242188,
      "learning_rate": 0.0001573518915060671,
      "loss": 0.0804,
      "step": 1196
    },
    {
      "epoch": 0.6409638554216868,
      "grad_norm": 0.5775055289268494,
      "learning_rate": 0.00015731620271234833,
      "loss": 0.1224,
      "step": 1197
    },
    {
      "epoch": 0.6414993306559572,
      "grad_norm": 0.589954137802124,
      "learning_rate": 0.00015728051391862956,
      "loss": 0.1478,
      "step": 1198
    },
    {
      "epoch": 0.6420348058902275,
      "grad_norm": 0.6634859442710876,
      "learning_rate": 0.0001572448251249108,
      "loss": 0.0763,
      "step": 1199
    },
    {
      "epoch": 0.642570281124498,
      "grad_norm": 0.8130331039428711,
      "learning_rate": 0.000157209136331192,
      "loss": 0.0431,
      "step": 1200
    },
    {
      "epoch": 0.6431057563587684,
      "grad_norm": 0.2555909752845764,
      "learning_rate": 0.00015717344753747326,
      "loss": 0.0889,
      "step": 1201
    },
    {
      "epoch": 0.6436412315930389,
      "grad_norm": 0.4291972517967224,
      "learning_rate": 0.00015713775874375447,
      "loss": 0.0836,
      "step": 1202
    },
    {
      "epoch": 0.6441767068273092,
      "grad_norm": 0.5158637166023254,
      "learning_rate": 0.0001571020699500357,
      "loss": 0.1281,
      "step": 1203
    },
    {
      "epoch": 0.6447121820615797,
      "grad_norm": 0.355433851480484,
      "learning_rate": 0.00015706638115631693,
      "loss": 0.0939,
      "step": 1204
    },
    {
      "epoch": 0.6452476572958501,
      "grad_norm": 0.4517214298248291,
      "learning_rate": 0.00015703069236259814,
      "loss": 0.1208,
      "step": 1205
    },
    {
      "epoch": 0.6457831325301204,
      "grad_norm": 0.4484531879425049,
      "learning_rate": 0.00015699500356887937,
      "loss": 0.0506,
      "step": 1206
    },
    {
      "epoch": 0.6463186077643909,
      "grad_norm": 1.0057955980300903,
      "learning_rate": 0.0001569593147751606,
      "loss": 0.1027,
      "step": 1207
    },
    {
      "epoch": 0.6468540829986613,
      "grad_norm": 0.8543111681938171,
      "learning_rate": 0.00015692362598144184,
      "loss": 0.146,
      "step": 1208
    },
    {
      "epoch": 0.6473895582329318,
      "grad_norm": 6.42987585067749,
      "learning_rate": 0.00015688793718772305,
      "loss": 0.1195,
      "step": 1209
    },
    {
      "epoch": 0.6479250334672021,
      "grad_norm": 0.3762745261192322,
      "learning_rate": 0.0001568522483940043,
      "loss": 0.055,
      "step": 1210
    },
    {
      "epoch": 0.6484605087014725,
      "grad_norm": 1.3584015369415283,
      "learning_rate": 0.00015681655960028551,
      "loss": 0.089,
      "step": 1211
    },
    {
      "epoch": 0.648995983935743,
      "grad_norm": 3.2874879837036133,
      "learning_rate": 0.00015678087080656675,
      "loss": 0.1131,
      "step": 1212
    },
    {
      "epoch": 0.6495314591700134,
      "grad_norm": 0.6702653169631958,
      "learning_rate": 0.00015674518201284798,
      "loss": 0.2105,
      "step": 1213
    },
    {
      "epoch": 0.6500669344042838,
      "grad_norm": 1.7482340335845947,
      "learning_rate": 0.0001567094932191292,
      "loss": 0.1455,
      "step": 1214
    },
    {
      "epoch": 0.6506024096385542,
      "grad_norm": 0.5531682372093201,
      "learning_rate": 0.00015667380442541042,
      "loss": 0.0514,
      "step": 1215
    },
    {
      "epoch": 0.6511378848728246,
      "grad_norm": 0.9855219125747681,
      "learning_rate": 0.00015663811563169165,
      "loss": 0.1294,
      "step": 1216
    },
    {
      "epoch": 0.6516733601070951,
      "grad_norm": 0.4864780008792877,
      "learning_rate": 0.0001566024268379729,
      "loss": 0.1002,
      "step": 1217
    },
    {
      "epoch": 0.6522088353413654,
      "grad_norm": 0.47868236899375916,
      "learning_rate": 0.0001565667380442541,
      "loss": 0.117,
      "step": 1218
    },
    {
      "epoch": 0.6527443105756359,
      "grad_norm": 2.6507956981658936,
      "learning_rate": 0.00015653104925053535,
      "loss": 0.105,
      "step": 1219
    },
    {
      "epoch": 0.6532797858099063,
      "grad_norm": 0.20406325161457062,
      "learning_rate": 0.00015649536045681656,
      "loss": 0.0289,
      "step": 1220
    },
    {
      "epoch": 0.6538152610441768,
      "grad_norm": 0.33532464504241943,
      "learning_rate": 0.0001564596716630978,
      "loss": 0.0863,
      "step": 1221
    },
    {
      "epoch": 0.6543507362784471,
      "grad_norm": 3.806190252304077,
      "learning_rate": 0.00015642398286937903,
      "loss": 0.1551,
      "step": 1222
    },
    {
      "epoch": 0.6548862115127175,
      "grad_norm": 0.31556597352027893,
      "learning_rate": 0.00015638829407566023,
      "loss": 0.0458,
      "step": 1223
    },
    {
      "epoch": 0.655421686746988,
      "grad_norm": 0.99995356798172,
      "learning_rate": 0.0001563526052819415,
      "loss": 0.1026,
      "step": 1224
    },
    {
      "epoch": 0.6559571619812584,
      "grad_norm": 4.634319305419922,
      "learning_rate": 0.0001563169164882227,
      "loss": 0.163,
      "step": 1225
    },
    {
      "epoch": 0.6564926372155288,
      "grad_norm": 1.249083161354065,
      "learning_rate": 0.00015628122769450393,
      "loss": 0.143,
      "step": 1226
    },
    {
      "epoch": 0.6570281124497992,
      "grad_norm": 0.8261697888374329,
      "learning_rate": 0.00015624553890078517,
      "loss": 0.1086,
      "step": 1227
    },
    {
      "epoch": 0.6575635876840696,
      "grad_norm": 0.6241869926452637,
      "learning_rate": 0.0001562098501070664,
      "loss": 0.1093,
      "step": 1228
    },
    {
      "epoch": 0.65809906291834,
      "grad_norm": 1.3930860757827759,
      "learning_rate": 0.0001561741613133476,
      "loss": 0.0814,
      "step": 1229
    },
    {
      "epoch": 0.6586345381526104,
      "grad_norm": 3.141704559326172,
      "learning_rate": 0.00015613847251962884,
      "loss": 0.1644,
      "step": 1230
    },
    {
      "epoch": 0.6591700133868809,
      "grad_norm": 0.8375884890556335,
      "learning_rate": 0.00015610278372591007,
      "loss": 0.1588,
      "step": 1231
    },
    {
      "epoch": 0.6597054886211513,
      "grad_norm": 3.434497594833374,
      "learning_rate": 0.00015606709493219128,
      "loss": 0.2463,
      "step": 1232
    },
    {
      "epoch": 0.6602409638554216,
      "grad_norm": 0.6917659044265747,
      "learning_rate": 0.00015603140613847254,
      "loss": 0.097,
      "step": 1233
    },
    {
      "epoch": 0.6607764390896921,
      "grad_norm": 3.4309258460998535,
      "learning_rate": 0.00015599571734475375,
      "loss": 0.1538,
      "step": 1234
    },
    {
      "epoch": 0.6613119143239625,
      "grad_norm": 1.0519554615020752,
      "learning_rate": 0.00015596002855103498,
      "loss": 0.0903,
      "step": 1235
    },
    {
      "epoch": 0.661847389558233,
      "grad_norm": 0.4677131772041321,
      "learning_rate": 0.00015592433975731621,
      "loss": 0.0563,
      "step": 1236
    },
    {
      "epoch": 0.6623828647925033,
      "grad_norm": 0.8118942379951477,
      "learning_rate": 0.00015588865096359745,
      "loss": 0.1759,
      "step": 1237
    },
    {
      "epoch": 0.6629183400267737,
      "grad_norm": 0.5382694005966187,
      "learning_rate": 0.00015585296216987865,
      "loss": 0.113,
      "step": 1238
    },
    {
      "epoch": 0.6634538152610442,
      "grad_norm": 0.29809001088142395,
      "learning_rate": 0.0001558172733761599,
      "loss": 0.0251,
      "step": 1239
    },
    {
      "epoch": 0.6639892904953146,
      "grad_norm": 0.4101320803165436,
      "learning_rate": 0.00015578158458244112,
      "loss": 0.0787,
      "step": 1240
    },
    {
      "epoch": 0.664524765729585,
      "grad_norm": 0.9000024795532227,
      "learning_rate": 0.00015574589578872233,
      "loss": 0.1436,
      "step": 1241
    },
    {
      "epoch": 0.6650602409638554,
      "grad_norm": 0.6881679892539978,
      "learning_rate": 0.0001557102069950036,
      "loss": 0.0729,
      "step": 1242
    },
    {
      "epoch": 0.6655957161981259,
      "grad_norm": 0.494356244802475,
      "learning_rate": 0.0001556745182012848,
      "loss": 0.0705,
      "step": 1243
    },
    {
      "epoch": 0.6661311914323963,
      "grad_norm": 2.799116373062134,
      "learning_rate": 0.00015563882940756603,
      "loss": 0.1895,
      "step": 1244
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 1.968123435974121,
      "learning_rate": 0.00015560314061384726,
      "loss": 0.1108,
      "step": 1245
    },
    {
      "epoch": 0.6672021419009371,
      "grad_norm": 6.024219036102295,
      "learning_rate": 0.0001555674518201285,
      "loss": 0.1561,
      "step": 1246
    },
    {
      "epoch": 0.6677376171352075,
      "grad_norm": 1.5264073610305786,
      "learning_rate": 0.0001555317630264097,
      "loss": 0.0755,
      "step": 1247
    },
    {
      "epoch": 0.668273092369478,
      "grad_norm": 2.843830108642578,
      "learning_rate": 0.00015549607423269093,
      "loss": 0.1283,
      "step": 1248
    },
    {
      "epoch": 0.6688085676037483,
      "grad_norm": 0.7103440165519714,
      "learning_rate": 0.00015546038543897217,
      "loss": 0.0705,
      "step": 1249
    },
    {
      "epoch": 0.6693440428380187,
      "grad_norm": 0.6414559483528137,
      "learning_rate": 0.00015542469664525337,
      "loss": 0.0907,
      "step": 1250
    },
    {
      "epoch": 0.6698795180722892,
      "grad_norm": 0.9281650185585022,
      "learning_rate": 0.00015538900785153463,
      "loss": 0.1222,
      "step": 1251
    },
    {
      "epoch": 0.6704149933065596,
      "grad_norm": 3.5867648124694824,
      "learning_rate": 0.00015535331905781584,
      "loss": 0.0788,
      "step": 1252
    },
    {
      "epoch": 0.67095046854083,
      "grad_norm": 0.6544581651687622,
      "learning_rate": 0.0001553176302640971,
      "loss": 0.0608,
      "step": 1253
    },
    {
      "epoch": 0.6714859437751004,
      "grad_norm": 2.0025148391723633,
      "learning_rate": 0.0001552819414703783,
      "loss": 0.0972,
      "step": 1254
    },
    {
      "epoch": 0.6720214190093708,
      "grad_norm": 1.8225682973861694,
      "learning_rate": 0.00015524625267665954,
      "loss": 0.0927,
      "step": 1255
    },
    {
      "epoch": 0.6725568942436412,
      "grad_norm": 1.3212577104568481,
      "learning_rate": 0.00015521056388294077,
      "loss": 0.1549,
      "step": 1256
    },
    {
      "epoch": 0.6730923694779116,
      "grad_norm": 2.6210572719573975,
      "learning_rate": 0.00015517487508922198,
      "loss": 0.1466,
      "step": 1257
    },
    {
      "epoch": 0.6736278447121821,
      "grad_norm": 3.8105216026306152,
      "learning_rate": 0.00015513918629550321,
      "loss": 0.1877,
      "step": 1258
    },
    {
      "epoch": 0.6741633199464525,
      "grad_norm": 2.0870702266693115,
      "learning_rate": 0.00015510349750178445,
      "loss": 0.1201,
      "step": 1259
    },
    {
      "epoch": 0.6746987951807228,
      "grad_norm": 2.4724318981170654,
      "learning_rate": 0.00015506780870806568,
      "loss": 0.2338,
      "step": 1260
    },
    {
      "epoch": 0.6752342704149933,
      "grad_norm": 1.5128443241119385,
      "learning_rate": 0.0001550321199143469,
      "loss": 0.1069,
      "step": 1261
    },
    {
      "epoch": 0.6757697456492637,
      "grad_norm": 2.268045425415039,
      "learning_rate": 0.00015499643112062815,
      "loss": 0.1548,
      "step": 1262
    },
    {
      "epoch": 0.6763052208835342,
      "grad_norm": 1.551160454750061,
      "learning_rate": 0.00015496074232690935,
      "loss": 0.104,
      "step": 1263
    },
    {
      "epoch": 0.6768406961178045,
      "grad_norm": 0.8983343839645386,
      "learning_rate": 0.0001549250535331906,
      "loss": 0.1321,
      "step": 1264
    },
    {
      "epoch": 0.677376171352075,
      "grad_norm": 0.5133907198905945,
      "learning_rate": 0.00015488936473947182,
      "loss": 0.1007,
      "step": 1265
    },
    {
      "epoch": 0.6779116465863454,
      "grad_norm": 0.9021561741828918,
      "learning_rate": 0.00015485367594575303,
      "loss": 0.1075,
      "step": 1266
    },
    {
      "epoch": 0.6784471218206158,
      "grad_norm": 0.2738838493824005,
      "learning_rate": 0.00015481798715203426,
      "loss": 0.0749,
      "step": 1267
    },
    {
      "epoch": 0.6789825970548862,
      "grad_norm": 0.5477023720741272,
      "learning_rate": 0.0001547822983583155,
      "loss": 0.1149,
      "step": 1268
    },
    {
      "epoch": 0.6795180722891566,
      "grad_norm": 4.425845146179199,
      "learning_rate": 0.00015474660956459673,
      "loss": 0.1909,
      "step": 1269
    },
    {
      "epoch": 0.6800535475234271,
      "grad_norm": 3.222515344619751,
      "learning_rate": 0.00015471092077087793,
      "loss": 0.2059,
      "step": 1270
    },
    {
      "epoch": 0.6805890227576975,
      "grad_norm": 0.847406804561615,
      "learning_rate": 0.0001546752319771592,
      "loss": 0.1623,
      "step": 1271
    },
    {
      "epoch": 0.6811244979919678,
      "grad_norm": 0.31621846556663513,
      "learning_rate": 0.0001546395431834404,
      "loss": 0.0962,
      "step": 1272
    },
    {
      "epoch": 0.6816599732262383,
      "grad_norm": 0.6258323788642883,
      "learning_rate": 0.00015460385438972163,
      "loss": 0.0912,
      "step": 1273
    },
    {
      "epoch": 0.6821954484605087,
      "grad_norm": 6.7358717918396,
      "learning_rate": 0.00015456816559600287,
      "loss": 0.1668,
      "step": 1274
    },
    {
      "epoch": 0.6827309236947792,
      "grad_norm": 0.5161750912666321,
      "learning_rate": 0.0001545324768022841,
      "loss": 0.0974,
      "step": 1275
    },
    {
      "epoch": 0.6832663989290495,
      "grad_norm": 13.902141571044922,
      "learning_rate": 0.0001544967880085653,
      "loss": 0.0785,
      "step": 1276
    },
    {
      "epoch": 0.68380187416332,
      "grad_norm": 0.6623885631561279,
      "learning_rate": 0.00015446109921484654,
      "loss": 0.0909,
      "step": 1277
    },
    {
      "epoch": 0.6843373493975904,
      "grad_norm": 0.3694254755973816,
      "learning_rate": 0.00015442541042112777,
      "loss": 0.0867,
      "step": 1278
    },
    {
      "epoch": 0.6848728246318607,
      "grad_norm": 0.4324072599411011,
      "learning_rate": 0.000154389721627409,
      "loss": 0.1126,
      "step": 1279
    },
    {
      "epoch": 0.6854082998661312,
      "grad_norm": 0.4432417154312134,
      "learning_rate": 0.00015435403283369024,
      "loss": 0.097,
      "step": 1280
    },
    {
      "epoch": 0.6859437751004016,
      "grad_norm": 5.877734184265137,
      "learning_rate": 0.00015431834403997145,
      "loss": 0.0663,
      "step": 1281
    },
    {
      "epoch": 0.686479250334672,
      "grad_norm": 0.4733872413635254,
      "learning_rate": 0.00015428265524625268,
      "loss": 0.1161,
      "step": 1282
    },
    {
      "epoch": 0.6870147255689424,
      "grad_norm": 0.5621803998947144,
      "learning_rate": 0.00015424696645253391,
      "loss": 0.0129,
      "step": 1283
    },
    {
      "epoch": 0.6875502008032128,
      "grad_norm": 0.6381002068519592,
      "learning_rate": 0.00015421127765881515,
      "loss": 0.1217,
      "step": 1284
    },
    {
      "epoch": 0.6880856760374833,
      "grad_norm": 0.7182695865631104,
      "learning_rate": 0.00015417558886509638,
      "loss": 0.102,
      "step": 1285
    },
    {
      "epoch": 0.6886211512717537,
      "grad_norm": 0.5400565266609192,
      "learning_rate": 0.0001541399000713776,
      "loss": 0.1119,
      "step": 1286
    },
    {
      "epoch": 0.689156626506024,
      "grad_norm": 1.5339583158493042,
      "learning_rate": 0.00015410421127765882,
      "loss": 0.1397,
      "step": 1287
    },
    {
      "epoch": 0.6896921017402945,
      "grad_norm": 2.3360064029693604,
      "learning_rate": 0.00015406852248394005,
      "loss": 0.2012,
      "step": 1288
    },
    {
      "epoch": 0.6902275769745649,
      "grad_norm": 0.7857134938240051,
      "learning_rate": 0.0001540328336902213,
      "loss": 0.076,
      "step": 1289
    },
    {
      "epoch": 0.6907630522088354,
      "grad_norm": 0.34719324111938477,
      "learning_rate": 0.0001539971448965025,
      "loss": 0.1187,
      "step": 1290
    },
    {
      "epoch": 0.6912985274431057,
      "grad_norm": 0.6511332988739014,
      "learning_rate": 0.00015396145610278375,
      "loss": 0.1009,
      "step": 1291
    },
    {
      "epoch": 0.6918340026773762,
      "grad_norm": 0.5150029063224792,
      "learning_rate": 0.00015392576730906496,
      "loss": 0.0725,
      "step": 1292
    },
    {
      "epoch": 0.6923694779116466,
      "grad_norm": 1.1786999702453613,
      "learning_rate": 0.0001538900785153462,
      "loss": 0.0872,
      "step": 1293
    },
    {
      "epoch": 0.692904953145917,
      "grad_norm": 0.49224528670310974,
      "learning_rate": 0.00015385438972162743,
      "loss": 0.0819,
      "step": 1294
    },
    {
      "epoch": 0.6934404283801874,
      "grad_norm": 0.541705310344696,
      "learning_rate": 0.00015381870092790863,
      "loss": 0.1587,
      "step": 1295
    },
    {
      "epoch": 0.6939759036144578,
      "grad_norm": 0.5009889006614685,
      "learning_rate": 0.00015378301213418987,
      "loss": 0.11,
      "step": 1296
    },
    {
      "epoch": 0.6945113788487283,
      "grad_norm": 0.6781711578369141,
      "learning_rate": 0.0001537473233404711,
      "loss": 0.1056,
      "step": 1297
    },
    {
      "epoch": 0.6950468540829987,
      "grad_norm": 5.202280044555664,
      "learning_rate": 0.00015371163454675233,
      "loss": 0.1183,
      "step": 1298
    },
    {
      "epoch": 0.695582329317269,
      "grad_norm": 0.25178778171539307,
      "learning_rate": 0.00015367594575303354,
      "loss": 0.0365,
      "step": 1299
    },
    {
      "epoch": 0.6961178045515395,
      "grad_norm": 1.1273078918457031,
      "learning_rate": 0.0001536402569593148,
      "loss": 0.0615,
      "step": 1300
    },
    {
      "epoch": 0.6966532797858099,
      "grad_norm": 0.5282015204429626,
      "learning_rate": 0.000153604568165596,
      "loss": 0.0871,
      "step": 1301
    },
    {
      "epoch": 0.6971887550200804,
      "grad_norm": 6.467315673828125,
      "learning_rate": 0.00015356887937187724,
      "loss": 0.0949,
      "step": 1302
    },
    {
      "epoch": 0.6977242302543507,
      "grad_norm": 0.6145325303077698,
      "learning_rate": 0.00015353319057815847,
      "loss": 0.1014,
      "step": 1303
    },
    {
      "epoch": 0.6982597054886212,
      "grad_norm": 0.6708501577377319,
      "learning_rate": 0.00015349750178443968,
      "loss": 0.1242,
      "step": 1304
    },
    {
      "epoch": 0.6987951807228916,
      "grad_norm": 0.4540252089500427,
      "learning_rate": 0.00015346181299072091,
      "loss": 0.0832,
      "step": 1305
    },
    {
      "epoch": 0.6993306559571619,
      "grad_norm": 0.5780459046363831,
      "learning_rate": 0.00015342612419700215,
      "loss": 0.0948,
      "step": 1306
    },
    {
      "epoch": 0.6998661311914324,
      "grad_norm": 0.8366060256958008,
      "learning_rate": 0.00015339043540328338,
      "loss": 0.0934,
      "step": 1307
    },
    {
      "epoch": 0.7004016064257028,
      "grad_norm": 0.4409293830394745,
      "learning_rate": 0.00015335474660956461,
      "loss": 0.092,
      "step": 1308
    },
    {
      "epoch": 0.7009370816599733,
      "grad_norm": 1.3649381399154663,
      "learning_rate": 0.00015331905781584585,
      "loss": 0.1811,
      "step": 1309
    },
    {
      "epoch": 0.7014725568942436,
      "grad_norm": 0.6382328271865845,
      "learning_rate": 0.00015328336902212705,
      "loss": 0.1379,
      "step": 1310
    },
    {
      "epoch": 0.702008032128514,
      "grad_norm": 2.813739776611328,
      "learning_rate": 0.0001532476802284083,
      "loss": 0.2201,
      "step": 1311
    },
    {
      "epoch": 0.7025435073627845,
      "grad_norm": 3.2228426933288574,
      "learning_rate": 0.00015321199143468952,
      "loss": 0.1266,
      "step": 1312
    },
    {
      "epoch": 0.7030789825970549,
      "grad_norm": 0.5518184900283813,
      "learning_rate": 0.00015317630264097073,
      "loss": 0.126,
      "step": 1313
    },
    {
      "epoch": 0.7036144578313253,
      "grad_norm": 0.4808565378189087,
      "learning_rate": 0.000153140613847252,
      "loss": 0.0551,
      "step": 1314
    },
    {
      "epoch": 0.7041499330655957,
      "grad_norm": 0.3550630211830139,
      "learning_rate": 0.0001531049250535332,
      "loss": 0.0439,
      "step": 1315
    },
    {
      "epoch": 0.7046854082998661,
      "grad_norm": 14.46182632446289,
      "learning_rate": 0.00015306923625981443,
      "loss": 0.1636,
      "step": 1316
    },
    {
      "epoch": 0.7052208835341366,
      "grad_norm": 0.561663806438446,
      "learning_rate": 0.00015303354746609566,
      "loss": 0.132,
      "step": 1317
    },
    {
      "epoch": 0.7057563587684069,
      "grad_norm": 0.4597994089126587,
      "learning_rate": 0.0001529978586723769,
      "loss": 0.0881,
      "step": 1318
    },
    {
      "epoch": 0.7062918340026774,
      "grad_norm": 0.8867577314376831,
      "learning_rate": 0.0001529621698786581,
      "loss": 0.1409,
      "step": 1319
    },
    {
      "epoch": 0.7068273092369478,
      "grad_norm": 0.807161808013916,
      "learning_rate": 0.00015292648108493933,
      "loss": 0.1105,
      "step": 1320
    },
    {
      "epoch": 0.7073627844712183,
      "grad_norm": 0.9473879933357239,
      "learning_rate": 0.00015289079229122057,
      "loss": 0.0514,
      "step": 1321
    },
    {
      "epoch": 0.7078982597054886,
      "grad_norm": 0.8472570180892944,
      "learning_rate": 0.00015285510349750177,
      "loss": 0.158,
      "step": 1322
    },
    {
      "epoch": 0.708433734939759,
      "grad_norm": 0.5505856275558472,
      "learning_rate": 0.00015281941470378303,
      "loss": 0.0548,
      "step": 1323
    },
    {
      "epoch": 0.7089692101740295,
      "grad_norm": 0.5571446418762207,
      "learning_rate": 0.00015278372591006424,
      "loss": 0.1258,
      "step": 1324
    },
    {
      "epoch": 0.7095046854082999,
      "grad_norm": 1.8038297891616821,
      "learning_rate": 0.00015274803711634547,
      "loss": 0.1595,
      "step": 1325
    },
    {
      "epoch": 0.7100401606425703,
      "grad_norm": 0.2667696475982666,
      "learning_rate": 0.0001527123483226267,
      "loss": 0.0669,
      "step": 1326
    },
    {
      "epoch": 0.7105756358768407,
      "grad_norm": 1.288647174835205,
      "learning_rate": 0.00015267665952890794,
      "loss": 0.1226,
      "step": 1327
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 0.5426396727561951,
      "learning_rate": 0.00015264097073518915,
      "loss": 0.0635,
      "step": 1328
    },
    {
      "epoch": 0.7116465863453815,
      "grad_norm": 0.5271905660629272,
      "learning_rate": 0.00015260528194147038,
      "loss": 0.1428,
      "step": 1329
    },
    {
      "epoch": 0.7121820615796519,
      "grad_norm": 0.9482981562614441,
      "learning_rate": 0.00015256959314775161,
      "loss": 0.1617,
      "step": 1330
    },
    {
      "epoch": 0.7127175368139224,
      "grad_norm": 0.4729275703430176,
      "learning_rate": 0.00015253390435403282,
      "loss": 0.1014,
      "step": 1331
    },
    {
      "epoch": 0.7132530120481928,
      "grad_norm": 0.3021159768104553,
      "learning_rate": 0.00015249821556031408,
      "loss": 0.0519,
      "step": 1332
    },
    {
      "epoch": 0.7137884872824631,
      "grad_norm": 0.9636244177818298,
      "learning_rate": 0.0001524625267665953,
      "loss": 0.108,
      "step": 1333
    },
    {
      "epoch": 0.7143239625167336,
      "grad_norm": 1.25119948387146,
      "learning_rate": 0.00015242683797287655,
      "loss": 0.0509,
      "step": 1334
    },
    {
      "epoch": 0.714859437751004,
      "grad_norm": 2.926328420639038,
      "learning_rate": 0.00015239114917915775,
      "loss": 0.2644,
      "step": 1335
    },
    {
      "epoch": 0.7153949129852745,
      "grad_norm": 1.2378407716751099,
      "learning_rate": 0.000152355460385439,
      "loss": 0.1443,
      "step": 1336
    },
    {
      "epoch": 0.7159303882195448,
      "grad_norm": 1.0163373947143555,
      "learning_rate": 0.00015231977159172022,
      "loss": 0.1916,
      "step": 1337
    },
    {
      "epoch": 0.7164658634538152,
      "grad_norm": 0.24567118287086487,
      "learning_rate": 0.00015228408279800143,
      "loss": 0.0121,
      "step": 1338
    },
    {
      "epoch": 0.7170013386880857,
      "grad_norm": 0.6139593124389648,
      "learning_rate": 0.00015224839400428266,
      "loss": 0.1331,
      "step": 1339
    },
    {
      "epoch": 0.7175368139223561,
      "grad_norm": 0.5771610140800476,
      "learning_rate": 0.0001522127052105639,
      "loss": 0.1038,
      "step": 1340
    },
    {
      "epoch": 0.7180722891566265,
      "grad_norm": 1.1251006126403809,
      "learning_rate": 0.00015217701641684513,
      "loss": 0.1167,
      "step": 1341
    },
    {
      "epoch": 0.7186077643908969,
      "grad_norm": 0.24142161011695862,
      "learning_rate": 0.00015214132762312633,
      "loss": 0.0736,
      "step": 1342
    },
    {
      "epoch": 0.7191432396251674,
      "grad_norm": 0.5571720004081726,
      "learning_rate": 0.0001521056388294076,
      "loss": 0.0363,
      "step": 1343
    },
    {
      "epoch": 0.7196787148594378,
      "grad_norm": 1.463288426399231,
      "learning_rate": 0.0001520699500356888,
      "loss": 0.1746,
      "step": 1344
    },
    {
      "epoch": 0.7202141900937081,
      "grad_norm": 0.22826561331748962,
      "learning_rate": 0.00015203426124197003,
      "loss": 0.056,
      "step": 1345
    },
    {
      "epoch": 0.7207496653279786,
      "grad_norm": 0.5446098446846008,
      "learning_rate": 0.00015199857244825127,
      "loss": 0.105,
      "step": 1346
    },
    {
      "epoch": 0.721285140562249,
      "grad_norm": 0.5813408493995667,
      "learning_rate": 0.00015196288365453247,
      "loss": 0.1767,
      "step": 1347
    },
    {
      "epoch": 0.7218206157965195,
      "grad_norm": 8.394745826721191,
      "learning_rate": 0.0001519271948608137,
      "loss": 0.1897,
      "step": 1348
    },
    {
      "epoch": 0.7223560910307898,
      "grad_norm": 0.6973211169242859,
      "learning_rate": 0.00015189150606709494,
      "loss": 0.0897,
      "step": 1349
    },
    {
      "epoch": 0.7228915662650602,
      "grad_norm": 3.2652359008789062,
      "learning_rate": 0.00015185581727337617,
      "loss": 0.1461,
      "step": 1350
    },
    {
      "epoch": 0.7234270414993307,
      "grad_norm": 1.151682734489441,
      "learning_rate": 0.00015182012847965738,
      "loss": 0.2244,
      "step": 1351
    },
    {
      "epoch": 0.7239625167336011,
      "grad_norm": 0.7708771824836731,
      "learning_rate": 0.00015178443968593864,
      "loss": 0.189,
      "step": 1352
    },
    {
      "epoch": 0.7244979919678715,
      "grad_norm": 0.438396155834198,
      "learning_rate": 0.00015174875089221985,
      "loss": 0.0701,
      "step": 1353
    },
    {
      "epoch": 0.7250334672021419,
      "grad_norm": 0.23756253719329834,
      "learning_rate": 0.00015171306209850108,
      "loss": 0.0648,
      "step": 1354
    },
    {
      "epoch": 0.7255689424364123,
      "grad_norm": 0.20809723436832428,
      "learning_rate": 0.00015167737330478231,
      "loss": 0.0281,
      "step": 1355
    },
    {
      "epoch": 0.7261044176706827,
      "grad_norm": 1.1108224391937256,
      "learning_rate": 0.00015164168451106352,
      "loss": 0.1318,
      "step": 1356
    },
    {
      "epoch": 0.7266398929049531,
      "grad_norm": 0.7952926754951477,
      "learning_rate": 0.00015160599571734475,
      "loss": 0.2035,
      "step": 1357
    },
    {
      "epoch": 0.7271753681392236,
      "grad_norm": 0.47545504570007324,
      "learning_rate": 0.000151570306923626,
      "loss": 0.088,
      "step": 1358
    },
    {
      "epoch": 0.727710843373494,
      "grad_norm": 0.3281234800815582,
      "learning_rate": 0.00015153461812990722,
      "loss": 0.1136,
      "step": 1359
    },
    {
      "epoch": 0.7282463186077643,
      "grad_norm": 2.110178232192993,
      "learning_rate": 0.00015149892933618843,
      "loss": 0.0499,
      "step": 1360
    },
    {
      "epoch": 0.7287817938420348,
      "grad_norm": 0.18559962511062622,
      "learning_rate": 0.0001514632405424697,
      "loss": 0.0304,
      "step": 1361
    },
    {
      "epoch": 0.7293172690763052,
      "grad_norm": 0.442253053188324,
      "learning_rate": 0.0001514275517487509,
      "loss": 0.0642,
      "step": 1362
    },
    {
      "epoch": 0.7298527443105757,
      "grad_norm": 2.4202115535736084,
      "learning_rate": 0.00015139186295503213,
      "loss": 0.1493,
      "step": 1363
    },
    {
      "epoch": 0.730388219544846,
      "grad_norm": 0.2699146866798401,
      "learning_rate": 0.00015135617416131336,
      "loss": 0.0537,
      "step": 1364
    },
    {
      "epoch": 0.7309236947791165,
      "grad_norm": 0.538121223449707,
      "learning_rate": 0.00015132048536759457,
      "loss": 0.1646,
      "step": 1365
    },
    {
      "epoch": 0.7314591700133869,
      "grad_norm": 0.4702740013599396,
      "learning_rate": 0.00015128479657387583,
      "loss": 0.0956,
      "step": 1366
    },
    {
      "epoch": 0.7319946452476573,
      "grad_norm": 0.4256666600704193,
      "learning_rate": 0.00015124910778015703,
      "loss": 0.0918,
      "step": 1367
    },
    {
      "epoch": 0.7325301204819277,
      "grad_norm": 2.4167263507843018,
      "learning_rate": 0.00015121341898643827,
      "loss": 0.1461,
      "step": 1368
    },
    {
      "epoch": 0.7330655957161981,
      "grad_norm": 2.7683637142181396,
      "learning_rate": 0.0001511777301927195,
      "loss": 0.0939,
      "step": 1369
    },
    {
      "epoch": 0.7336010709504686,
      "grad_norm": 0.4754995107650757,
      "learning_rate": 0.00015114204139900073,
      "loss": 0.1741,
      "step": 1370
    },
    {
      "epoch": 0.734136546184739,
      "grad_norm": 0.3778923451900482,
      "learning_rate": 0.00015110635260528194,
      "loss": 0.1014,
      "step": 1371
    },
    {
      "epoch": 0.7346720214190093,
      "grad_norm": 0.413417786359787,
      "learning_rate": 0.00015107066381156317,
      "loss": 0.1019,
      "step": 1372
    },
    {
      "epoch": 0.7352074966532798,
      "grad_norm": 0.33045846223831177,
      "learning_rate": 0.0001510349750178444,
      "loss": 0.0903,
      "step": 1373
    },
    {
      "epoch": 0.7357429718875502,
      "grad_norm": 0.9780468344688416,
      "learning_rate": 0.00015099928622412561,
      "loss": 0.0524,
      "step": 1374
    },
    {
      "epoch": 0.7362784471218207,
      "grad_norm": 1.4546562433242798,
      "learning_rate": 0.00015096359743040687,
      "loss": 0.1549,
      "step": 1375
    },
    {
      "epoch": 0.736813922356091,
      "grad_norm": 0.7214534878730774,
      "learning_rate": 0.00015092790863668808,
      "loss": 0.1281,
      "step": 1376
    },
    {
      "epoch": 0.7373493975903614,
      "grad_norm": 0.43485555052757263,
      "learning_rate": 0.00015089221984296931,
      "loss": 0.059,
      "step": 1377
    },
    {
      "epoch": 0.7378848728246319,
      "grad_norm": 0.5828473567962646,
      "learning_rate": 0.00015085653104925055,
      "loss": 0.1151,
      "step": 1378
    },
    {
      "epoch": 0.7384203480589022,
      "grad_norm": 2.1007237434387207,
      "learning_rate": 0.00015082084225553178,
      "loss": 0.1124,
      "step": 1379
    },
    {
      "epoch": 0.7389558232931727,
      "grad_norm": 0.4343965947628021,
      "learning_rate": 0.000150785153461813,
      "loss": 0.0634,
      "step": 1380
    },
    {
      "epoch": 0.7394912985274431,
      "grad_norm": 0.3977660834789276,
      "learning_rate": 0.00015074946466809422,
      "loss": 0.1289,
      "step": 1381
    },
    {
      "epoch": 0.7400267737617136,
      "grad_norm": 0.6115788221359253,
      "learning_rate": 0.00015071377587437545,
      "loss": 0.0912,
      "step": 1382
    },
    {
      "epoch": 0.7405622489959839,
      "grad_norm": 0.4239937663078308,
      "learning_rate": 0.00015067808708065666,
      "loss": 0.0756,
      "step": 1383
    },
    {
      "epoch": 0.7410977242302543,
      "grad_norm": 8.65427017211914,
      "learning_rate": 0.00015064239828693792,
      "loss": 0.1761,
      "step": 1384
    },
    {
      "epoch": 0.7416331994645248,
      "grad_norm": 0.5186752080917358,
      "learning_rate": 0.00015060670949321913,
      "loss": 0.0879,
      "step": 1385
    },
    {
      "epoch": 0.7421686746987952,
      "grad_norm": 0.6465213894844055,
      "learning_rate": 0.00015057102069950036,
      "loss": 0.1161,
      "step": 1386
    },
    {
      "epoch": 0.7427041499330655,
      "grad_norm": 0.8738171458244324,
      "learning_rate": 0.0001505353319057816,
      "loss": 0.1603,
      "step": 1387
    },
    {
      "epoch": 0.743239625167336,
      "grad_norm": 4.881255149841309,
      "learning_rate": 0.00015049964311206283,
      "loss": 0.1897,
      "step": 1388
    },
    {
      "epoch": 0.7437751004016064,
      "grad_norm": 0.5752174854278564,
      "learning_rate": 0.00015046395431834406,
      "loss": 0.0636,
      "step": 1389
    },
    {
      "epoch": 0.7443105756358769,
      "grad_norm": 0.5710172653198242,
      "learning_rate": 0.00015042826552462527,
      "loss": 0.0779,
      "step": 1390
    },
    {
      "epoch": 0.7448460508701472,
      "grad_norm": 0.4291861355304718,
      "learning_rate": 0.0001503925767309065,
      "loss": 0.1051,
      "step": 1391
    },
    {
      "epoch": 0.7453815261044177,
      "grad_norm": 0.5529207587242126,
      "learning_rate": 0.00015035688793718773,
      "loss": 0.0887,
      "step": 1392
    },
    {
      "epoch": 0.7459170013386881,
      "grad_norm": 0.6424288749694824,
      "learning_rate": 0.00015032119914346897,
      "loss": 0.1241,
      "step": 1393
    },
    {
      "epoch": 0.7464524765729585,
      "grad_norm": 1.0023226737976074,
      "learning_rate": 0.00015028551034975017,
      "loss": 0.0844,
      "step": 1394
    },
    {
      "epoch": 0.7469879518072289,
      "grad_norm": 0.7222315073013306,
      "learning_rate": 0.00015024982155603143,
      "loss": 0.1316,
      "step": 1395
    },
    {
      "epoch": 0.7475234270414993,
      "grad_norm": 0.688190221786499,
      "learning_rate": 0.00015021413276231264,
      "loss": 0.0741,
      "step": 1396
    },
    {
      "epoch": 0.7480589022757698,
      "grad_norm": 6.15479040145874,
      "learning_rate": 0.00015017844396859387,
      "loss": 0.1901,
      "step": 1397
    },
    {
      "epoch": 0.7485943775100402,
      "grad_norm": 0.5732587575912476,
      "learning_rate": 0.0001501427551748751,
      "loss": 0.1296,
      "step": 1398
    },
    {
      "epoch": 0.7491298527443105,
      "grad_norm": 0.39884036779403687,
      "learning_rate": 0.00015010706638115631,
      "loss": 0.0643,
      "step": 1399
    },
    {
      "epoch": 0.749665327978581,
      "grad_norm": 0.5179086327552795,
      "learning_rate": 0.00015007137758743755,
      "loss": 0.1409,
      "step": 1400
    },
    {
      "epoch": 0.7502008032128514,
      "grad_norm": 4.982760429382324,
      "learning_rate": 0.00015003568879371878,
      "loss": 0.0648,
      "step": 1401
    },
    {
      "epoch": 0.7507362784471219,
      "grad_norm": 0.21699517965316772,
      "learning_rate": 0.00015000000000000001,
      "loss": 0.0327,
      "step": 1402
    },
    {
      "epoch": 0.7512717536813922,
      "grad_norm": 0.5522528290748596,
      "learning_rate": 0.00014996431120628122,
      "loss": 0.0783,
      "step": 1403
    },
    {
      "epoch": 0.7518072289156627,
      "grad_norm": 0.531777560710907,
      "learning_rate": 0.00014992862241256248,
      "loss": 0.1399,
      "step": 1404
    },
    {
      "epoch": 0.7523427041499331,
      "grad_norm": 0.4864193797111511,
      "learning_rate": 0.0001498929336188437,
      "loss": 0.1231,
      "step": 1405
    },
    {
      "epoch": 0.7528781793842034,
      "grad_norm": 0.4941517114639282,
      "learning_rate": 0.00014985724482512492,
      "loss": 0.0831,
      "step": 1406
    },
    {
      "epoch": 0.7534136546184739,
      "grad_norm": 0.7448712587356567,
      "learning_rate": 0.00014982155603140615,
      "loss": 0.2187,
      "step": 1407
    },
    {
      "epoch": 0.7539491298527443,
      "grad_norm": 1.8950653076171875,
      "learning_rate": 0.00014978586723768736,
      "loss": 0.2702,
      "step": 1408
    },
    {
      "epoch": 0.7544846050870148,
      "grad_norm": 0.9226006865501404,
      "learning_rate": 0.0001497501784439686,
      "loss": 0.1824,
      "step": 1409
    },
    {
      "epoch": 0.7550200803212851,
      "grad_norm": 2.70626163482666,
      "learning_rate": 0.00014971448965024983,
      "loss": 0.091,
      "step": 1410
    },
    {
      "epoch": 0.7555555555555555,
      "grad_norm": 0.7446577548980713,
      "learning_rate": 0.00014967880085653106,
      "loss": 0.146,
      "step": 1411
    },
    {
      "epoch": 0.756091030789826,
      "grad_norm": 0.4562889635562897,
      "learning_rate": 0.00014964311206281227,
      "loss": 0.1221,
      "step": 1412
    },
    {
      "epoch": 0.7566265060240964,
      "grad_norm": 0.4227222800254822,
      "learning_rate": 0.00014960742326909353,
      "loss": 0.0848,
      "step": 1413
    },
    {
      "epoch": 0.7571619812583668,
      "grad_norm": 0.44486668705940247,
      "learning_rate": 0.00014957173447537473,
      "loss": 0.037,
      "step": 1414
    },
    {
      "epoch": 0.7576974564926372,
      "grad_norm": 0.44289225339889526,
      "learning_rate": 0.00014953604568165597,
      "loss": 0.1056,
      "step": 1415
    },
    {
      "epoch": 0.7582329317269076,
      "grad_norm": 0.724788248538971,
      "learning_rate": 0.0001495003568879372,
      "loss": 0.1814,
      "step": 1416
    },
    {
      "epoch": 0.7587684069611781,
      "grad_norm": 0.4042039215564728,
      "learning_rate": 0.0001494646680942184,
      "loss": 0.0914,
      "step": 1417
    },
    {
      "epoch": 0.7593038821954484,
      "grad_norm": 0.6658592224121094,
      "learning_rate": 0.00014942897930049967,
      "loss": 0.0692,
      "step": 1418
    },
    {
      "epoch": 0.7598393574297189,
      "grad_norm": 1.223788857460022,
      "learning_rate": 0.00014939329050678087,
      "loss": 0.131,
      "step": 1419
    },
    {
      "epoch": 0.7603748326639893,
      "grad_norm": 0.7573360800743103,
      "learning_rate": 0.0001493576017130621,
      "loss": 0.0806,
      "step": 1420
    },
    {
      "epoch": 0.7609103078982598,
      "grad_norm": 1.971360206604004,
      "learning_rate": 0.00014932191291934334,
      "loss": 0.1348,
      "step": 1421
    },
    {
      "epoch": 0.7614457831325301,
      "grad_norm": 0.3199903070926666,
      "learning_rate": 0.00014928622412562457,
      "loss": 0.0562,
      "step": 1422
    },
    {
      "epoch": 0.7619812583668005,
      "grad_norm": 0.394937127828598,
      "learning_rate": 0.00014925053533190578,
      "loss": 0.1023,
      "step": 1423
    },
    {
      "epoch": 0.762516733601071,
      "grad_norm": 0.48280438780784607,
      "learning_rate": 0.00014921484653818701,
      "loss": 0.1099,
      "step": 1424
    },
    {
      "epoch": 0.7630522088353414,
      "grad_norm": 0.61848384141922,
      "learning_rate": 0.00014917915774446825,
      "loss": 0.0879,
      "step": 1425
    },
    {
      "epoch": 0.7635876840696117,
      "grad_norm": 4.776728630065918,
      "learning_rate": 0.00014914346895074945,
      "loss": 0.122,
      "step": 1426
    },
    {
      "epoch": 0.7641231593038822,
      "grad_norm": 0.7073138356208801,
      "learning_rate": 0.00014910778015703071,
      "loss": 0.1389,
      "step": 1427
    },
    {
      "epoch": 0.7646586345381526,
      "grad_norm": 0.39539265632629395,
      "learning_rate": 0.00014907209136331192,
      "loss": 0.1005,
      "step": 1428
    },
    {
      "epoch": 0.765194109772423,
      "grad_norm": 1.3393654823303223,
      "learning_rate": 0.00014903640256959315,
      "loss": 0.0364,
      "step": 1429
    },
    {
      "epoch": 0.7657295850066934,
      "grad_norm": 0.633110761642456,
      "learning_rate": 0.0001490007137758744,
      "loss": 0.1545,
      "step": 1430
    },
    {
      "epoch": 0.7662650602409639,
      "grad_norm": 0.49394553899765015,
      "learning_rate": 0.00014896502498215562,
      "loss": 0.1028,
      "step": 1431
    },
    {
      "epoch": 0.7668005354752343,
      "grad_norm": 0.2136402279138565,
      "learning_rate": 0.00014892933618843683,
      "loss": 0.0371,
      "step": 1432
    },
    {
      "epoch": 0.7673360107095046,
      "grad_norm": 0.719497561454773,
      "learning_rate": 0.00014889364739471806,
      "loss": 0.1172,
      "step": 1433
    },
    {
      "epoch": 0.7678714859437751,
      "grad_norm": 0.728245198726654,
      "learning_rate": 0.0001488579586009993,
      "loss": 0.0964,
      "step": 1434
    },
    {
      "epoch": 0.7684069611780455,
      "grad_norm": 0.6685951352119446,
      "learning_rate": 0.0001488222698072805,
      "loss": 0.0822,
      "step": 1435
    },
    {
      "epoch": 0.768942436412316,
      "grad_norm": 0.926531195640564,
      "learning_rate": 0.00014878658101356176,
      "loss": 0.1673,
      "step": 1436
    },
    {
      "epoch": 0.7694779116465863,
      "grad_norm": 1.8226677179336548,
      "learning_rate": 0.00014875089221984297,
      "loss": 0.1323,
      "step": 1437
    },
    {
      "epoch": 0.7700133868808567,
      "grad_norm": 1.1415894031524658,
      "learning_rate": 0.0001487152034261242,
      "loss": 0.1062,
      "step": 1438
    },
    {
      "epoch": 0.7705488621151272,
      "grad_norm": 0.8524804711341858,
      "learning_rate": 0.00014867951463240543,
      "loss": 0.0854,
      "step": 1439
    },
    {
      "epoch": 0.7710843373493976,
      "grad_norm": 1.109608769416809,
      "learning_rate": 0.00014864382583868667,
      "loss": 0.1225,
      "step": 1440
    },
    {
      "epoch": 0.771619812583668,
      "grad_norm": 1.5556365251541138,
      "learning_rate": 0.00014860813704496787,
      "loss": 0.1279,
      "step": 1441
    },
    {
      "epoch": 0.7721552878179384,
      "grad_norm": 1.0926066637039185,
      "learning_rate": 0.0001485724482512491,
      "loss": 0.1438,
      "step": 1442
    },
    {
      "epoch": 0.7726907630522089,
      "grad_norm": 0.8049495220184326,
      "learning_rate": 0.00014853675945753034,
      "loss": 0.1801,
      "step": 1443
    },
    {
      "epoch": 0.7732262382864793,
      "grad_norm": 2.0133216381073,
      "learning_rate": 0.00014850107066381157,
      "loss": 0.2154,
      "step": 1444
    },
    {
      "epoch": 0.7737617135207496,
      "grad_norm": 0.7713895440101624,
      "learning_rate": 0.0001484653818700928,
      "loss": 0.1375,
      "step": 1445
    },
    {
      "epoch": 0.7742971887550201,
      "grad_norm": 0.7850597500801086,
      "learning_rate": 0.00014842969307637401,
      "loss": 0.1256,
      "step": 1446
    },
    {
      "epoch": 0.7748326639892905,
      "grad_norm": 0.411117285490036,
      "learning_rate": 0.00014839400428265527,
      "loss": 0.0918,
      "step": 1447
    },
    {
      "epoch": 0.775368139223561,
      "grad_norm": 1.005375623703003,
      "learning_rate": 0.00014835831548893648,
      "loss": 0.1324,
      "step": 1448
    },
    {
      "epoch": 0.7759036144578313,
      "grad_norm": 1.9847842454910278,
      "learning_rate": 0.00014832262669521771,
      "loss": 0.1354,
      "step": 1449
    },
    {
      "epoch": 0.7764390896921017,
      "grad_norm": 0.2625100612640381,
      "learning_rate": 0.00014828693790149895,
      "loss": 0.0393,
      "step": 1450
    },
    {
      "epoch": 0.7769745649263722,
      "grad_norm": 0.5251755118370056,
      "learning_rate": 0.00014825124910778015,
      "loss": 0.0999,
      "step": 1451
    },
    {
      "epoch": 0.7775100401606426,
      "grad_norm": 0.7180200219154358,
      "learning_rate": 0.0001482155603140614,
      "loss": 0.1287,
      "step": 1452
    },
    {
      "epoch": 0.778045515394913,
      "grad_norm": 1.1800071001052856,
      "learning_rate": 0.00014817987152034262,
      "loss": 0.0974,
      "step": 1453
    },
    {
      "epoch": 0.7785809906291834,
      "grad_norm": 0.3189615309238434,
      "learning_rate": 0.00014814418272662385,
      "loss": 0.087,
      "step": 1454
    },
    {
      "epoch": 0.7791164658634538,
      "grad_norm": 1.99660062789917,
      "learning_rate": 0.00014810849393290506,
      "loss": 0.0682,
      "step": 1455
    },
    {
      "epoch": 0.7796519410977242,
      "grad_norm": 1.0671947002410889,
      "learning_rate": 0.00014807280513918632,
      "loss": 0.0911,
      "step": 1456
    },
    {
      "epoch": 0.7801874163319946,
      "grad_norm": 0.6191248893737793,
      "learning_rate": 0.00014803711634546753,
      "loss": 0.1115,
      "step": 1457
    },
    {
      "epoch": 0.7807228915662651,
      "grad_norm": 0.5393907427787781,
      "learning_rate": 0.00014800142755174876,
      "loss": 0.1338,
      "step": 1458
    },
    {
      "epoch": 0.7812583668005355,
      "grad_norm": 0.5661616325378418,
      "learning_rate": 0.00014796573875803,
      "loss": 0.1535,
      "step": 1459
    },
    {
      "epoch": 0.7817938420348058,
      "grad_norm": 0.4633846580982208,
      "learning_rate": 0.0001479300499643112,
      "loss": 0.0944,
      "step": 1460
    },
    {
      "epoch": 0.7823293172690763,
      "grad_norm": 0.7038483023643494,
      "learning_rate": 0.00014789436117059243,
      "loss": 0.1159,
      "step": 1461
    },
    {
      "epoch": 0.7828647925033467,
      "grad_norm": 4.555868148803711,
      "learning_rate": 0.00014785867237687367,
      "loss": 0.1333,
      "step": 1462
    },
    {
      "epoch": 0.7834002677376172,
      "grad_norm": 0.5529500842094421,
      "learning_rate": 0.0001478229835831549,
      "loss": 0.0639,
      "step": 1463
    },
    {
      "epoch": 0.7839357429718875,
      "grad_norm": 0.9306375980377197,
      "learning_rate": 0.0001477872947894361,
      "loss": 0.1254,
      "step": 1464
    },
    {
      "epoch": 0.784471218206158,
      "grad_norm": 0.6076040267944336,
      "learning_rate": 0.00014775160599571737,
      "loss": 0.1304,
      "step": 1465
    },
    {
      "epoch": 0.7850066934404284,
      "grad_norm": 0.3834415376186371,
      "learning_rate": 0.00014771591720199857,
      "loss": 0.066,
      "step": 1466
    },
    {
      "epoch": 0.7855421686746988,
      "grad_norm": 0.49353906512260437,
      "learning_rate": 0.0001476802284082798,
      "loss": 0.0543,
      "step": 1467
    },
    {
      "epoch": 0.7860776439089692,
      "grad_norm": 1.820120096206665,
      "learning_rate": 0.00014764453961456104,
      "loss": 0.0482,
      "step": 1468
    },
    {
      "epoch": 0.7866131191432396,
      "grad_norm": 0.8806355595588684,
      "learning_rate": 0.00014760885082084225,
      "loss": 0.0973,
      "step": 1469
    },
    {
      "epoch": 0.7871485943775101,
      "grad_norm": 0.4443012475967407,
      "learning_rate": 0.00014757316202712348,
      "loss": 0.0464,
      "step": 1470
    },
    {
      "epoch": 0.7876840696117805,
      "grad_norm": 0.8844009637832642,
      "learning_rate": 0.00014753747323340471,
      "loss": 0.1691,
      "step": 1471
    },
    {
      "epoch": 0.7882195448460508,
      "grad_norm": 1.4161728620529175,
      "learning_rate": 0.00014750178443968595,
      "loss": 0.0905,
      "step": 1472
    },
    {
      "epoch": 0.7887550200803213,
      "grad_norm": 1.0537947416305542,
      "learning_rate": 0.00014746609564596718,
      "loss": 0.1749,
      "step": 1473
    },
    {
      "epoch": 0.7892904953145917,
      "grad_norm": 0.5598026514053345,
      "learning_rate": 0.00014743040685224841,
      "loss": 0.0976,
      "step": 1474
    },
    {
      "epoch": 0.7898259705488622,
      "grad_norm": 1.2079523801803589,
      "learning_rate": 0.00014739471805852962,
      "loss": 0.135,
      "step": 1475
    },
    {
      "epoch": 0.7903614457831325,
      "grad_norm": 2.0838353633880615,
      "learning_rate": 0.00014735902926481085,
      "loss": 0.1155,
      "step": 1476
    },
    {
      "epoch": 0.7908969210174029,
      "grad_norm": 0.6221256852149963,
      "learning_rate": 0.0001473233404710921,
      "loss": 0.1609,
      "step": 1477
    },
    {
      "epoch": 0.7914323962516734,
      "grad_norm": 0.31004446744918823,
      "learning_rate": 0.0001472876516773733,
      "loss": 0.0513,
      "step": 1478
    },
    {
      "epoch": 0.7919678714859437,
      "grad_norm": 3.1015844345092773,
      "learning_rate": 0.00014725196288365455,
      "loss": 0.1554,
      "step": 1479
    },
    {
      "epoch": 0.7925033467202142,
      "grad_norm": 0.49884527921676636,
      "learning_rate": 0.00014721627408993576,
      "loss": 0.1053,
      "step": 1480
    },
    {
      "epoch": 0.7930388219544846,
      "grad_norm": 0.5667746067047119,
      "learning_rate": 0.000147180585296217,
      "loss": 0.0707,
      "step": 1481
    },
    {
      "epoch": 0.793574297188755,
      "grad_norm": 6.255638599395752,
      "learning_rate": 0.00014714489650249823,
      "loss": 0.1728,
      "step": 1482
    },
    {
      "epoch": 0.7941097724230254,
      "grad_norm": 0.2703576982021332,
      "learning_rate": 0.00014710920770877946,
      "loss": 0.0446,
      "step": 1483
    },
    {
      "epoch": 0.7946452476572958,
      "grad_norm": 0.8402599692344666,
      "learning_rate": 0.00014707351891506067,
      "loss": 0.195,
      "step": 1484
    },
    {
      "epoch": 0.7951807228915663,
      "grad_norm": 1.1387369632720947,
      "learning_rate": 0.0001470378301213419,
      "loss": 0.1232,
      "step": 1485
    },
    {
      "epoch": 0.7957161981258367,
      "grad_norm": 0.7124828100204468,
      "learning_rate": 0.00014700214132762313,
      "loss": 0.0265,
      "step": 1486
    },
    {
      "epoch": 0.796251673360107,
      "grad_norm": 0.35933399200439453,
      "learning_rate": 0.00014696645253390434,
      "loss": 0.0589,
      "step": 1487
    },
    {
      "epoch": 0.7967871485943775,
      "grad_norm": 0.5902355313301086,
      "learning_rate": 0.0001469307637401856,
      "loss": 0.1098,
      "step": 1488
    },
    {
      "epoch": 0.7973226238286479,
      "grad_norm": 0.5583535432815552,
      "learning_rate": 0.0001468950749464668,
      "loss": 0.0966,
      "step": 1489
    },
    {
      "epoch": 0.7978580990629184,
      "grad_norm": 4.4928975105285645,
      "learning_rate": 0.00014685938615274804,
      "loss": 0.1248,
      "step": 1490
    },
    {
      "epoch": 0.7983935742971887,
      "grad_norm": 0.4206887185573578,
      "learning_rate": 0.00014682369735902927,
      "loss": 0.0945,
      "step": 1491
    },
    {
      "epoch": 0.7989290495314592,
      "grad_norm": 1.6461960077285767,
      "learning_rate": 0.0001467880085653105,
      "loss": 0.106,
      "step": 1492
    },
    {
      "epoch": 0.7994645247657296,
      "grad_norm": 0.4920297861099243,
      "learning_rate": 0.0001467523197715917,
      "loss": 0.1162,
      "step": 1493
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.1302139312028885,
      "learning_rate": 0.00014671663097787295,
      "loss": 0.0166,
      "step": 1494
    },
    {
      "epoch": 0.8005354752342704,
      "grad_norm": 0.6841075420379639,
      "learning_rate": 0.00014668094218415418,
      "loss": 0.1142,
      "step": 1495
    },
    {
      "epoch": 0.8010709504685408,
      "grad_norm": 0.7133445143699646,
      "learning_rate": 0.0001466452533904354,
      "loss": 0.1038,
      "step": 1496
    },
    {
      "epoch": 0.8016064257028113,
      "grad_norm": 1.074116587638855,
      "learning_rate": 0.00014660956459671665,
      "loss": 0.1741,
      "step": 1497
    },
    {
      "epoch": 0.8021419009370817,
      "grad_norm": 0.30154120922088623,
      "learning_rate": 0.00014657387580299785,
      "loss": 0.0617,
      "step": 1498
    },
    {
      "epoch": 0.802677376171352,
      "grad_norm": 0.44513577222824097,
      "learning_rate": 0.00014653818700927911,
      "loss": 0.059,
      "step": 1499
    },
    {
      "epoch": 0.8032128514056225,
      "grad_norm": 0.3582855463027954,
      "learning_rate": 0.00014650249821556032,
      "loss": 0.0551,
      "step": 1500
    },
    {
      "epoch": 0.8037483266398929,
      "grad_norm": 0.7966866493225098,
      "learning_rate": 0.00014646680942184155,
      "loss": 0.1112,
      "step": 1501
    },
    {
      "epoch": 0.8042838018741634,
      "grad_norm": 4.232263565063477,
      "learning_rate": 0.0001464311206281228,
      "loss": 0.165,
      "step": 1502
    },
    {
      "epoch": 0.8048192771084337,
      "grad_norm": 3.7549498081207275,
      "learning_rate": 0.000146395431834404,
      "loss": 0.3507,
      "step": 1503
    },
    {
      "epoch": 0.8053547523427041,
      "grad_norm": 0.7307643890380859,
      "learning_rate": 0.00014635974304068523,
      "loss": 0.1605,
      "step": 1504
    },
    {
      "epoch": 0.8058902275769746,
      "grad_norm": 0.8994458913803101,
      "learning_rate": 0.00014632405424696646,
      "loss": 0.0933,
      "step": 1505
    },
    {
      "epoch": 0.8064257028112449,
      "grad_norm": 0.31072714924812317,
      "learning_rate": 0.0001462883654532477,
      "loss": 0.0715,
      "step": 1506
    },
    {
      "epoch": 0.8069611780455154,
      "grad_norm": 0.26193392276763916,
      "learning_rate": 0.0001462526766595289,
      "loss": 0.052,
      "step": 1507
    },
    {
      "epoch": 0.8074966532797858,
      "grad_norm": 0.6277575492858887,
      "learning_rate": 0.00014621698786581016,
      "loss": 0.0948,
      "step": 1508
    },
    {
      "epoch": 0.8080321285140563,
      "grad_norm": 0.6910203099250793,
      "learning_rate": 0.00014618129907209137,
      "loss": 0.0804,
      "step": 1509
    },
    {
      "epoch": 0.8085676037483266,
      "grad_norm": 0.5963696837425232,
      "learning_rate": 0.0001461456102783726,
      "loss": 0.0775,
      "step": 1510
    },
    {
      "epoch": 0.809103078982597,
      "grad_norm": 0.6805954575538635,
      "learning_rate": 0.00014610992148465383,
      "loss": 0.1011,
      "step": 1511
    },
    {
      "epoch": 0.8096385542168675,
      "grad_norm": 0.5251073837280273,
      "learning_rate": 0.00014607423269093504,
      "loss": 0.1447,
      "step": 1512
    },
    {
      "epoch": 0.8101740294511379,
      "grad_norm": 0.44355350732803345,
      "learning_rate": 0.00014603854389721627,
      "loss": 0.0642,
      "step": 1513
    },
    {
      "epoch": 0.8107095046854083,
      "grad_norm": 0.5990267992019653,
      "learning_rate": 0.0001460028551034975,
      "loss": 0.0915,
      "step": 1514
    },
    {
      "epoch": 0.8112449799196787,
      "grad_norm": 1.1292140483856201,
      "learning_rate": 0.00014596716630977874,
      "loss": 0.1017,
      "step": 1515
    },
    {
      "epoch": 0.8117804551539491,
      "grad_norm": 0.28184428811073303,
      "learning_rate": 0.00014593147751605995,
      "loss": 0.026,
      "step": 1516
    },
    {
      "epoch": 0.8123159303882196,
      "grad_norm": 0.3893173038959503,
      "learning_rate": 0.0001458957887223412,
      "loss": 0.0778,
      "step": 1517
    },
    {
      "epoch": 0.8128514056224899,
      "grad_norm": 0.9193909764289856,
      "learning_rate": 0.00014586009992862241,
      "loss": 0.1861,
      "step": 1518
    },
    {
      "epoch": 0.8133868808567604,
      "grad_norm": 1.2983624935150146,
      "learning_rate": 0.00014582441113490365,
      "loss": 0.1149,
      "step": 1519
    },
    {
      "epoch": 0.8139223560910308,
      "grad_norm": 3.6821000576019287,
      "learning_rate": 0.00014578872234118488,
      "loss": 0.0743,
      "step": 1520
    },
    {
      "epoch": 0.8144578313253013,
      "grad_norm": 0.7487098574638367,
      "learning_rate": 0.0001457530335474661,
      "loss": 0.099,
      "step": 1521
    },
    {
      "epoch": 0.8149933065595716,
      "grad_norm": 1.9972201585769653,
      "learning_rate": 0.00014571734475374732,
      "loss": 0.1377,
      "step": 1522
    },
    {
      "epoch": 0.815528781793842,
      "grad_norm": 0.46389907598495483,
      "learning_rate": 0.00014568165596002855,
      "loss": 0.059,
      "step": 1523
    },
    {
      "epoch": 0.8160642570281125,
      "grad_norm": 0.651577353477478,
      "learning_rate": 0.0001456459671663098,
      "loss": 0.0837,
      "step": 1524
    },
    {
      "epoch": 0.8165997322623829,
      "grad_norm": 0.5413867235183716,
      "learning_rate": 0.000145610278372591,
      "loss": 0.0932,
      "step": 1525
    },
    {
      "epoch": 0.8171352074966532,
      "grad_norm": 2.4400036334991455,
      "learning_rate": 0.00014557458957887225,
      "loss": 0.1515,
      "step": 1526
    },
    {
      "epoch": 0.8176706827309237,
      "grad_norm": 0.760731041431427,
      "learning_rate": 0.00014553890078515346,
      "loss": 0.0782,
      "step": 1527
    },
    {
      "epoch": 0.8182061579651941,
      "grad_norm": 0.8245863318443298,
      "learning_rate": 0.0001455032119914347,
      "loss": 0.1146,
      "step": 1528
    },
    {
      "epoch": 0.8187416331994646,
      "grad_norm": 0.6071152687072754,
      "learning_rate": 0.00014546752319771593,
      "loss": 0.106,
      "step": 1529
    },
    {
      "epoch": 0.8192771084337349,
      "grad_norm": 0.3768329620361328,
      "learning_rate": 0.00014543183440399716,
      "loss": 0.0923,
      "step": 1530
    },
    {
      "epoch": 0.8198125836680054,
      "grad_norm": 0.534578800201416,
      "learning_rate": 0.0001453961456102784,
      "loss": 0.0678,
      "step": 1531
    },
    {
      "epoch": 0.8203480589022758,
      "grad_norm": 0.8989842534065247,
      "learning_rate": 0.0001453604568165596,
      "loss": 0.132,
      "step": 1532
    },
    {
      "epoch": 0.8208835341365461,
      "grad_norm": 0.40533965826034546,
      "learning_rate": 0.00014532476802284083,
      "loss": 0.0762,
      "step": 1533
    },
    {
      "epoch": 0.8214190093708166,
      "grad_norm": 0.6821181178092957,
      "learning_rate": 0.00014528907922912207,
      "loss": 0.0815,
      "step": 1534
    },
    {
      "epoch": 0.821954484605087,
      "grad_norm": 0.5504275560379028,
      "learning_rate": 0.0001452533904354033,
      "loss": 0.0801,
      "step": 1535
    },
    {
      "epoch": 0.8224899598393575,
      "grad_norm": 0.5265232920646667,
      "learning_rate": 0.0001452177016416845,
      "loss": 0.1351,
      "step": 1536
    },
    {
      "epoch": 0.8230254350736278,
      "grad_norm": 2.0798096656799316,
      "learning_rate": 0.00014518201284796574,
      "loss": 0.0912,
      "step": 1537
    },
    {
      "epoch": 0.8235609103078982,
      "grad_norm": 0.5049363970756531,
      "learning_rate": 0.00014514632405424697,
      "loss": 0.0814,
      "step": 1538
    },
    {
      "epoch": 0.8240963855421687,
      "grad_norm": 0.46778351068496704,
      "learning_rate": 0.0001451106352605282,
      "loss": 0.0764,
      "step": 1539
    },
    {
      "epoch": 0.8246318607764391,
      "grad_norm": 0.47383779287338257,
      "learning_rate": 0.00014507494646680944,
      "loss": 0.1635,
      "step": 1540
    },
    {
      "epoch": 0.8251673360107095,
      "grad_norm": 2.4704549312591553,
      "learning_rate": 0.00014503925767309065,
      "loss": 0.1002,
      "step": 1541
    },
    {
      "epoch": 0.8257028112449799,
      "grad_norm": 0.37522128224372864,
      "learning_rate": 0.00014500356887937188,
      "loss": 0.0481,
      "step": 1542
    },
    {
      "epoch": 0.8262382864792504,
      "grad_norm": 0.5013812780380249,
      "learning_rate": 0.00014496788008565311,
      "loss": 0.0669,
      "step": 1543
    },
    {
      "epoch": 0.8267737617135208,
      "grad_norm": 1.5999079942703247,
      "learning_rate": 0.00014493219129193435,
      "loss": 0.1482,
      "step": 1544
    },
    {
      "epoch": 0.8273092369477911,
      "grad_norm": 0.5844258069992065,
      "learning_rate": 0.00014489650249821555,
      "loss": 0.0738,
      "step": 1545
    },
    {
      "epoch": 0.8278447121820616,
      "grad_norm": 0.6412523984909058,
      "learning_rate": 0.00014486081370449681,
      "loss": 0.0815,
      "step": 1546
    },
    {
      "epoch": 0.828380187416332,
      "grad_norm": 1.3947993516921997,
      "learning_rate": 0.00014482512491077802,
      "loss": 0.1553,
      "step": 1547
    },
    {
      "epoch": 0.8289156626506025,
      "grad_norm": 0.5279886722564697,
      "learning_rate": 0.00014478943611705925,
      "loss": 0.143,
      "step": 1548
    },
    {
      "epoch": 0.8294511378848728,
      "grad_norm": 0.5814839601516724,
      "learning_rate": 0.0001447537473233405,
      "loss": 0.0891,
      "step": 1549
    },
    {
      "epoch": 0.8299866131191432,
      "grad_norm": 0.5051943063735962,
      "learning_rate": 0.0001447180585296217,
      "loss": 0.0667,
      "step": 1550
    },
    {
      "epoch": 0.8305220883534137,
      "grad_norm": 0.5636405944824219,
      "learning_rate": 0.00014468236973590293,
      "loss": 0.1151,
      "step": 1551
    },
    {
      "epoch": 0.8310575635876841,
      "grad_norm": 0.587683916091919,
      "learning_rate": 0.00014464668094218416,
      "loss": 0.0879,
      "step": 1552
    },
    {
      "epoch": 0.8315930388219545,
      "grad_norm": 3.337210178375244,
      "learning_rate": 0.0001446109921484654,
      "loss": 0.1089,
      "step": 1553
    },
    {
      "epoch": 0.8321285140562249,
      "grad_norm": 0.4499630033969879,
      "learning_rate": 0.00014457530335474663,
      "loss": 0.1149,
      "step": 1554
    },
    {
      "epoch": 0.8326639892904953,
      "grad_norm": 0.22997601330280304,
      "learning_rate": 0.00014453961456102786,
      "loss": 0.07,
      "step": 1555
    },
    {
      "epoch": 0.8331994645247657,
      "grad_norm": 0.48500823974609375,
      "learning_rate": 0.00014450392576730907,
      "loss": 0.0444,
      "step": 1556
    },
    {
      "epoch": 0.8337349397590361,
      "grad_norm": 0.3051151931285858,
      "learning_rate": 0.0001444682369735903,
      "loss": 0.0565,
      "step": 1557
    },
    {
      "epoch": 0.8342704149933066,
      "grad_norm": 0.7320799231529236,
      "learning_rate": 0.00014443254817987153,
      "loss": 0.1542,
      "step": 1558
    },
    {
      "epoch": 0.834805890227577,
      "grad_norm": 0.3785446584224701,
      "learning_rate": 0.00014439685938615274,
      "loss": 0.0904,
      "step": 1559
    },
    {
      "epoch": 0.8353413654618473,
      "grad_norm": 0.3296748697757721,
      "learning_rate": 0.000144361170592434,
      "loss": 0.0722,
      "step": 1560
    },
    {
      "epoch": 0.8358768406961178,
      "grad_norm": 4.1033148765563965,
      "learning_rate": 0.0001443254817987152,
      "loss": 0.2025,
      "step": 1561
    },
    {
      "epoch": 0.8364123159303882,
      "grad_norm": 0.8802558779716492,
      "learning_rate": 0.00014428979300499644,
      "loss": 0.0996,
      "step": 1562
    },
    {
      "epoch": 0.8369477911646587,
      "grad_norm": 0.34790298342704773,
      "learning_rate": 0.00014425410421127767,
      "loss": 0.0503,
      "step": 1563
    },
    {
      "epoch": 0.837483266398929,
      "grad_norm": 0.7059986591339111,
      "learning_rate": 0.0001442184154175589,
      "loss": 0.1067,
      "step": 1564
    },
    {
      "epoch": 0.8380187416331994,
      "grad_norm": 4.5341267585754395,
      "learning_rate": 0.0001441827266238401,
      "loss": 0.307,
      "step": 1565
    },
    {
      "epoch": 0.8385542168674699,
      "grad_norm": 0.2732158303260803,
      "learning_rate": 0.00014414703783012135,
      "loss": 0.0352,
      "step": 1566
    },
    {
      "epoch": 0.8390896921017403,
      "grad_norm": 3.3157694339752197,
      "learning_rate": 0.00014411134903640258,
      "loss": 0.2663,
      "step": 1567
    },
    {
      "epoch": 0.8396251673360107,
      "grad_norm": 0.5765419006347656,
      "learning_rate": 0.0001440756602426838,
      "loss": 0.1029,
      "step": 1568
    },
    {
      "epoch": 0.8401606425702811,
      "grad_norm": 4.8532490730285645,
      "learning_rate": 0.00014403997144896505,
      "loss": 0.2224,
      "step": 1569
    },
    {
      "epoch": 0.8406961178045516,
      "grad_norm": 0.9721518754959106,
      "learning_rate": 0.00014400428265524625,
      "loss": 0.0424,
      "step": 1570
    },
    {
      "epoch": 0.841231593038822,
      "grad_norm": 0.7742151618003845,
      "learning_rate": 0.0001439685938615275,
      "loss": 0.0905,
      "step": 1571
    },
    {
      "epoch": 0.8417670682730923,
      "grad_norm": 0.5349134206771851,
      "learning_rate": 0.00014393290506780872,
      "loss": 0.0978,
      "step": 1572
    },
    {
      "epoch": 0.8423025435073628,
      "grad_norm": 0.586755633354187,
      "learning_rate": 0.00014389721627408995,
      "loss": 0.1377,
      "step": 1573
    },
    {
      "epoch": 0.8428380187416332,
      "grad_norm": 3.9732506275177,
      "learning_rate": 0.00014386152748037116,
      "loss": 0.1728,
      "step": 1574
    },
    {
      "epoch": 0.8433734939759037,
      "grad_norm": 0.42550843954086304,
      "learning_rate": 0.0001438258386866524,
      "loss": 0.088,
      "step": 1575
    },
    {
      "epoch": 0.843908969210174,
      "grad_norm": 0.5189884901046753,
      "learning_rate": 0.00014379014989293363,
      "loss": 0.1006,
      "step": 1576
    },
    {
      "epoch": 0.8444444444444444,
      "grad_norm": 0.43569326400756836,
      "learning_rate": 0.00014375446109921483,
      "loss": 0.0878,
      "step": 1577
    },
    {
      "epoch": 0.8449799196787149,
      "grad_norm": 0.2630288004875183,
      "learning_rate": 0.0001437187723054961,
      "loss": 0.042,
      "step": 1578
    },
    {
      "epoch": 0.8455153949129853,
      "grad_norm": 0.7029753923416138,
      "learning_rate": 0.0001436830835117773,
      "loss": 0.1564,
      "step": 1579
    },
    {
      "epoch": 0.8460508701472557,
      "grad_norm": 0.35962915420532227,
      "learning_rate": 0.00014364739471805853,
      "loss": 0.0585,
      "step": 1580
    },
    {
      "epoch": 0.8465863453815261,
      "grad_norm": 0.5581856966018677,
      "learning_rate": 0.00014361170592433977,
      "loss": 0.0678,
      "step": 1581
    },
    {
      "epoch": 0.8471218206157966,
      "grad_norm": 4.967388153076172,
      "learning_rate": 0.000143576017130621,
      "loss": 0.122,
      "step": 1582
    },
    {
      "epoch": 0.8476572958500669,
      "grad_norm": 0.6123006343841553,
      "learning_rate": 0.00014354032833690223,
      "loss": 0.0796,
      "step": 1583
    },
    {
      "epoch": 0.8481927710843373,
      "grad_norm": 0.5212255716323853,
      "learning_rate": 0.00014350463954318344,
      "loss": 0.1118,
      "step": 1584
    },
    {
      "epoch": 0.8487282463186078,
      "grad_norm": 0.2856506407260895,
      "learning_rate": 0.00014346895074946467,
      "loss": 0.065,
      "step": 1585
    },
    {
      "epoch": 0.8492637215528782,
      "grad_norm": 0.6718930602073669,
      "learning_rate": 0.0001434332619557459,
      "loss": 0.1367,
      "step": 1586
    },
    {
      "epoch": 0.8497991967871485,
      "grad_norm": 0.7407254576683044,
      "learning_rate": 0.00014339757316202714,
      "loss": 0.0686,
      "step": 1587
    },
    {
      "epoch": 0.850334672021419,
      "grad_norm": 0.3957250416278839,
      "learning_rate": 0.00014336188436830835,
      "loss": 0.0418,
      "step": 1588
    },
    {
      "epoch": 0.8508701472556894,
      "grad_norm": 0.40850529074668884,
      "learning_rate": 0.0001433261955745896,
      "loss": 0.0813,
      "step": 1589
    },
    {
      "epoch": 0.8514056224899599,
      "grad_norm": 0.724901556968689,
      "learning_rate": 0.0001432905067808708,
      "loss": 0.0393,
      "step": 1590
    },
    {
      "epoch": 0.8519410977242302,
      "grad_norm": 0.6266571283340454,
      "learning_rate": 0.00014325481798715205,
      "loss": 0.0784,
      "step": 1591
    },
    {
      "epoch": 0.8524765729585007,
      "grad_norm": 1.7543025016784668,
      "learning_rate": 0.00014321912919343328,
      "loss": 0.1097,
      "step": 1592
    },
    {
      "epoch": 0.8530120481927711,
      "grad_norm": 0.7694048881530762,
      "learning_rate": 0.0001431834403997145,
      "loss": 0.108,
      "step": 1593
    },
    {
      "epoch": 0.8535475234270415,
      "grad_norm": 2.290982723236084,
      "learning_rate": 0.00014314775160599572,
      "loss": 0.0559,
      "step": 1594
    },
    {
      "epoch": 0.8540829986613119,
      "grad_norm": 0.6653255820274353,
      "learning_rate": 0.00014311206281227695,
      "loss": 0.1108,
      "step": 1595
    },
    {
      "epoch": 0.8546184738955823,
      "grad_norm": 0.4458393454551697,
      "learning_rate": 0.0001430763740185582,
      "loss": 0.096,
      "step": 1596
    },
    {
      "epoch": 0.8551539491298528,
      "grad_norm": 0.9019449353218079,
      "learning_rate": 0.0001430406852248394,
      "loss": 0.106,
      "step": 1597
    },
    {
      "epoch": 0.8556894243641232,
      "grad_norm": 1.2985330820083618,
      "learning_rate": 0.00014300499643112065,
      "loss": 0.075,
      "step": 1598
    },
    {
      "epoch": 0.8562248995983935,
      "grad_norm": 0.486941933631897,
      "learning_rate": 0.00014296930763740186,
      "loss": 0.063,
      "step": 1599
    },
    {
      "epoch": 0.856760374832664,
      "grad_norm": 0.630820631980896,
      "learning_rate": 0.0001429336188436831,
      "loss": 0.126,
      "step": 1600
    },
    {
      "epoch": 0.8572958500669344,
      "grad_norm": 0.9275423288345337,
      "learning_rate": 0.00014289793004996433,
      "loss": 0.2096,
      "step": 1601
    },
    {
      "epoch": 0.8578313253012049,
      "grad_norm": 0.3656240999698639,
      "learning_rate": 0.00014286224125624553,
      "loss": 0.1189,
      "step": 1602
    },
    {
      "epoch": 0.8583668005354752,
      "grad_norm": 0.33522912859916687,
      "learning_rate": 0.00014282655246252677,
      "loss": 0.1213,
      "step": 1603
    },
    {
      "epoch": 0.8589022757697456,
      "grad_norm": 0.9607125520706177,
      "learning_rate": 0.000142790863668808,
      "loss": 0.1621,
      "step": 1604
    },
    {
      "epoch": 0.8594377510040161,
      "grad_norm": 0.40185055136680603,
      "learning_rate": 0.00014275517487508923,
      "loss": 0.1417,
      "step": 1605
    },
    {
      "epoch": 0.8599732262382864,
      "grad_norm": 0.3557124435901642,
      "learning_rate": 0.00014271948608137044,
      "loss": 0.0292,
      "step": 1606
    },
    {
      "epoch": 0.8605087014725569,
      "grad_norm": 0.416548490524292,
      "learning_rate": 0.0001426837972876517,
      "loss": 0.1032,
      "step": 1607
    },
    {
      "epoch": 0.8610441767068273,
      "grad_norm": 0.4097362458705902,
      "learning_rate": 0.0001426481084939329,
      "loss": 0.0519,
      "step": 1608
    },
    {
      "epoch": 0.8615796519410978,
      "grad_norm": 0.5238605737686157,
      "learning_rate": 0.00014261241970021414,
      "loss": 0.1655,
      "step": 1609
    },
    {
      "epoch": 0.8621151271753681,
      "grad_norm": 1.0167434215545654,
      "learning_rate": 0.00014257673090649537,
      "loss": 0.1236,
      "step": 1610
    },
    {
      "epoch": 0.8626506024096385,
      "grad_norm": 0.5277453660964966,
      "learning_rate": 0.00014254104211277658,
      "loss": 0.0856,
      "step": 1611
    },
    {
      "epoch": 0.863186077643909,
      "grad_norm": 0.8562256693840027,
      "learning_rate": 0.00014250535331905784,
      "loss": 0.1824,
      "step": 1612
    },
    {
      "epoch": 0.8637215528781794,
      "grad_norm": 0.57368004322052,
      "learning_rate": 0.00014246966452533905,
      "loss": 0.1003,
      "step": 1613
    },
    {
      "epoch": 0.8642570281124498,
      "grad_norm": 0.3430359363555908,
      "learning_rate": 0.00014243397573162028,
      "loss": 0.0921,
      "step": 1614
    },
    {
      "epoch": 0.8647925033467202,
      "grad_norm": 4.322025775909424,
      "learning_rate": 0.00014239828693790151,
      "loss": 0.1194,
      "step": 1615
    },
    {
      "epoch": 0.8653279785809906,
      "grad_norm": 0.757547914981842,
      "learning_rate": 0.00014236259814418275,
      "loss": 0.175,
      "step": 1616
    },
    {
      "epoch": 0.8658634538152611,
      "grad_norm": 0.28753119707107544,
      "learning_rate": 0.00014232690935046395,
      "loss": 0.103,
      "step": 1617
    },
    {
      "epoch": 0.8663989290495314,
      "grad_norm": 1.4767210483551025,
      "learning_rate": 0.0001422912205567452,
      "loss": 0.1023,
      "step": 1618
    },
    {
      "epoch": 0.8669344042838019,
      "grad_norm": 0.09302232414484024,
      "learning_rate": 0.00014225553176302642,
      "loss": 0.0266,
      "step": 1619
    },
    {
      "epoch": 0.8674698795180723,
      "grad_norm": 0.7236466407775879,
      "learning_rate": 0.00014221984296930763,
      "loss": 0.127,
      "step": 1620
    },
    {
      "epoch": 0.8680053547523428,
      "grad_norm": 0.8829464912414551,
      "learning_rate": 0.0001421841541755889,
      "loss": 0.1444,
      "step": 1621
    },
    {
      "epoch": 0.8685408299866131,
      "grad_norm": 0.5720734000205994,
      "learning_rate": 0.0001421484653818701,
      "loss": 0.0819,
      "step": 1622
    },
    {
      "epoch": 0.8690763052208835,
      "grad_norm": 0.894488513469696,
      "learning_rate": 0.00014211277658815133,
      "loss": 0.1659,
      "step": 1623
    },
    {
      "epoch": 0.869611780455154,
      "grad_norm": 0.7749118208885193,
      "learning_rate": 0.00014207708779443256,
      "loss": 0.1754,
      "step": 1624
    },
    {
      "epoch": 0.8701472556894244,
      "grad_norm": 4.561514854431152,
      "learning_rate": 0.0001420413990007138,
      "loss": 0.1536,
      "step": 1625
    },
    {
      "epoch": 0.8706827309236947,
      "grad_norm": 0.7834731340408325,
      "learning_rate": 0.000142005710206995,
      "loss": 0.1713,
      "step": 1626
    },
    {
      "epoch": 0.8712182061579652,
      "grad_norm": 0.3370523154735565,
      "learning_rate": 0.00014197002141327623,
      "loss": 0.0484,
      "step": 1627
    },
    {
      "epoch": 0.8717536813922356,
      "grad_norm": 0.5612072348594666,
      "learning_rate": 0.00014193433261955747,
      "loss": 0.1968,
      "step": 1628
    },
    {
      "epoch": 0.8722891566265061,
      "grad_norm": 5.082962989807129,
      "learning_rate": 0.00014189864382583867,
      "loss": 0.25,
      "step": 1629
    },
    {
      "epoch": 0.8728246318607764,
      "grad_norm": 0.32038789987564087,
      "learning_rate": 0.00014186295503211993,
      "loss": 0.0659,
      "step": 1630
    },
    {
      "epoch": 0.8733601070950469,
      "grad_norm": 0.24053572118282318,
      "learning_rate": 0.00014182726623840114,
      "loss": 0.0492,
      "step": 1631
    },
    {
      "epoch": 0.8738955823293173,
      "grad_norm": 1.426559567451477,
      "learning_rate": 0.00014179157744468237,
      "loss": 0.1059,
      "step": 1632
    },
    {
      "epoch": 0.8744310575635876,
      "grad_norm": 0.44225379824638367,
      "learning_rate": 0.0001417558886509636,
      "loss": 0.1162,
      "step": 1633
    },
    {
      "epoch": 0.8749665327978581,
      "grad_norm": 0.3986215591430664,
      "learning_rate": 0.00014172019985724484,
      "loss": 0.0746,
      "step": 1634
    },
    {
      "epoch": 0.8755020080321285,
      "grad_norm": 0.6639775037765503,
      "learning_rate": 0.00014168451106352605,
      "loss": 0.154,
      "step": 1635
    },
    {
      "epoch": 0.876037483266399,
      "grad_norm": 0.6241652369499207,
      "learning_rate": 0.00014164882226980728,
      "loss": 0.1601,
      "step": 1636
    },
    {
      "epoch": 0.8765729585006693,
      "grad_norm": 0.7410919666290283,
      "learning_rate": 0.0001416131334760885,
      "loss": 0.2717,
      "step": 1637
    },
    {
      "epoch": 0.8771084337349397,
      "grad_norm": 0.46604618430137634,
      "learning_rate": 0.00014157744468236975,
      "loss": 0.1413,
      "step": 1638
    },
    {
      "epoch": 0.8776439089692102,
      "grad_norm": 0.47939634323120117,
      "learning_rate": 0.00014154175588865098,
      "loss": 0.1492,
      "step": 1639
    },
    {
      "epoch": 0.8781793842034806,
      "grad_norm": 0.4255427420139313,
      "learning_rate": 0.0001415060670949322,
      "loss": 0.1277,
      "step": 1640
    },
    {
      "epoch": 0.878714859437751,
      "grad_norm": 0.2992214262485504,
      "learning_rate": 0.00014147037830121345,
      "loss": 0.1031,
      "step": 1641
    },
    {
      "epoch": 0.8792503346720214,
      "grad_norm": 0.6231303811073303,
      "learning_rate": 0.00014143468950749465,
      "loss": 0.1246,
      "step": 1642
    },
    {
      "epoch": 0.8797858099062918,
      "grad_norm": 0.5598924160003662,
      "learning_rate": 0.0001413990007137759,
      "loss": 0.1706,
      "step": 1643
    },
    {
      "epoch": 0.8803212851405623,
      "grad_norm": 0.3953736126422882,
      "learning_rate": 0.00014136331192005712,
      "loss": 0.1052,
      "step": 1644
    },
    {
      "epoch": 0.8808567603748326,
      "grad_norm": 1.9297738075256348,
      "learning_rate": 0.00014132762312633833,
      "loss": 0.0655,
      "step": 1645
    },
    {
      "epoch": 0.8813922356091031,
      "grad_norm": 0.27580583095550537,
      "learning_rate": 0.00014129193433261956,
      "loss": 0.068,
      "step": 1646
    },
    {
      "epoch": 0.8819277108433735,
      "grad_norm": 0.5659658908843994,
      "learning_rate": 0.0001412562455389008,
      "loss": 0.0629,
      "step": 1647
    },
    {
      "epoch": 0.882463186077644,
      "grad_norm": 1.8364568948745728,
      "learning_rate": 0.00014122055674518203,
      "loss": 0.0673,
      "step": 1648
    },
    {
      "epoch": 0.8829986613119143,
      "grad_norm": 2.2539687156677246,
      "learning_rate": 0.00014118486795146323,
      "loss": 0.1188,
      "step": 1649
    },
    {
      "epoch": 0.8835341365461847,
      "grad_norm": 0.2410842478275299,
      "learning_rate": 0.0001411491791577445,
      "loss": 0.0333,
      "step": 1650
    },
    {
      "epoch": 0.8840696117804552,
      "grad_norm": 0.4091572165489197,
      "learning_rate": 0.0001411134903640257,
      "loss": 0.0746,
      "step": 1651
    },
    {
      "epoch": 0.8846050870147256,
      "grad_norm": 0.9719313383102417,
      "learning_rate": 0.00014107780157030693,
      "loss": 0.1559,
      "step": 1652
    },
    {
      "epoch": 0.885140562248996,
      "grad_norm": 3.416666269302368,
      "learning_rate": 0.00014104211277658817,
      "loss": 0.1828,
      "step": 1653
    },
    {
      "epoch": 0.8856760374832664,
      "grad_norm": 0.49500247836112976,
      "learning_rate": 0.00014100642398286937,
      "loss": 0.0744,
      "step": 1654
    },
    {
      "epoch": 0.8862115127175368,
      "grad_norm": 0.7485280632972717,
      "learning_rate": 0.0001409707351891506,
      "loss": 0.1397,
      "step": 1655
    },
    {
      "epoch": 0.8867469879518072,
      "grad_norm": 0.38243648409843445,
      "learning_rate": 0.00014093504639543184,
      "loss": 0.0967,
      "step": 1656
    },
    {
      "epoch": 0.8872824631860776,
      "grad_norm": 0.7332327961921692,
      "learning_rate": 0.00014089935760171307,
      "loss": 0.201,
      "step": 1657
    },
    {
      "epoch": 0.8878179384203481,
      "grad_norm": 0.7050690054893494,
      "learning_rate": 0.00014086366880799428,
      "loss": 0.1019,
      "step": 1658
    },
    {
      "epoch": 0.8883534136546185,
      "grad_norm": 0.599208414554596,
      "learning_rate": 0.00014082798001427554,
      "loss": 0.0941,
      "step": 1659
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 1.0975010395050049,
      "learning_rate": 0.00014079229122055675,
      "loss": 0.1463,
      "step": 1660
    },
    {
      "epoch": 0.8894243641231593,
      "grad_norm": 0.875365674495697,
      "learning_rate": 0.00014075660242683798,
      "loss": 0.1865,
      "step": 1661
    },
    {
      "epoch": 0.8899598393574297,
      "grad_norm": 0.6349136829376221,
      "learning_rate": 0.0001407209136331192,
      "loss": 0.107,
      "step": 1662
    },
    {
      "epoch": 0.8904953145917002,
      "grad_norm": 0.9940404295921326,
      "learning_rate": 0.00014068522483940042,
      "loss": 0.1319,
      "step": 1663
    },
    {
      "epoch": 0.8910307898259705,
      "grad_norm": 0.5661380290985107,
      "learning_rate": 0.00014064953604568168,
      "loss": 0.1137,
      "step": 1664
    },
    {
      "epoch": 0.891566265060241,
      "grad_norm": 0.5196921825408936,
      "learning_rate": 0.0001406138472519629,
      "loss": 0.1627,
      "step": 1665
    },
    {
      "epoch": 0.8921017402945114,
      "grad_norm": 0.37175217270851135,
      "learning_rate": 0.00014057815845824412,
      "loss": 0.0678,
      "step": 1666
    },
    {
      "epoch": 0.8926372155287818,
      "grad_norm": 0.3300309479236603,
      "learning_rate": 0.00014054246966452535,
      "loss": 0.0497,
      "step": 1667
    },
    {
      "epoch": 0.8931726907630522,
      "grad_norm": 0.5601951479911804,
      "learning_rate": 0.0001405067808708066,
      "loss": 0.1441,
      "step": 1668
    },
    {
      "epoch": 0.8937081659973226,
      "grad_norm": 0.5192617177963257,
      "learning_rate": 0.0001404710920770878,
      "loss": 0.0907,
      "step": 1669
    },
    {
      "epoch": 0.8942436412315931,
      "grad_norm": 0.41017451882362366,
      "learning_rate": 0.00014043540328336903,
      "loss": 0.0383,
      "step": 1670
    },
    {
      "epoch": 0.8947791164658635,
      "grad_norm": 0.3630172312259674,
      "learning_rate": 0.00014039971448965026,
      "loss": 0.0656,
      "step": 1671
    },
    {
      "epoch": 0.8953145917001338,
      "grad_norm": 0.7944452166557312,
      "learning_rate": 0.00014036402569593147,
      "loss": 0.117,
      "step": 1672
    },
    {
      "epoch": 0.8958500669344043,
      "grad_norm": 1.071341633796692,
      "learning_rate": 0.00014032833690221273,
      "loss": 0.1212,
      "step": 1673
    },
    {
      "epoch": 0.8963855421686747,
      "grad_norm": 0.5370748043060303,
      "learning_rate": 0.00014029264810849393,
      "loss": 0.115,
      "step": 1674
    },
    {
      "epoch": 0.8969210174029452,
      "grad_norm": 0.40054404735565186,
      "learning_rate": 0.00014025695931477517,
      "loss": 0.0903,
      "step": 1675
    },
    {
      "epoch": 0.8974564926372155,
      "grad_norm": 2.2627151012420654,
      "learning_rate": 0.0001402212705210564,
      "loss": 0.1785,
      "step": 1676
    },
    {
      "epoch": 0.8979919678714859,
      "grad_norm": 0.5628984570503235,
      "learning_rate": 0.00014018558172733763,
      "loss": 0.1641,
      "step": 1677
    },
    {
      "epoch": 0.8985274431057564,
      "grad_norm": 0.47669005393981934,
      "learning_rate": 0.00014014989293361884,
      "loss": 0.0921,
      "step": 1678
    },
    {
      "epoch": 0.8990629183400268,
      "grad_norm": 1.9105632305145264,
      "learning_rate": 0.00014011420413990007,
      "loss": 0.1735,
      "step": 1679
    },
    {
      "epoch": 0.8995983935742972,
      "grad_norm": 0.43033939599990845,
      "learning_rate": 0.0001400785153461813,
      "loss": 0.1741,
      "step": 1680
    },
    {
      "epoch": 0.9001338688085676,
      "grad_norm": 0.42557492852211,
      "learning_rate": 0.0001400428265524625,
      "loss": 0.1127,
      "step": 1681
    },
    {
      "epoch": 0.900669344042838,
      "grad_norm": 0.47056180238723755,
      "learning_rate": 0.00014000713775874377,
      "loss": 0.1111,
      "step": 1682
    },
    {
      "epoch": 0.9012048192771084,
      "grad_norm": 0.38430511951446533,
      "learning_rate": 0.00013997144896502498,
      "loss": 0.0856,
      "step": 1683
    },
    {
      "epoch": 0.9017402945113788,
      "grad_norm": 0.45229095220565796,
      "learning_rate": 0.0001399357601713062,
      "loss": 0.108,
      "step": 1684
    },
    {
      "epoch": 0.9022757697456493,
      "grad_norm": 0.36056050658226013,
      "learning_rate": 0.00013990007137758745,
      "loss": 0.1019,
      "step": 1685
    },
    {
      "epoch": 0.9028112449799197,
      "grad_norm": 0.46975958347320557,
      "learning_rate": 0.00013986438258386868,
      "loss": 0.0679,
      "step": 1686
    },
    {
      "epoch": 0.90334672021419,
      "grad_norm": 0.31197869777679443,
      "learning_rate": 0.0001398286937901499,
      "loss": 0.1076,
      "step": 1687
    },
    {
      "epoch": 0.9038821954484605,
      "grad_norm": 0.2295190393924713,
      "learning_rate": 0.00013979300499643112,
      "loss": 0.0499,
      "step": 1688
    },
    {
      "epoch": 0.9044176706827309,
      "grad_norm": 4.178244590759277,
      "learning_rate": 0.00013975731620271235,
      "loss": 0.1536,
      "step": 1689
    },
    {
      "epoch": 0.9049531459170014,
      "grad_norm": 0.32599174976348877,
      "learning_rate": 0.00013972162740899356,
      "loss": 0.074,
      "step": 1690
    },
    {
      "epoch": 0.9054886211512717,
      "grad_norm": 0.29812854528427124,
      "learning_rate": 0.00013968593861527482,
      "loss": 0.071,
      "step": 1691
    },
    {
      "epoch": 0.9060240963855422,
      "grad_norm": 0.19729281961917877,
      "learning_rate": 0.00013965024982155603,
      "loss": 0.0463,
      "step": 1692
    },
    {
      "epoch": 0.9065595716198126,
      "grad_norm": 0.460888147354126,
      "learning_rate": 0.0001396145610278373,
      "loss": 0.143,
      "step": 1693
    },
    {
      "epoch": 0.907095046854083,
      "grad_norm": 1.1534556150436401,
      "learning_rate": 0.0001395788722341185,
      "loss": 0.0715,
      "step": 1694
    },
    {
      "epoch": 0.9076305220883534,
      "grad_norm": 0.3494231104850769,
      "learning_rate": 0.00013954318344039973,
      "loss": 0.0965,
      "step": 1695
    },
    {
      "epoch": 0.9081659973226238,
      "grad_norm": 0.3224954605102539,
      "learning_rate": 0.00013950749464668096,
      "loss": 0.0749,
      "step": 1696
    },
    {
      "epoch": 0.9087014725568943,
      "grad_norm": 0.35308489203453064,
      "learning_rate": 0.00013947180585296217,
      "loss": 0.0263,
      "step": 1697
    },
    {
      "epoch": 0.9092369477911647,
      "grad_norm": 0.44789260625839233,
      "learning_rate": 0.0001394361170592434,
      "loss": 0.1062,
      "step": 1698
    },
    {
      "epoch": 0.909772423025435,
      "grad_norm": 1.0845001935958862,
      "learning_rate": 0.00013940042826552463,
      "loss": 0.1384,
      "step": 1699
    },
    {
      "epoch": 0.9103078982597055,
      "grad_norm": 0.34362348914146423,
      "learning_rate": 0.00013936473947180587,
      "loss": 0.0979,
      "step": 1700
    },
    {
      "epoch": 0.9108433734939759,
      "grad_norm": 1.2120670080184937,
      "learning_rate": 0.00013932905067808707,
      "loss": 0.139,
      "step": 1701
    },
    {
      "epoch": 0.9113788487282464,
      "grad_norm": 1.4703558683395386,
      "learning_rate": 0.00013929336188436833,
      "loss": 0.0524,
      "step": 1702
    },
    {
      "epoch": 0.9119143239625167,
      "grad_norm": 1.288569688796997,
      "learning_rate": 0.00013925767309064954,
      "loss": 0.1081,
      "step": 1703
    },
    {
      "epoch": 0.9124497991967871,
      "grad_norm": 0.389358788728714,
      "learning_rate": 0.00013922198429693077,
      "loss": 0.0632,
      "step": 1704
    },
    {
      "epoch": 0.9129852744310576,
      "grad_norm": 4.466228485107422,
      "learning_rate": 0.000139186295503212,
      "loss": 0.1921,
      "step": 1705
    },
    {
      "epoch": 0.9135207496653279,
      "grad_norm": 3.7429490089416504,
      "learning_rate": 0.0001391506067094932,
      "loss": 0.1236,
      "step": 1706
    },
    {
      "epoch": 0.9140562248995984,
      "grad_norm": 0.5169629454612732,
      "learning_rate": 0.00013911491791577445,
      "loss": 0.0814,
      "step": 1707
    },
    {
      "epoch": 0.9145917001338688,
      "grad_norm": 0.5220270752906799,
      "learning_rate": 0.00013907922912205568,
      "loss": 0.1646,
      "step": 1708
    },
    {
      "epoch": 0.9151271753681393,
      "grad_norm": 0.34823286533355713,
      "learning_rate": 0.0001390435403283369,
      "loss": 0.0789,
      "step": 1709
    },
    {
      "epoch": 0.9156626506024096,
      "grad_norm": 0.3827068507671356,
      "learning_rate": 0.00013900785153461812,
      "loss": 0.1094,
      "step": 1710
    },
    {
      "epoch": 0.91619812583668,
      "grad_norm": 0.5847347378730774,
      "learning_rate": 0.00013897216274089938,
      "loss": 0.0682,
      "step": 1711
    },
    {
      "epoch": 0.9167336010709505,
      "grad_norm": 1.0408847332000732,
      "learning_rate": 0.0001389364739471806,
      "loss": 0.1695,
      "step": 1712
    },
    {
      "epoch": 0.9172690763052209,
      "grad_norm": 0.872529149055481,
      "learning_rate": 0.00013890078515346182,
      "loss": 0.0836,
      "step": 1713
    },
    {
      "epoch": 0.9178045515394913,
      "grad_norm": 2.520015239715576,
      "learning_rate": 0.00013886509635974305,
      "loss": 0.0824,
      "step": 1714
    },
    {
      "epoch": 0.9183400267737617,
      "grad_norm": 0.9803007245063782,
      "learning_rate": 0.00013882940756602426,
      "loss": 0.1228,
      "step": 1715
    },
    {
      "epoch": 0.9188755020080321,
      "grad_norm": 0.6610777974128723,
      "learning_rate": 0.0001387937187723055,
      "loss": 0.1192,
      "step": 1716
    },
    {
      "epoch": 0.9194109772423026,
      "grad_norm": 4.301791191101074,
      "learning_rate": 0.00013875802997858673,
      "loss": 0.3031,
      "step": 1717
    },
    {
      "epoch": 0.9199464524765729,
      "grad_norm": 1.407585859298706,
      "learning_rate": 0.00013872234118486796,
      "loss": 0.2258,
      "step": 1718
    },
    {
      "epoch": 0.9204819277108434,
      "grad_norm": 0.36747631430625916,
      "learning_rate": 0.0001386866523911492,
      "loss": 0.0747,
      "step": 1719
    },
    {
      "epoch": 0.9210174029451138,
      "grad_norm": 1.7246372699737549,
      "learning_rate": 0.00013865096359743043,
      "loss": 0.1577,
      "step": 1720
    },
    {
      "epoch": 0.9215528781793842,
      "grad_norm": 3.4760804176330566,
      "learning_rate": 0.00013861527480371163,
      "loss": 0.1327,
      "step": 1721
    },
    {
      "epoch": 0.9220883534136546,
      "grad_norm": 0.7135468125343323,
      "learning_rate": 0.00013857958600999287,
      "loss": 0.1497,
      "step": 1722
    },
    {
      "epoch": 0.922623828647925,
      "grad_norm": 0.4770914614200592,
      "learning_rate": 0.0001385438972162741,
      "loss": 0.1256,
      "step": 1723
    },
    {
      "epoch": 0.9231593038821955,
      "grad_norm": 0.5609875917434692,
      "learning_rate": 0.0001385082084225553,
      "loss": 0.1011,
      "step": 1724
    },
    {
      "epoch": 0.9236947791164659,
      "grad_norm": 1.082449197769165,
      "learning_rate": 0.00013847251962883657,
      "loss": 0.1991,
      "step": 1725
    },
    {
      "epoch": 0.9242302543507362,
      "grad_norm": 0.521769106388092,
      "learning_rate": 0.00013843683083511777,
      "loss": 0.1238,
      "step": 1726
    },
    {
      "epoch": 0.9247657295850067,
      "grad_norm": 0.5362361669540405,
      "learning_rate": 0.000138401142041399,
      "loss": 0.0985,
      "step": 1727
    },
    {
      "epoch": 0.9253012048192771,
      "grad_norm": 0.6434279084205627,
      "learning_rate": 0.00013836545324768024,
      "loss": 0.0461,
      "step": 1728
    },
    {
      "epoch": 0.9258366800535476,
      "grad_norm": 0.26547059416770935,
      "learning_rate": 0.00013832976445396147,
      "loss": 0.0729,
      "step": 1729
    },
    {
      "epoch": 0.9263721552878179,
      "grad_norm": 0.4957509934902191,
      "learning_rate": 0.00013829407566024268,
      "loss": 0.1577,
      "step": 1730
    },
    {
      "epoch": 0.9269076305220884,
      "grad_norm": 0.554925262928009,
      "learning_rate": 0.0001382583868665239,
      "loss": 0.1091,
      "step": 1731
    },
    {
      "epoch": 0.9274431057563588,
      "grad_norm": 0.8733494281768799,
      "learning_rate": 0.00013822269807280515,
      "loss": 0.3856,
      "step": 1732
    },
    {
      "epoch": 0.9279785809906291,
      "grad_norm": 0.47188881039619446,
      "learning_rate": 0.00013818700927908635,
      "loss": 0.1266,
      "step": 1733
    },
    {
      "epoch": 0.9285140562248996,
      "grad_norm": 0.42323848605155945,
      "learning_rate": 0.0001381513204853676,
      "loss": 0.1389,
      "step": 1734
    },
    {
      "epoch": 0.92904953145917,
      "grad_norm": 0.4729992151260376,
      "learning_rate": 0.00013811563169164882,
      "loss": 0.1042,
      "step": 1735
    },
    {
      "epoch": 0.9295850066934405,
      "grad_norm": 0.2604999840259552,
      "learning_rate": 0.00013807994289793005,
      "loss": 0.0382,
      "step": 1736
    },
    {
      "epoch": 0.9301204819277108,
      "grad_norm": 0.6540669798851013,
      "learning_rate": 0.0001380442541042113,
      "loss": 0.2199,
      "step": 1737
    },
    {
      "epoch": 0.9306559571619812,
      "grad_norm": 0.9099178314208984,
      "learning_rate": 0.00013800856531049252,
      "loss": 0.1653,
      "step": 1738
    },
    {
      "epoch": 0.9311914323962517,
      "grad_norm": 1.1390130519866943,
      "learning_rate": 0.00013797287651677373,
      "loss": 0.056,
      "step": 1739
    },
    {
      "epoch": 0.9317269076305221,
      "grad_norm": 6.376377105712891,
      "learning_rate": 0.00013793718772305496,
      "loss": 0.1971,
      "step": 1740
    },
    {
      "epoch": 0.9322623828647925,
      "grad_norm": 0.34558653831481934,
      "learning_rate": 0.0001379014989293362,
      "loss": 0.1149,
      "step": 1741
    },
    {
      "epoch": 0.9327978580990629,
      "grad_norm": 0.5452317595481873,
      "learning_rate": 0.0001378658101356174,
      "loss": 0.1097,
      "step": 1742
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.3389696478843689,
      "learning_rate": 0.00013783012134189866,
      "loss": 0.043,
      "step": 1743
    },
    {
      "epoch": 0.9338688085676038,
      "grad_norm": 0.7162159085273743,
      "learning_rate": 0.00013779443254817987,
      "loss": 0.1351,
      "step": 1744
    },
    {
      "epoch": 0.9344042838018741,
      "grad_norm": 0.529093325138092,
      "learning_rate": 0.0001377587437544611,
      "loss": 0.088,
      "step": 1745
    },
    {
      "epoch": 0.9349397590361446,
      "grad_norm": 1.0625783205032349,
      "learning_rate": 0.00013772305496074233,
      "loss": 0.1827,
      "step": 1746
    },
    {
      "epoch": 0.935475234270415,
      "grad_norm": 2.6314620971679688,
      "learning_rate": 0.00013768736616702357,
      "loss": 0.1198,
      "step": 1747
    },
    {
      "epoch": 0.9360107095046855,
      "grad_norm": 0.43528831005096436,
      "learning_rate": 0.0001376516773733048,
      "loss": 0.129,
      "step": 1748
    },
    {
      "epoch": 0.9365461847389558,
      "grad_norm": 0.7628701329231262,
      "learning_rate": 0.000137615988579586,
      "loss": 0.1199,
      "step": 1749
    },
    {
      "epoch": 0.9370816599732262,
      "grad_norm": 0.6168137192726135,
      "learning_rate": 0.00013758029978586724,
      "loss": 0.125,
      "step": 1750
    },
    {
      "epoch": 0.9376171352074967,
      "grad_norm": 0.26952192187309265,
      "learning_rate": 0.00013754461099214847,
      "loss": 0.0492,
      "step": 1751
    },
    {
      "epoch": 0.9381526104417671,
      "grad_norm": 0.48063838481903076,
      "learning_rate": 0.0001375089221984297,
      "loss": 0.0521,
      "step": 1752
    },
    {
      "epoch": 0.9386880856760375,
      "grad_norm": 0.7136610150337219,
      "learning_rate": 0.0001374732334047109,
      "loss": 0.1462,
      "step": 1753
    },
    {
      "epoch": 0.9392235609103079,
      "grad_norm": 5.778210163116455,
      "learning_rate": 0.00013743754461099217,
      "loss": 0.1443,
      "step": 1754
    },
    {
      "epoch": 0.9397590361445783,
      "grad_norm": 0.4119228422641754,
      "learning_rate": 0.00013740185581727338,
      "loss": 0.0715,
      "step": 1755
    },
    {
      "epoch": 0.9402945113788487,
      "grad_norm": 4.149666786193848,
      "learning_rate": 0.0001373661670235546,
      "loss": 0.162,
      "step": 1756
    },
    {
      "epoch": 0.9408299866131191,
      "grad_norm": 0.4793182909488678,
      "learning_rate": 0.00013733047822983585,
      "loss": 0.1295,
      "step": 1757
    },
    {
      "epoch": 0.9413654618473896,
      "grad_norm": 0.40451163053512573,
      "learning_rate": 0.00013729478943611705,
      "loss": 0.0981,
      "step": 1758
    },
    {
      "epoch": 0.94190093708166,
      "grad_norm": 0.3953050971031189,
      "learning_rate": 0.0001372591006423983,
      "loss": 0.1253,
      "step": 1759
    },
    {
      "epoch": 0.9424364123159303,
      "grad_norm": 0.4637787938117981,
      "learning_rate": 0.00013722341184867952,
      "loss": 0.0628,
      "step": 1760
    },
    {
      "epoch": 0.9429718875502008,
      "grad_norm": 0.44276875257492065,
      "learning_rate": 0.00013718772305496075,
      "loss": 0.0841,
      "step": 1761
    },
    {
      "epoch": 0.9435073627844712,
      "grad_norm": 0.3063785135746002,
      "learning_rate": 0.00013715203426124196,
      "loss": 0.0449,
      "step": 1762
    },
    {
      "epoch": 0.9440428380187417,
      "grad_norm": 0.4432140290737152,
      "learning_rate": 0.00013711634546752322,
      "loss": 0.0624,
      "step": 1763
    },
    {
      "epoch": 0.944578313253012,
      "grad_norm": 0.3836517930030823,
      "learning_rate": 0.00013708065667380443,
      "loss": 0.0952,
      "step": 1764
    },
    {
      "epoch": 0.9451137884872824,
      "grad_norm": 0.43637341260910034,
      "learning_rate": 0.00013704496788008566,
      "loss": 0.1241,
      "step": 1765
    },
    {
      "epoch": 0.9456492637215529,
      "grad_norm": 0.4333951473236084,
      "learning_rate": 0.0001370092790863669,
      "loss": 0.1086,
      "step": 1766
    },
    {
      "epoch": 0.9461847389558233,
      "grad_norm": 5.559132099151611,
      "learning_rate": 0.0001369735902926481,
      "loss": 0.185,
      "step": 1767
    },
    {
      "epoch": 0.9467202141900937,
      "grad_norm": 8.708871841430664,
      "learning_rate": 0.00013693790149892933,
      "loss": 0.1782,
      "step": 1768
    },
    {
      "epoch": 0.9472556894243641,
      "grad_norm": 0.31803664565086365,
      "learning_rate": 0.00013690221270521057,
      "loss": 0.065,
      "step": 1769
    },
    {
      "epoch": 0.9477911646586346,
      "grad_norm": 1.1431243419647217,
      "learning_rate": 0.0001368665239114918,
      "loss": 0.1681,
      "step": 1770
    },
    {
      "epoch": 0.948326639892905,
      "grad_norm": 0.3742256462574005,
      "learning_rate": 0.000136830835117773,
      "loss": 0.0613,
      "step": 1771
    },
    {
      "epoch": 0.9488621151271753,
      "grad_norm": 0.44847580790519714,
      "learning_rate": 0.00013679514632405427,
      "loss": 0.1108,
      "step": 1772
    },
    {
      "epoch": 0.9493975903614458,
      "grad_norm": 0.5564761161804199,
      "learning_rate": 0.00013675945753033547,
      "loss": 0.1575,
      "step": 1773
    },
    {
      "epoch": 0.9499330655957162,
      "grad_norm": 0.5782579183578491,
      "learning_rate": 0.0001367237687366167,
      "loss": 0.1068,
      "step": 1774
    },
    {
      "epoch": 0.9504685408299867,
      "grad_norm": 0.5793911218643188,
      "learning_rate": 0.00013668807994289794,
      "loss": 0.1069,
      "step": 1775
    },
    {
      "epoch": 0.951004016064257,
      "grad_norm": 0.581373393535614,
      "learning_rate": 0.00013665239114917915,
      "loss": 0.1347,
      "step": 1776
    },
    {
      "epoch": 0.9515394912985274,
      "grad_norm": 0.5714746713638306,
      "learning_rate": 0.0001366167023554604,
      "loss": 0.0986,
      "step": 1777
    },
    {
      "epoch": 0.9520749665327979,
      "grad_norm": 0.5366060733795166,
      "learning_rate": 0.0001365810135617416,
      "loss": 0.0445,
      "step": 1778
    },
    {
      "epoch": 0.9526104417670683,
      "grad_norm": 0.8434257507324219,
      "learning_rate": 0.00013654532476802285,
      "loss": 0.0759,
      "step": 1779
    },
    {
      "epoch": 0.9531459170013387,
      "grad_norm": 0.886864423751831,
      "learning_rate": 0.00013650963597430408,
      "loss": 0.1523,
      "step": 1780
    },
    {
      "epoch": 0.9536813922356091,
      "grad_norm": 0.576518714427948,
      "learning_rate": 0.0001364739471805853,
      "loss": 0.0655,
      "step": 1781
    },
    {
      "epoch": 0.9542168674698795,
      "grad_norm": 0.8000361919403076,
      "learning_rate": 0.00013643825838686652,
      "loss": 0.1525,
      "step": 1782
    },
    {
      "epoch": 0.9547523427041499,
      "grad_norm": 0.4444895088672638,
      "learning_rate": 0.00013640256959314775,
      "loss": 0.1075,
      "step": 1783
    },
    {
      "epoch": 0.9552878179384203,
      "grad_norm": 0.43780118227005005,
      "learning_rate": 0.000136366880799429,
      "loss": 0.0828,
      "step": 1784
    },
    {
      "epoch": 0.9558232931726908,
      "grad_norm": 0.46784916520118713,
      "learning_rate": 0.00013633119200571022,
      "loss": 0.0781,
      "step": 1785
    },
    {
      "epoch": 0.9563587684069612,
      "grad_norm": 0.33199402689933777,
      "learning_rate": 0.00013629550321199145,
      "loss": 0.0621,
      "step": 1786
    },
    {
      "epoch": 0.9568942436412315,
      "grad_norm": 0.6585484147071838,
      "learning_rate": 0.00013625981441827266,
      "loss": 0.199,
      "step": 1787
    },
    {
      "epoch": 0.957429718875502,
      "grad_norm": 0.5631869435310364,
      "learning_rate": 0.0001362241256245539,
      "loss": 0.0946,
      "step": 1788
    },
    {
      "epoch": 0.9579651941097724,
      "grad_norm": 0.8976907134056091,
      "learning_rate": 0.00013618843683083513,
      "loss": 0.0894,
      "step": 1789
    },
    {
      "epoch": 0.9585006693440429,
      "grad_norm": 0.7966577410697937,
      "learning_rate": 0.00013615274803711636,
      "loss": 0.1344,
      "step": 1790
    },
    {
      "epoch": 0.9590361445783132,
      "grad_norm": 0.3453582525253296,
      "learning_rate": 0.00013611705924339757,
      "loss": 0.0448,
      "step": 1791
    },
    {
      "epoch": 0.9595716198125837,
      "grad_norm": 0.44673749804496765,
      "learning_rate": 0.00013608137044967883,
      "loss": 0.0715,
      "step": 1792
    },
    {
      "epoch": 0.9601070950468541,
      "grad_norm": 0.9322648644447327,
      "learning_rate": 0.00013604568165596003,
      "loss": 0.1131,
      "step": 1793
    },
    {
      "epoch": 0.9606425702811245,
      "grad_norm": 0.3803909420967102,
      "learning_rate": 0.00013600999286224127,
      "loss": 0.052,
      "step": 1794
    },
    {
      "epoch": 0.9611780455153949,
      "grad_norm": 0.5971953272819519,
      "learning_rate": 0.0001359743040685225,
      "loss": 0.1281,
      "step": 1795
    },
    {
      "epoch": 0.9617135207496653,
      "grad_norm": 0.4812183678150177,
      "learning_rate": 0.0001359386152748037,
      "loss": 0.0599,
      "step": 1796
    },
    {
      "epoch": 0.9622489959839358,
      "grad_norm": 0.5678354501724243,
      "learning_rate": 0.00013590292648108494,
      "loss": 0.155,
      "step": 1797
    },
    {
      "epoch": 0.9627844712182062,
      "grad_norm": 0.30414122343063354,
      "learning_rate": 0.00013586723768736617,
      "loss": 0.0401,
      "step": 1798
    },
    {
      "epoch": 0.9633199464524765,
      "grad_norm": 0.40741488337516785,
      "learning_rate": 0.0001358315488936474,
      "loss": 0.046,
      "step": 1799
    },
    {
      "epoch": 0.963855421686747,
      "grad_norm": 0.3400718569755554,
      "learning_rate": 0.0001357958600999286,
      "loss": 0.0671,
      "step": 1800
    },
    {
      "epoch": 0.9643908969210174,
      "grad_norm": 0.4896909296512604,
      "learning_rate": 0.00013576017130620987,
      "loss": 0.0825,
      "step": 1801
    },
    {
      "epoch": 0.9649263721552879,
      "grad_norm": 3.9951069355010986,
      "learning_rate": 0.00013572448251249108,
      "loss": 0.0561,
      "step": 1802
    },
    {
      "epoch": 0.9654618473895582,
      "grad_norm": 0.4678346514701843,
      "learning_rate": 0.0001356887937187723,
      "loss": 0.1134,
      "step": 1803
    },
    {
      "epoch": 0.9659973226238286,
      "grad_norm": 0.4393758475780487,
      "learning_rate": 0.00013565310492505355,
      "loss": 0.0918,
      "step": 1804
    },
    {
      "epoch": 0.9665327978580991,
      "grad_norm": 0.5078723430633545,
      "learning_rate": 0.00013561741613133475,
      "loss": 0.0566,
      "step": 1805
    },
    {
      "epoch": 0.9670682730923694,
      "grad_norm": 0.5815538167953491,
      "learning_rate": 0.000135581727337616,
      "loss": 0.1331,
      "step": 1806
    },
    {
      "epoch": 0.9676037483266399,
      "grad_norm": 0.3834075331687927,
      "learning_rate": 0.00013554603854389722,
      "loss": 0.059,
      "step": 1807
    },
    {
      "epoch": 0.9681392235609103,
      "grad_norm": 7.120383262634277,
      "learning_rate": 0.00013551034975017845,
      "loss": 0.201,
      "step": 1808
    },
    {
      "epoch": 0.9686746987951808,
      "grad_norm": 0.7033491134643555,
      "learning_rate": 0.0001354746609564597,
      "loss": 0.1127,
      "step": 1809
    },
    {
      "epoch": 0.9692101740294511,
      "grad_norm": 0.635881245136261,
      "learning_rate": 0.00013543897216274092,
      "loss": 0.1568,
      "step": 1810
    },
    {
      "epoch": 0.9697456492637215,
      "grad_norm": 0.624646782875061,
      "learning_rate": 0.00013540328336902213,
      "loss": 0.0916,
      "step": 1811
    },
    {
      "epoch": 0.970281124497992,
      "grad_norm": 3.0272369384765625,
      "learning_rate": 0.00013536759457530336,
      "loss": 0.173,
      "step": 1812
    },
    {
      "epoch": 0.9708165997322624,
      "grad_norm": 0.35822737216949463,
      "learning_rate": 0.0001353319057815846,
      "loss": 0.0681,
      "step": 1813
    },
    {
      "epoch": 0.9713520749665328,
      "grad_norm": 0.20431971549987793,
      "learning_rate": 0.0001352962169878658,
      "loss": 0.0475,
      "step": 1814
    },
    {
      "epoch": 0.9718875502008032,
      "grad_norm": 0.36481088399887085,
      "learning_rate": 0.00013526052819414706,
      "loss": 0.0819,
      "step": 1815
    },
    {
      "epoch": 0.9724230254350736,
      "grad_norm": 2.3014793395996094,
      "learning_rate": 0.00013522483940042827,
      "loss": 0.0481,
      "step": 1816
    },
    {
      "epoch": 0.9729585006693441,
      "grad_norm": 0.46597787737846375,
      "learning_rate": 0.0001351891506067095,
      "loss": 0.0985,
      "step": 1817
    },
    {
      "epoch": 0.9734939759036144,
      "grad_norm": 1.4733318090438843,
      "learning_rate": 0.00013515346181299073,
      "loss": 0.0847,
      "step": 1818
    },
    {
      "epoch": 0.9740294511378849,
      "grad_norm": 10.297545433044434,
      "learning_rate": 0.00013511777301927197,
      "loss": 0.1633,
      "step": 1819
    },
    {
      "epoch": 0.9745649263721553,
      "grad_norm": 1.1595920324325562,
      "learning_rate": 0.00013508208422555317,
      "loss": 0.1467,
      "step": 1820
    },
    {
      "epoch": 0.9751004016064257,
      "grad_norm": 0.5917824506759644,
      "learning_rate": 0.0001350463954318344,
      "loss": 0.0747,
      "step": 1821
    },
    {
      "epoch": 0.9756358768406961,
      "grad_norm": 1.3537076711654663,
      "learning_rate": 0.00013501070663811564,
      "loss": 0.1556,
      "step": 1822
    },
    {
      "epoch": 0.9761713520749665,
      "grad_norm": 0.6302539110183716,
      "learning_rate": 0.00013497501784439685,
      "loss": 0.044,
      "step": 1823
    },
    {
      "epoch": 0.976706827309237,
      "grad_norm": 0.6782301068305969,
      "learning_rate": 0.0001349393290506781,
      "loss": 0.0985,
      "step": 1824
    },
    {
      "epoch": 0.9772423025435074,
      "grad_norm": 1.0266059637069702,
      "learning_rate": 0.0001349036402569593,
      "loss": 0.1297,
      "step": 1825
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 1.249212384223938,
      "learning_rate": 0.00013486795146324055,
      "loss": 0.1318,
      "step": 1826
    },
    {
      "epoch": 0.9783132530120482,
      "grad_norm": 0.8178799748420715,
      "learning_rate": 0.00013483226266952178,
      "loss": 0.103,
      "step": 1827
    },
    {
      "epoch": 0.9788487282463186,
      "grad_norm": 0.4225347340106964,
      "learning_rate": 0.000134796573875803,
      "loss": 0.107,
      "step": 1828
    },
    {
      "epoch": 0.9793842034805891,
      "grad_norm": 0.641507089138031,
      "learning_rate": 0.00013476088508208425,
      "loss": 0.0997,
      "step": 1829
    },
    {
      "epoch": 0.9799196787148594,
      "grad_norm": 1.5680758953094482,
      "learning_rate": 0.00013472519628836545,
      "loss": 0.113,
      "step": 1830
    },
    {
      "epoch": 0.9804551539491299,
      "grad_norm": 0.43890371918678284,
      "learning_rate": 0.00013468950749464669,
      "loss": 0.0603,
      "step": 1831
    },
    {
      "epoch": 0.9809906291834003,
      "grad_norm": 0.5710662603378296,
      "learning_rate": 0.00013465381870092792,
      "loss": 0.1232,
      "step": 1832
    },
    {
      "epoch": 0.9815261044176706,
      "grad_norm": 0.7071980237960815,
      "learning_rate": 0.00013461812990720915,
      "loss": 0.1204,
      "step": 1833
    },
    {
      "epoch": 0.9820615796519411,
      "grad_norm": 0.5005481243133545,
      "learning_rate": 0.00013458244111349036,
      "loss": 0.0981,
      "step": 1834
    },
    {
      "epoch": 0.9825970548862115,
      "grad_norm": 0.6571415066719055,
      "learning_rate": 0.00013454675231977162,
      "loss": 0.1816,
      "step": 1835
    },
    {
      "epoch": 0.983132530120482,
      "grad_norm": 0.5152519941329956,
      "learning_rate": 0.00013451106352605283,
      "loss": 0.148,
      "step": 1836
    },
    {
      "epoch": 0.9836680053547523,
      "grad_norm": 0.4009685814380646,
      "learning_rate": 0.00013447537473233406,
      "loss": 0.0619,
      "step": 1837
    },
    {
      "epoch": 0.9842034805890227,
      "grad_norm": 0.4659446179866791,
      "learning_rate": 0.0001344396859386153,
      "loss": 0.0962,
      "step": 1838
    },
    {
      "epoch": 0.9847389558232932,
      "grad_norm": 0.6254392266273499,
      "learning_rate": 0.0001344039971448965,
      "loss": 0.1088,
      "step": 1839
    },
    {
      "epoch": 0.9852744310575636,
      "grad_norm": 0.576460063457489,
      "learning_rate": 0.00013436830835117773,
      "loss": 0.0861,
      "step": 1840
    },
    {
      "epoch": 0.985809906291834,
      "grad_norm": 0.6070014238357544,
      "learning_rate": 0.00013433261955745897,
      "loss": 0.1195,
      "step": 1841
    },
    {
      "epoch": 0.9863453815261044,
      "grad_norm": 0.36813053488731384,
      "learning_rate": 0.0001342969307637402,
      "loss": 0.1191,
      "step": 1842
    },
    {
      "epoch": 0.9868808567603748,
      "grad_norm": 0.5332969427108765,
      "learning_rate": 0.0001342612419700214,
      "loss": 0.0648,
      "step": 1843
    },
    {
      "epoch": 0.9874163319946453,
      "grad_norm": 0.43260467052459717,
      "learning_rate": 0.00013422555317630267,
      "loss": 0.1127,
      "step": 1844
    },
    {
      "epoch": 0.9879518072289156,
      "grad_norm": 0.6998113393783569,
      "learning_rate": 0.00013418986438258387,
      "loss": 0.1367,
      "step": 1845
    },
    {
      "epoch": 0.9884872824631861,
      "grad_norm": 0.4216881990432739,
      "learning_rate": 0.0001341541755888651,
      "loss": 0.1107,
      "step": 1846
    },
    {
      "epoch": 0.9890227576974565,
      "grad_norm": 0.45400819182395935,
      "learning_rate": 0.00013411848679514634,
      "loss": 0.0914,
      "step": 1847
    },
    {
      "epoch": 0.989558232931727,
      "grad_norm": 0.40300706028938293,
      "learning_rate": 0.00013408279800142755,
      "loss": 0.1064,
      "step": 1848
    },
    {
      "epoch": 0.9900937081659973,
      "grad_norm": 0.3079092800617218,
      "learning_rate": 0.00013404710920770878,
      "loss": 0.0576,
      "step": 1849
    },
    {
      "epoch": 0.9906291834002677,
      "grad_norm": 1.4282559156417847,
      "learning_rate": 0.00013401142041399,
      "loss": 0.106,
      "step": 1850
    },
    {
      "epoch": 0.9911646586345382,
      "grad_norm": 0.49678370356559753,
      "learning_rate": 0.00013397573162027125,
      "loss": 0.1321,
      "step": 1851
    },
    {
      "epoch": 0.9917001338688086,
      "grad_norm": 0.2340049147605896,
      "learning_rate": 0.00013394004282655245,
      "loss": 0.0427,
      "step": 1852
    },
    {
      "epoch": 0.992235609103079,
      "grad_norm": 0.2955192029476166,
      "learning_rate": 0.0001339043540328337,
      "loss": 0.0628,
      "step": 1853
    },
    {
      "epoch": 0.9927710843373494,
      "grad_norm": 0.5348454713821411,
      "learning_rate": 0.00013386866523911492,
      "loss": 0.0778,
      "step": 1854
    },
    {
      "epoch": 0.9933065595716198,
      "grad_norm": 0.6434788107872009,
      "learning_rate": 0.00013383297644539615,
      "loss": 0.1348,
      "step": 1855
    },
    {
      "epoch": 0.9938420348058902,
      "grad_norm": 0.404043048620224,
      "learning_rate": 0.0001337972876516774,
      "loss": 0.0565,
      "step": 1856
    },
    {
      "epoch": 0.9943775100401606,
      "grad_norm": 0.45798927545547485,
      "learning_rate": 0.0001337615988579586,
      "loss": 0.1268,
      "step": 1857
    },
    {
      "epoch": 0.9949129852744311,
      "grad_norm": 0.6375171542167664,
      "learning_rate": 0.00013372591006423985,
      "loss": 0.1294,
      "step": 1858
    },
    {
      "epoch": 0.9954484605087015,
      "grad_norm": 0.5440264344215393,
      "learning_rate": 0.00013369022127052106,
      "loss": 0.0915,
      "step": 1859
    },
    {
      "epoch": 0.9959839357429718,
      "grad_norm": 0.537777304649353,
      "learning_rate": 0.0001336545324768023,
      "loss": 0.1017,
      "step": 1860
    },
    {
      "epoch": 0.9965194109772423,
      "grad_norm": 0.7231582999229431,
      "learning_rate": 0.00013361884368308353,
      "loss": 0.2019,
      "step": 1861
    },
    {
      "epoch": 0.9970548862115127,
      "grad_norm": 0.4130995273590088,
      "learning_rate": 0.00013358315488936476,
      "loss": 0.1218,
      "step": 1862
    },
    {
      "epoch": 0.9975903614457832,
      "grad_norm": 0.4746584892272949,
      "learning_rate": 0.00013354746609564597,
      "loss": 0.1457,
      "step": 1863
    },
    {
      "epoch": 0.9981258366800535,
      "grad_norm": 0.5069870948791504,
      "learning_rate": 0.0001335117773019272,
      "loss": 0.1147,
      "step": 1864
    },
    {
      "epoch": 0.998661311914324,
      "grad_norm": 0.5898817181587219,
      "learning_rate": 0.00013347608850820843,
      "loss": 0.1422,
      "step": 1865
    },
    {
      "epoch": 0.9991967871485944,
      "grad_norm": 0.4408768117427826,
      "learning_rate": 0.00013344039971448964,
      "loss": 0.0996,
      "step": 1866
    },
    {
      "epoch": 0.9997322623828648,
      "grad_norm": 0.6694408059120178,
      "learning_rate": 0.0001334047109207709,
      "loss": 0.1762,
      "step": 1867
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.020587656646966934,
      "learning_rate": 0.0001333690221270521,
      "loss": 0.0002,
      "step": 1868
    },
    {
      "epoch": 1.0005354752342703,
      "grad_norm": 0.2755027711391449,
      "learning_rate": 0.00013333333333333334,
      "loss": 0.1058,
      "step": 1869
    },
    {
      "epoch": 1.0010709504685409,
      "grad_norm": 0.42788010835647583,
      "learning_rate": 0.00013329764453961457,
      "loss": 0.0733,
      "step": 1870
    },
    {
      "epoch": 1.0016064257028112,
      "grad_norm": 0.5988460779190063,
      "learning_rate": 0.0001332619557458958,
      "loss": 0.1936,
      "step": 1871
    },
    {
      "epoch": 1.0021419009370816,
      "grad_norm": 0.2739125192165375,
      "learning_rate": 0.000133226266952177,
      "loss": 0.051,
      "step": 1872
    },
    {
      "epoch": 1.002677376171352,
      "grad_norm": 1.9683326482772827,
      "learning_rate": 0.00013319057815845825,
      "loss": 0.1901,
      "step": 1873
    },
    {
      "epoch": 1.0032128514056224,
      "grad_norm": 1.0634323358535767,
      "learning_rate": 0.00013315488936473948,
      "loss": 0.1563,
      "step": 1874
    },
    {
      "epoch": 1.003748326639893,
      "grad_norm": 0.3505132794380188,
      "learning_rate": 0.00013311920057102069,
      "loss": 0.1035,
      "step": 1875
    },
    {
      "epoch": 1.0042838018741633,
      "grad_norm": 0.316078782081604,
      "learning_rate": 0.00013308351177730195,
      "loss": 0.0774,
      "step": 1876
    },
    {
      "epoch": 1.0048192771084337,
      "grad_norm": 0.42949429154396057,
      "learning_rate": 0.00013304782298358315,
      "loss": 0.1298,
      "step": 1877
    },
    {
      "epoch": 1.0053547523427042,
      "grad_norm": 0.4393726587295532,
      "learning_rate": 0.00013301213418986439,
      "loss": 0.1286,
      "step": 1878
    },
    {
      "epoch": 1.0058902275769745,
      "grad_norm": 0.511766254901886,
      "learning_rate": 0.00013297644539614562,
      "loss": 0.0843,
      "step": 1879
    },
    {
      "epoch": 1.0064257028112449,
      "grad_norm": 0.31575849652290344,
      "learning_rate": 0.00013294075660242685,
      "loss": 0.0974,
      "step": 1880
    },
    {
      "epoch": 1.0069611780455154,
      "grad_norm": 0.3928869068622589,
      "learning_rate": 0.00013290506780870806,
      "loss": 0.1386,
      "step": 1881
    },
    {
      "epoch": 1.0074966532797858,
      "grad_norm": 1.0706437826156616,
      "learning_rate": 0.0001328693790149893,
      "loss": 0.1139,
      "step": 1882
    },
    {
      "epoch": 1.0080321285140563,
      "grad_norm": 0.2794305384159088,
      "learning_rate": 0.00013283369022127053,
      "loss": 0.0651,
      "step": 1883
    },
    {
      "epoch": 1.0085676037483267,
      "grad_norm": 0.4451645612716675,
      "learning_rate": 0.00013279800142755176,
      "loss": 0.1211,
      "step": 1884
    },
    {
      "epoch": 1.009103078982597,
      "grad_norm": 0.680701732635498,
      "learning_rate": 0.000132762312633833,
      "loss": 0.1538,
      "step": 1885
    },
    {
      "epoch": 1.0096385542168675,
      "grad_norm": 1.072788953781128,
      "learning_rate": 0.0001327266238401142,
      "loss": 0.1681,
      "step": 1886
    },
    {
      "epoch": 1.0101740294511379,
      "grad_norm": 0.3092523515224457,
      "learning_rate": 0.00013269093504639546,
      "loss": 0.0797,
      "step": 1887
    },
    {
      "epoch": 1.0107095046854082,
      "grad_norm": 0.9972196817398071,
      "learning_rate": 0.00013265524625267667,
      "loss": 0.1593,
      "step": 1888
    },
    {
      "epoch": 1.0112449799196788,
      "grad_norm": 0.29839086532592773,
      "learning_rate": 0.0001326195574589579,
      "loss": 0.0752,
      "step": 1889
    },
    {
      "epoch": 1.011780455153949,
      "grad_norm": 0.35006365180015564,
      "learning_rate": 0.00013258386866523913,
      "loss": 0.0723,
      "step": 1890
    },
    {
      "epoch": 1.0123159303882197,
      "grad_norm": 0.24303822219371796,
      "learning_rate": 0.00013254817987152034,
      "loss": 0.0375,
      "step": 1891
    },
    {
      "epoch": 1.01285140562249,
      "grad_norm": 0.33467897772789,
      "learning_rate": 0.00013251249107780157,
      "loss": 0.0971,
      "step": 1892
    },
    {
      "epoch": 1.0133868808567603,
      "grad_norm": 0.5648014545440674,
      "learning_rate": 0.0001324768022840828,
      "loss": 0.1581,
      "step": 1893
    },
    {
      "epoch": 1.0139223560910309,
      "grad_norm": 0.5111075043678284,
      "learning_rate": 0.00013244111349036404,
      "loss": 0.1132,
      "step": 1894
    },
    {
      "epoch": 1.0144578313253012,
      "grad_norm": 0.9160680770874023,
      "learning_rate": 0.00013240542469664525,
      "loss": 0.0491,
      "step": 1895
    },
    {
      "epoch": 1.0149933065595715,
      "grad_norm": 3.0942487716674805,
      "learning_rate": 0.0001323697359029265,
      "loss": 0.1174,
      "step": 1896
    },
    {
      "epoch": 1.015528781793842,
      "grad_norm": 0.44152408838272095,
      "learning_rate": 0.0001323340471092077,
      "loss": 0.0403,
      "step": 1897
    },
    {
      "epoch": 1.0160642570281124,
      "grad_norm": 1.7134766578674316,
      "learning_rate": 0.00013229835831548895,
      "loss": 0.1123,
      "step": 1898
    },
    {
      "epoch": 1.0165997322623828,
      "grad_norm": 0.8151409029960632,
      "learning_rate": 0.00013226266952177018,
      "loss": 0.1043,
      "step": 1899
    },
    {
      "epoch": 1.0171352074966533,
      "grad_norm": 0.4950658679008484,
      "learning_rate": 0.00013222698072805139,
      "loss": 0.1263,
      "step": 1900
    },
    {
      "epoch": 1.0176706827309236,
      "grad_norm": 0.5314009785652161,
      "learning_rate": 0.00013219129193433262,
      "loss": 0.0779,
      "step": 1901
    },
    {
      "epoch": 1.0182061579651942,
      "grad_norm": 1.0327130556106567,
      "learning_rate": 0.00013215560314061385,
      "loss": 0.1696,
      "step": 1902
    },
    {
      "epoch": 1.0187416331994645,
      "grad_norm": 0.843947172164917,
      "learning_rate": 0.00013211991434689509,
      "loss": 0.0424,
      "step": 1903
    },
    {
      "epoch": 1.0192771084337349,
      "grad_norm": 1.6950114965438843,
      "learning_rate": 0.0001320842255531763,
      "loss": 0.1112,
      "step": 1904
    },
    {
      "epoch": 1.0198125836680054,
      "grad_norm": 0.33791494369506836,
      "learning_rate": 0.00013204853675945755,
      "loss": 0.0281,
      "step": 1905
    },
    {
      "epoch": 1.0203480589022758,
      "grad_norm": 0.9093629121780396,
      "learning_rate": 0.00013201284796573876,
      "loss": 0.1803,
      "step": 1906
    },
    {
      "epoch": 1.020883534136546,
      "grad_norm": 0.7836743593215942,
      "learning_rate": 0.00013197715917202,
      "loss": 0.0754,
      "step": 1907
    },
    {
      "epoch": 1.0214190093708166,
      "grad_norm": 0.9314230680465698,
      "learning_rate": 0.00013194147037830123,
      "loss": 0.1439,
      "step": 1908
    },
    {
      "epoch": 1.021954484605087,
      "grad_norm": 0.7085061073303223,
      "learning_rate": 0.00013190578158458243,
      "loss": 0.1047,
      "step": 1909
    },
    {
      "epoch": 1.0224899598393575,
      "grad_norm": 0.48371484875679016,
      "learning_rate": 0.00013187009279086367,
      "loss": 0.1136,
      "step": 1910
    },
    {
      "epoch": 1.0230254350736279,
      "grad_norm": 1.4188584089279175,
      "learning_rate": 0.0001318344039971449,
      "loss": 0.1411,
      "step": 1911
    },
    {
      "epoch": 1.0235609103078982,
      "grad_norm": 0.7188448309898376,
      "learning_rate": 0.00013179871520342613,
      "loss": 0.1304,
      "step": 1912
    },
    {
      "epoch": 1.0240963855421688,
      "grad_norm": 6.710216522216797,
      "learning_rate": 0.00013176302640970737,
      "loss": 0.3597,
      "step": 1913
    },
    {
      "epoch": 1.024631860776439,
      "grad_norm": 0.8194788694381714,
      "learning_rate": 0.0001317273376159886,
      "loss": 0.091,
      "step": 1914
    },
    {
      "epoch": 1.0251673360107094,
      "grad_norm": 0.590566873550415,
      "learning_rate": 0.0001316916488222698,
      "loss": 0.1141,
      "step": 1915
    },
    {
      "epoch": 1.02570281124498,
      "grad_norm": 0.6880890727043152,
      "learning_rate": 0.00013165596002855104,
      "loss": 0.1089,
      "step": 1916
    },
    {
      "epoch": 1.0262382864792503,
      "grad_norm": 0.9013465046882629,
      "learning_rate": 0.00013162027123483227,
      "loss": 0.0899,
      "step": 1917
    },
    {
      "epoch": 1.0267737617135206,
      "grad_norm": 0.44456496834754944,
      "learning_rate": 0.00013158458244111348,
      "loss": 0.0626,
      "step": 1918
    },
    {
      "epoch": 1.0273092369477912,
      "grad_norm": 0.5386397838592529,
      "learning_rate": 0.00013154889364739474,
      "loss": 0.0756,
      "step": 1919
    },
    {
      "epoch": 1.0278447121820615,
      "grad_norm": 2.223186731338501,
      "learning_rate": 0.00013151320485367595,
      "loss": 0.0904,
      "step": 1920
    },
    {
      "epoch": 1.028380187416332,
      "grad_norm": 1.0949971675872803,
      "learning_rate": 0.00013147751605995718,
      "loss": 0.1424,
      "step": 1921
    },
    {
      "epoch": 1.0289156626506024,
      "grad_norm": 0.5922494530677795,
      "learning_rate": 0.0001314418272662384,
      "loss": 0.063,
      "step": 1922
    },
    {
      "epoch": 1.0294511378848727,
      "grad_norm": 0.4543151259422302,
      "learning_rate": 0.00013140613847251965,
      "loss": 0.1065,
      "step": 1923
    },
    {
      "epoch": 1.0299866131191433,
      "grad_norm": 0.3784891963005066,
      "learning_rate": 0.00013137044967880085,
      "loss": 0.0942,
      "step": 1924
    },
    {
      "epoch": 1.0305220883534136,
      "grad_norm": 0.6604328155517578,
      "learning_rate": 0.00013133476088508209,
      "loss": 0.165,
      "step": 1925
    },
    {
      "epoch": 1.031057563587684,
      "grad_norm": 0.5483724474906921,
      "learning_rate": 0.00013129907209136332,
      "loss": 0.1096,
      "step": 1926
    },
    {
      "epoch": 1.0315930388219545,
      "grad_norm": 0.3599032461643219,
      "learning_rate": 0.00013126338329764453,
      "loss": 0.083,
      "step": 1927
    },
    {
      "epoch": 1.0321285140562249,
      "grad_norm": 0.32174211740493774,
      "learning_rate": 0.0001312276945039258,
      "loss": 0.0629,
      "step": 1928
    },
    {
      "epoch": 1.0326639892904954,
      "grad_norm": 1.8172190189361572,
      "learning_rate": 0.000131192005710207,
      "loss": 0.0542,
      "step": 1929
    },
    {
      "epoch": 1.0331994645247657,
      "grad_norm": 0.3851020932197571,
      "learning_rate": 0.00013115631691648823,
      "loss": 0.0576,
      "step": 1930
    },
    {
      "epoch": 1.033734939759036,
      "grad_norm": 0.37737295031547546,
      "learning_rate": 0.00013112062812276946,
      "loss": 0.0958,
      "step": 1931
    },
    {
      "epoch": 1.0342704149933066,
      "grad_norm": 0.47411710023880005,
      "learning_rate": 0.0001310849393290507,
      "loss": 0.1676,
      "step": 1932
    },
    {
      "epoch": 1.034805890227577,
      "grad_norm": 0.2401987910270691,
      "learning_rate": 0.0001310492505353319,
      "loss": 0.0193,
      "step": 1933
    },
    {
      "epoch": 1.0353413654618473,
      "grad_norm": 0.5561507940292358,
      "learning_rate": 0.00013101356174161313,
      "loss": 0.0491,
      "step": 1934
    },
    {
      "epoch": 1.0358768406961179,
      "grad_norm": 0.35458236932754517,
      "learning_rate": 0.00013097787294789437,
      "loss": 0.0716,
      "step": 1935
    },
    {
      "epoch": 1.0364123159303882,
      "grad_norm": 0.3023180663585663,
      "learning_rate": 0.00013094218415417557,
      "loss": 0.0775,
      "step": 1936
    },
    {
      "epoch": 1.0369477911646587,
      "grad_norm": 1.712981104850769,
      "learning_rate": 0.00013090649536045683,
      "loss": 0.0715,
      "step": 1937
    },
    {
      "epoch": 1.037483266398929,
      "grad_norm": 0.8170792460441589,
      "learning_rate": 0.00013087080656673804,
      "loss": 0.141,
      "step": 1938
    },
    {
      "epoch": 1.0380187416331994,
      "grad_norm": 0.9236469864845276,
      "learning_rate": 0.0001308351177730193,
      "loss": 0.1187,
      "step": 1939
    },
    {
      "epoch": 1.03855421686747,
      "grad_norm": 0.3206508159637451,
      "learning_rate": 0.0001307994289793005,
      "loss": 0.0229,
      "step": 1940
    },
    {
      "epoch": 1.0390896921017403,
      "grad_norm": 0.418462872505188,
      "learning_rate": 0.00013076374018558174,
      "loss": 0.0654,
      "step": 1941
    },
    {
      "epoch": 1.0396251673360106,
      "grad_norm": 0.323429673910141,
      "learning_rate": 0.00013072805139186297,
      "loss": 0.056,
      "step": 1942
    },
    {
      "epoch": 1.0401606425702812,
      "grad_norm": 0.4801052212715149,
      "learning_rate": 0.00013069236259814418,
      "loss": 0.0478,
      "step": 1943
    },
    {
      "epoch": 1.0406961178045515,
      "grad_norm": 1.1570099592208862,
      "learning_rate": 0.0001306566738044254,
      "loss": 0.0622,
      "step": 1944
    },
    {
      "epoch": 1.0412315930388218,
      "grad_norm": 0.4896092712879181,
      "learning_rate": 0.00013062098501070665,
      "loss": 0.0487,
      "step": 1945
    },
    {
      "epoch": 1.0417670682730924,
      "grad_norm": 2.6503396034240723,
      "learning_rate": 0.00013058529621698788,
      "loss": 0.0982,
      "step": 1946
    },
    {
      "epoch": 1.0423025435073627,
      "grad_norm": 1.0497143268585205,
      "learning_rate": 0.00013054960742326909,
      "loss": 0.1223,
      "step": 1947
    },
    {
      "epoch": 1.0428380187416333,
      "grad_norm": 0.46669331192970276,
      "learning_rate": 0.00013051391862955035,
      "loss": 0.0573,
      "step": 1948
    },
    {
      "epoch": 1.0433734939759036,
      "grad_norm": 0.9611944556236267,
      "learning_rate": 0.00013047822983583155,
      "loss": 0.1229,
      "step": 1949
    },
    {
      "epoch": 1.043908969210174,
      "grad_norm": 0.8605873584747314,
      "learning_rate": 0.00013044254104211279,
      "loss": 0.0842,
      "step": 1950
    },
    {
      "epoch": 1.0444444444444445,
      "grad_norm": 0.672150731086731,
      "learning_rate": 0.00013040685224839402,
      "loss": 0.0928,
      "step": 1951
    },
    {
      "epoch": 1.0449799196787148,
      "grad_norm": 0.7371860146522522,
      "learning_rate": 0.00013037116345467523,
      "loss": 0.1394,
      "step": 1952
    },
    {
      "epoch": 1.0455153949129852,
      "grad_norm": 0.5548846125602722,
      "learning_rate": 0.00013033547466095646,
      "loss": 0.0751,
      "step": 1953
    },
    {
      "epoch": 1.0460508701472557,
      "grad_norm": 0.6609454154968262,
      "learning_rate": 0.0001302997858672377,
      "loss": 0.1202,
      "step": 1954
    },
    {
      "epoch": 1.046586345381526,
      "grad_norm": 0.8206131458282471,
      "learning_rate": 0.00013026409707351893,
      "loss": 0.1222,
      "step": 1955
    },
    {
      "epoch": 1.0471218206157966,
      "grad_norm": 1.1181113719940186,
      "learning_rate": 0.00013022840827980013,
      "loss": 0.1225,
      "step": 1956
    },
    {
      "epoch": 1.047657295850067,
      "grad_norm": 0.8034523129463196,
      "learning_rate": 0.0001301927194860814,
      "loss": 0.0404,
      "step": 1957
    },
    {
      "epoch": 1.0481927710843373,
      "grad_norm": 0.8929154872894287,
      "learning_rate": 0.0001301570306923626,
      "loss": 0.1079,
      "step": 1958
    },
    {
      "epoch": 1.0487282463186078,
      "grad_norm": 0.492317259311676,
      "learning_rate": 0.00013012134189864383,
      "loss": 0.0264,
      "step": 1959
    },
    {
      "epoch": 1.0492637215528782,
      "grad_norm": 1.1481740474700928,
      "learning_rate": 0.00013008565310492507,
      "loss": 0.1112,
      "step": 1960
    },
    {
      "epoch": 1.0497991967871485,
      "grad_norm": 8.555525779724121,
      "learning_rate": 0.00013004996431120627,
      "loss": 0.2101,
      "step": 1961
    },
    {
      "epoch": 1.050334672021419,
      "grad_norm": 6.236369609832764,
      "learning_rate": 0.0001300142755174875,
      "loss": 0.126,
      "step": 1962
    },
    {
      "epoch": 1.0508701472556894,
      "grad_norm": 0.5023929476737976,
      "learning_rate": 0.00012997858672376874,
      "loss": 0.0591,
      "step": 1963
    },
    {
      "epoch": 1.05140562248996,
      "grad_norm": 5.24298095703125,
      "learning_rate": 0.00012994289793004997,
      "loss": 0.1179,
      "step": 1964
    },
    {
      "epoch": 1.0519410977242303,
      "grad_norm": 0.7759522795677185,
      "learning_rate": 0.00012990720913633118,
      "loss": 0.1743,
      "step": 1965
    },
    {
      "epoch": 1.0524765729585006,
      "grad_norm": 0.6615459322929382,
      "learning_rate": 0.00012987152034261244,
      "loss": 0.114,
      "step": 1966
    },
    {
      "epoch": 1.0530120481927712,
      "grad_norm": 1.394724726676941,
      "learning_rate": 0.00012983583154889365,
      "loss": 0.1449,
      "step": 1967
    },
    {
      "epoch": 1.0535475234270415,
      "grad_norm": 1.0717021226882935,
      "learning_rate": 0.00012980014275517488,
      "loss": 0.0969,
      "step": 1968
    },
    {
      "epoch": 1.0540829986613118,
      "grad_norm": 0.5433619618415833,
      "learning_rate": 0.0001297644539614561,
      "loss": 0.0915,
      "step": 1969
    },
    {
      "epoch": 1.0546184738955824,
      "grad_norm": 0.38492825627326965,
      "learning_rate": 0.00012972876516773732,
      "loss": 0.0756,
      "step": 1970
    },
    {
      "epoch": 1.0551539491298527,
      "grad_norm": 0.6293299794197083,
      "learning_rate": 0.00012969307637401858,
      "loss": 0.1346,
      "step": 1971
    },
    {
      "epoch": 1.055689424364123,
      "grad_norm": 0.5044587850570679,
      "learning_rate": 0.00012965738758029979,
      "loss": 0.0715,
      "step": 1972
    },
    {
      "epoch": 1.0562248995983936,
      "grad_norm": 0.7791756987571716,
      "learning_rate": 0.00012962169878658102,
      "loss": 0.1162,
      "step": 1973
    },
    {
      "epoch": 1.056760374832664,
      "grad_norm": 0.4499607980251312,
      "learning_rate": 0.00012958600999286225,
      "loss": 0.1779,
      "step": 1974
    },
    {
      "epoch": 1.0572958500669345,
      "grad_norm": 0.577777624130249,
      "learning_rate": 0.00012955032119914349,
      "loss": 0.0851,
      "step": 1975
    },
    {
      "epoch": 1.0578313253012048,
      "grad_norm": 0.5097274780273438,
      "learning_rate": 0.0001295146324054247,
      "loss": 0.0713,
      "step": 1976
    },
    {
      "epoch": 1.0583668005354752,
      "grad_norm": 0.650155246257782,
      "learning_rate": 0.00012947894361170593,
      "loss": 0.0984,
      "step": 1977
    },
    {
      "epoch": 1.0589022757697457,
      "grad_norm": 0.6577835083007812,
      "learning_rate": 0.00012944325481798716,
      "loss": 0.0945,
      "step": 1978
    },
    {
      "epoch": 1.059437751004016,
      "grad_norm": 1.2791208028793335,
      "learning_rate": 0.00012940756602426837,
      "loss": 0.1323,
      "step": 1979
    },
    {
      "epoch": 1.0599732262382864,
      "grad_norm": 1.6563913822174072,
      "learning_rate": 0.00012937187723054963,
      "loss": 0.0267,
      "step": 1980
    },
    {
      "epoch": 1.060508701472557,
      "grad_norm": 1.0504570007324219,
      "learning_rate": 0.00012933618843683083,
      "loss": 0.1765,
      "step": 1981
    },
    {
      "epoch": 1.0610441767068273,
      "grad_norm": 0.8830996155738831,
      "learning_rate": 0.00012930049964311207,
      "loss": 0.1697,
      "step": 1982
    },
    {
      "epoch": 1.0615796519410978,
      "grad_norm": 1.8265880346298218,
      "learning_rate": 0.0001292648108493933,
      "loss": 0.2145,
      "step": 1983
    },
    {
      "epoch": 1.0621151271753682,
      "grad_norm": 0.3203657269477844,
      "learning_rate": 0.00012922912205567453,
      "loss": 0.0429,
      "step": 1984
    },
    {
      "epoch": 1.0626506024096385,
      "grad_norm": 0.6892693042755127,
      "learning_rate": 0.00012919343326195574,
      "loss": 0.1078,
      "step": 1985
    },
    {
      "epoch": 1.063186077643909,
      "grad_norm": 0.4612610340118408,
      "learning_rate": 0.00012915774446823697,
      "loss": 0.098,
      "step": 1986
    },
    {
      "epoch": 1.0637215528781794,
      "grad_norm": 0.7320184707641602,
      "learning_rate": 0.0001291220556745182,
      "loss": 0.1153,
      "step": 1987
    },
    {
      "epoch": 1.0642570281124497,
      "grad_norm": 0.5030260682106018,
      "learning_rate": 0.0001290863668807994,
      "loss": 0.0723,
      "step": 1988
    },
    {
      "epoch": 1.0647925033467203,
      "grad_norm": 0.7813096046447754,
      "learning_rate": 0.00012905067808708067,
      "loss": 0.0801,
      "step": 1989
    },
    {
      "epoch": 1.0653279785809906,
      "grad_norm": 1.0513392686843872,
      "learning_rate": 0.00012901498929336188,
      "loss": 0.1409,
      "step": 1990
    },
    {
      "epoch": 1.0658634538152612,
      "grad_norm": 1.0824114084243774,
      "learning_rate": 0.0001289793004996431,
      "loss": 0.1291,
      "step": 1991
    },
    {
      "epoch": 1.0663989290495315,
      "grad_norm": 0.9437936544418335,
      "learning_rate": 0.00012894361170592435,
      "loss": 0.1694,
      "step": 1992
    },
    {
      "epoch": 1.0669344042838018,
      "grad_norm": 0.7112041711807251,
      "learning_rate": 0.00012890792291220558,
      "loss": 0.0982,
      "step": 1993
    },
    {
      "epoch": 1.0674698795180724,
      "grad_norm": 0.41590049862861633,
      "learning_rate": 0.0001288722341184868,
      "loss": 0.0772,
      "step": 1994
    },
    {
      "epoch": 1.0680053547523427,
      "grad_norm": 0.7175880074501038,
      "learning_rate": 0.00012883654532476802,
      "loss": 0.0302,
      "step": 1995
    },
    {
      "epoch": 1.068540829986613,
      "grad_norm": 0.4209560751914978,
      "learning_rate": 0.00012880085653104925,
      "loss": 0.0709,
      "step": 1996
    },
    {
      "epoch": 1.0690763052208836,
      "grad_norm": 0.538282036781311,
      "learning_rate": 0.00012876516773733049,
      "loss": 0.0494,
      "step": 1997
    },
    {
      "epoch": 1.069611780455154,
      "grad_norm": 0.6623476147651672,
      "learning_rate": 0.00012872947894361172,
      "loss": 0.0692,
      "step": 1998
    },
    {
      "epoch": 1.0701472556894243,
      "grad_norm": 0.7387366890907288,
      "learning_rate": 0.00012869379014989293,
      "loss": 0.1224,
      "step": 1999
    },
    {
      "epoch": 1.0706827309236948,
      "grad_norm": 0.34719476103782654,
      "learning_rate": 0.0001286581013561742,
      "loss": 0.0289,
      "step": 2000
    },
    {
      "epoch": 1.0712182061579651,
      "grad_norm": 0.36067885160446167,
      "learning_rate": 0.0001286224125624554,
      "loss": 0.0532,
      "step": 2001
    },
    {
      "epoch": 1.0717536813922357,
      "grad_norm": 0.41062024235725403,
      "learning_rate": 0.00012858672376873663,
      "loss": 0.0521,
      "step": 2002
    },
    {
      "epoch": 1.072289156626506,
      "grad_norm": 0.46592792868614197,
      "learning_rate": 0.00012855103497501786,
      "loss": 0.0687,
      "step": 2003
    },
    {
      "epoch": 1.0728246318607764,
      "grad_norm": 0.5265172719955444,
      "learning_rate": 0.00012851534618129907,
      "loss": 0.0963,
      "step": 2004
    },
    {
      "epoch": 1.073360107095047,
      "grad_norm": 0.667841911315918,
      "learning_rate": 0.0001284796573875803,
      "loss": 0.1089,
      "step": 2005
    },
    {
      "epoch": 1.0738955823293173,
      "grad_norm": 0.8456820249557495,
      "learning_rate": 0.00012844396859386153,
      "loss": 0.0954,
      "step": 2006
    },
    {
      "epoch": 1.0744310575635876,
      "grad_norm": 1.789095163345337,
      "learning_rate": 0.00012840827980014277,
      "loss": 0.0489,
      "step": 2007
    },
    {
      "epoch": 1.0749665327978581,
      "grad_norm": 0.4229103624820709,
      "learning_rate": 0.00012837259100642397,
      "loss": 0.085,
      "step": 2008
    },
    {
      "epoch": 1.0755020080321285,
      "grad_norm": 0.5534087419509888,
      "learning_rate": 0.00012833690221270523,
      "loss": 0.1106,
      "step": 2009
    },
    {
      "epoch": 1.076037483266399,
      "grad_norm": 0.6012312173843384,
      "learning_rate": 0.00012830121341898644,
      "loss": 0.0863,
      "step": 2010
    },
    {
      "epoch": 1.0765729585006694,
      "grad_norm": 0.938941478729248,
      "learning_rate": 0.00012826552462526767,
      "loss": 0.151,
      "step": 2011
    },
    {
      "epoch": 1.0771084337349397,
      "grad_norm": 0.2914031744003296,
      "learning_rate": 0.0001282298358315489,
      "loss": 0.0579,
      "step": 2012
    },
    {
      "epoch": 1.0776439089692103,
      "grad_norm": 0.8814843893051147,
      "learning_rate": 0.0001281941470378301,
      "loss": 0.113,
      "step": 2013
    },
    {
      "epoch": 1.0781793842034806,
      "grad_norm": 1.7095844745635986,
      "learning_rate": 0.00012815845824411135,
      "loss": 0.117,
      "step": 2014
    },
    {
      "epoch": 1.078714859437751,
      "grad_norm": 0.28302860260009766,
      "learning_rate": 0.00012812276945039258,
      "loss": 0.0236,
      "step": 2015
    },
    {
      "epoch": 1.0792503346720215,
      "grad_norm": 1.0954463481903076,
      "learning_rate": 0.0001280870806566738,
      "loss": 0.0628,
      "step": 2016
    },
    {
      "epoch": 1.0797858099062918,
      "grad_norm": 0.9640547037124634,
      "learning_rate": 0.00012805139186295502,
      "loss": 0.1238,
      "step": 2017
    },
    {
      "epoch": 1.0803212851405624,
      "grad_norm": 4.36018180847168,
      "learning_rate": 0.00012801570306923628,
      "loss": 0.1766,
      "step": 2018
    },
    {
      "epoch": 1.0808567603748327,
      "grad_norm": 6.800357341766357,
      "learning_rate": 0.00012798001427551749,
      "loss": 0.1974,
      "step": 2019
    },
    {
      "epoch": 1.081392235609103,
      "grad_norm": 1.0064841508865356,
      "learning_rate": 0.00012794432548179872,
      "loss": 0.1424,
      "step": 2020
    },
    {
      "epoch": 1.0819277108433736,
      "grad_norm": 0.5537645220756531,
      "learning_rate": 0.00012790863668807995,
      "loss": 0.0822,
      "step": 2021
    },
    {
      "epoch": 1.082463186077644,
      "grad_norm": 0.8392770886421204,
      "learning_rate": 0.00012787294789436116,
      "loss": 0.1443,
      "step": 2022
    },
    {
      "epoch": 1.0829986613119142,
      "grad_norm": 0.45319944620132446,
      "learning_rate": 0.00012783725910064242,
      "loss": 0.0953,
      "step": 2023
    },
    {
      "epoch": 1.0835341365461848,
      "grad_norm": 2.468386650085449,
      "learning_rate": 0.00012780157030692363,
      "loss": 0.1781,
      "step": 2024
    },
    {
      "epoch": 1.0840696117804551,
      "grad_norm": 0.4282650649547577,
      "learning_rate": 0.00012776588151320486,
      "loss": 0.0813,
      "step": 2025
    },
    {
      "epoch": 1.0846050870147255,
      "grad_norm": 0.9101742506027222,
      "learning_rate": 0.0001277301927194861,
      "loss": 0.0845,
      "step": 2026
    },
    {
      "epoch": 1.085140562248996,
      "grad_norm": 0.34590235352516174,
      "learning_rate": 0.00012769450392576733,
      "loss": 0.0698,
      "step": 2027
    },
    {
      "epoch": 1.0856760374832664,
      "grad_norm": 0.6209458112716675,
      "learning_rate": 0.00012765881513204853,
      "loss": 0.1479,
      "step": 2028
    },
    {
      "epoch": 1.086211512717537,
      "grad_norm": 0.6993346214294434,
      "learning_rate": 0.00012762312633832977,
      "loss": 0.1866,
      "step": 2029
    },
    {
      "epoch": 1.0867469879518072,
      "grad_norm": 0.7385419607162476,
      "learning_rate": 0.000127587437544611,
      "loss": 0.1233,
      "step": 2030
    },
    {
      "epoch": 1.0872824631860776,
      "grad_norm": 1.3547725677490234,
      "learning_rate": 0.0001275517487508922,
      "loss": 0.1092,
      "step": 2031
    },
    {
      "epoch": 1.0878179384203481,
      "grad_norm": 0.3038974404335022,
      "learning_rate": 0.00012751605995717347,
      "loss": 0.0432,
      "step": 2032
    },
    {
      "epoch": 1.0883534136546185,
      "grad_norm": 0.6006441116333008,
      "learning_rate": 0.00012748037116345467,
      "loss": 0.1327,
      "step": 2033
    },
    {
      "epoch": 1.0888888888888888,
      "grad_norm": 1.0071179866790771,
      "learning_rate": 0.0001274446823697359,
      "loss": 0.1347,
      "step": 2034
    },
    {
      "epoch": 1.0894243641231594,
      "grad_norm": 0.5121913552284241,
      "learning_rate": 0.00012740899357601714,
      "loss": 0.0978,
      "step": 2035
    },
    {
      "epoch": 1.0899598393574297,
      "grad_norm": 0.7423173785209656,
      "learning_rate": 0.00012737330478229837,
      "loss": 0.1191,
      "step": 2036
    },
    {
      "epoch": 1.0904953145917002,
      "grad_norm": 0.4027461111545563,
      "learning_rate": 0.00012733761598857958,
      "loss": 0.0751,
      "step": 2037
    },
    {
      "epoch": 1.0910307898259706,
      "grad_norm": 0.8647971153259277,
      "learning_rate": 0.0001273019271948608,
      "loss": 0.17,
      "step": 2038
    },
    {
      "epoch": 1.091566265060241,
      "grad_norm": 0.8172003626823425,
      "learning_rate": 0.00012726623840114205,
      "loss": 0.1251,
      "step": 2039
    },
    {
      "epoch": 1.0921017402945115,
      "grad_norm": 0.5291972160339355,
      "learning_rate": 0.00012723054960742328,
      "loss": 0.0865,
      "step": 2040
    },
    {
      "epoch": 1.0926372155287818,
      "grad_norm": 0.33642157912254333,
      "learning_rate": 0.0001271948608137045,
      "loss": 0.0528,
      "step": 2041
    },
    {
      "epoch": 1.0931726907630521,
      "grad_norm": 1.7279994487762451,
      "learning_rate": 0.00012715917201998572,
      "loss": 0.0605,
      "step": 2042
    },
    {
      "epoch": 1.0937081659973227,
      "grad_norm": 0.4251002073287964,
      "learning_rate": 0.00012712348322626695,
      "loss": 0.0688,
      "step": 2043
    },
    {
      "epoch": 1.094243641231593,
      "grad_norm": 0.793941080570221,
      "learning_rate": 0.00012708779443254819,
      "loss": 0.1415,
      "step": 2044
    },
    {
      "epoch": 1.0947791164658636,
      "grad_norm": 0.9757038354873657,
      "learning_rate": 0.00012705210563882942,
      "loss": 0.1712,
      "step": 2045
    },
    {
      "epoch": 1.095314591700134,
      "grad_norm": 0.9654375314712524,
      "learning_rate": 0.00012701641684511063,
      "loss": 0.0885,
      "step": 2046
    },
    {
      "epoch": 1.0958500669344042,
      "grad_norm": 0.8873333930969238,
      "learning_rate": 0.00012698072805139189,
      "loss": 0.1204,
      "step": 2047
    },
    {
      "epoch": 1.0963855421686748,
      "grad_norm": 0.6466727256774902,
      "learning_rate": 0.0001269450392576731,
      "loss": 0.1229,
      "step": 2048
    },
    {
      "epoch": 1.0969210174029451,
      "grad_norm": 0.6399481892585754,
      "learning_rate": 0.00012690935046395433,
      "loss": 0.0715,
      "step": 2049
    },
    {
      "epoch": 1.0974564926372155,
      "grad_norm": 0.7998898029327393,
      "learning_rate": 0.00012687366167023556,
      "loss": 0.1449,
      "step": 2050
    },
    {
      "epoch": 1.097991967871486,
      "grad_norm": 0.6148260235786438,
      "learning_rate": 0.00012683797287651677,
      "loss": 0.1358,
      "step": 2051
    },
    {
      "epoch": 1.0985274431057563,
      "grad_norm": 0.35550329089164734,
      "learning_rate": 0.00012680228408279803,
      "loss": 0.0166,
      "step": 2052
    },
    {
      "epoch": 1.0990629183400267,
      "grad_norm": 1.442643642425537,
      "learning_rate": 0.00012676659528907923,
      "loss": 0.1391,
      "step": 2053
    },
    {
      "epoch": 1.0995983935742972,
      "grad_norm": 0.8121286034584045,
      "learning_rate": 0.00012673090649536047,
      "loss": 0.152,
      "step": 2054
    },
    {
      "epoch": 1.1001338688085676,
      "grad_norm": 0.31176260113716125,
      "learning_rate": 0.0001266952177016417,
      "loss": 0.0632,
      "step": 2055
    },
    {
      "epoch": 1.1006693440428381,
      "grad_norm": 2.085357904434204,
      "learning_rate": 0.00012665952890792293,
      "loss": 0.1543,
      "step": 2056
    },
    {
      "epoch": 1.1012048192771084,
      "grad_norm": 0.7369207739830017,
      "learning_rate": 0.00012662384011420414,
      "loss": 0.1662,
      "step": 2057
    },
    {
      "epoch": 1.1017402945113788,
      "grad_norm": 0.5545552968978882,
      "learning_rate": 0.00012658815132048537,
      "loss": 0.166,
      "step": 2058
    },
    {
      "epoch": 1.1022757697456493,
      "grad_norm": 0.621898889541626,
      "learning_rate": 0.0001265524625267666,
      "loss": 0.1506,
      "step": 2059
    },
    {
      "epoch": 1.1028112449799197,
      "grad_norm": 0.5939724445343018,
      "learning_rate": 0.0001265167737330478,
      "loss": 0.0787,
      "step": 2060
    },
    {
      "epoch": 1.10334672021419,
      "grad_norm": 0.31459537148475647,
      "learning_rate": 0.00012648108493932907,
      "loss": 0.0381,
      "step": 2061
    },
    {
      "epoch": 1.1038821954484606,
      "grad_norm": 4.882580280303955,
      "learning_rate": 0.00012644539614561028,
      "loss": 0.2779,
      "step": 2062
    },
    {
      "epoch": 1.104417670682731,
      "grad_norm": 0.753747820854187,
      "learning_rate": 0.0001264097073518915,
      "loss": 0.0952,
      "step": 2063
    },
    {
      "epoch": 1.1049531459170012,
      "grad_norm": 0.2502298951148987,
      "learning_rate": 0.00012637401855817275,
      "loss": 0.0406,
      "step": 2064
    },
    {
      "epoch": 1.1054886211512718,
      "grad_norm": 2.0491890907287598,
      "learning_rate": 0.00012633832976445398,
      "loss": 0.198,
      "step": 2065
    },
    {
      "epoch": 1.106024096385542,
      "grad_norm": 0.7596263885498047,
      "learning_rate": 0.00012630264097073519,
      "loss": 0.0854,
      "step": 2066
    },
    {
      "epoch": 1.1065595716198127,
      "grad_norm": 0.6467105150222778,
      "learning_rate": 0.00012626695217701642,
      "loss": 0.1208,
      "step": 2067
    },
    {
      "epoch": 1.107095046854083,
      "grad_norm": 0.8674032092094421,
      "learning_rate": 0.00012623126338329765,
      "loss": 0.0573,
      "step": 2068
    },
    {
      "epoch": 1.1076305220883533,
      "grad_norm": 1.0107923746109009,
      "learning_rate": 0.00012619557458957886,
      "loss": 0.1903,
      "step": 2069
    },
    {
      "epoch": 1.1081659973226239,
      "grad_norm": 0.4564683139324188,
      "learning_rate": 0.00012615988579586012,
      "loss": 0.0582,
      "step": 2070
    },
    {
      "epoch": 1.1087014725568942,
      "grad_norm": 0.7302854061126709,
      "learning_rate": 0.00012612419700214133,
      "loss": 0.1291,
      "step": 2071
    },
    {
      "epoch": 1.1092369477911648,
      "grad_norm": 0.34577643871307373,
      "learning_rate": 0.00012608850820842256,
      "loss": 0.0562,
      "step": 2072
    },
    {
      "epoch": 1.109772423025435,
      "grad_norm": 0.7423960566520691,
      "learning_rate": 0.0001260528194147038,
      "loss": 0.1329,
      "step": 2073
    },
    {
      "epoch": 1.1103078982597054,
      "grad_norm": 0.6201387047767639,
      "learning_rate": 0.00012601713062098503,
      "loss": 0.0943,
      "step": 2074
    },
    {
      "epoch": 1.110843373493976,
      "grad_norm": 0.5228474140167236,
      "learning_rate": 0.00012598144182726623,
      "loss": 0.092,
      "step": 2075
    },
    {
      "epoch": 1.1113788487282463,
      "grad_norm": 0.4701695144176483,
      "learning_rate": 0.00012594575303354747,
      "loss": 0.0753,
      "step": 2076
    },
    {
      "epoch": 1.1119143239625167,
      "grad_norm": 0.4097941815853119,
      "learning_rate": 0.0001259100642398287,
      "loss": 0.0633,
      "step": 2077
    },
    {
      "epoch": 1.1124497991967872,
      "grad_norm": 0.5229597091674805,
      "learning_rate": 0.00012587437544610993,
      "loss": 0.0866,
      "step": 2078
    },
    {
      "epoch": 1.1129852744310575,
      "grad_norm": 0.31248050928115845,
      "learning_rate": 0.00012583868665239117,
      "loss": 0.0432,
      "step": 2079
    },
    {
      "epoch": 1.1135207496653279,
      "grad_norm": 0.41706523299217224,
      "learning_rate": 0.00012580299785867237,
      "loss": 0.0426,
      "step": 2080
    },
    {
      "epoch": 1.1140562248995984,
      "grad_norm": 0.3332251012325287,
      "learning_rate": 0.00012576730906495363,
      "loss": 0.0304,
      "step": 2081
    },
    {
      "epoch": 1.1145917001338688,
      "grad_norm": 2.977973222732544,
      "learning_rate": 0.00012573162027123484,
      "loss": 0.0905,
      "step": 2082
    },
    {
      "epoch": 1.1151271753681393,
      "grad_norm": 1.1331037282943726,
      "learning_rate": 0.00012569593147751607,
      "loss": 0.1443,
      "step": 2083
    },
    {
      "epoch": 1.1156626506024097,
      "grad_norm": 1.1630319356918335,
      "learning_rate": 0.0001256602426837973,
      "loss": 0.1114,
      "step": 2084
    },
    {
      "epoch": 1.11619812583668,
      "grad_norm": 0.8322272300720215,
      "learning_rate": 0.0001256245538900785,
      "loss": 0.1207,
      "step": 2085
    },
    {
      "epoch": 1.1167336010709505,
      "grad_norm": 0.7581587433815002,
      "learning_rate": 0.00012558886509635975,
      "loss": 0.0755,
      "step": 2086
    },
    {
      "epoch": 1.1172690763052209,
      "grad_norm": 10.79870319366455,
      "learning_rate": 0.00012555317630264098,
      "loss": 0.067,
      "step": 2087
    },
    {
      "epoch": 1.1178045515394912,
      "grad_norm": 7.783412933349609,
      "learning_rate": 0.0001255174875089222,
      "loss": 0.3502,
      "step": 2088
    },
    {
      "epoch": 1.1183400267737618,
      "grad_norm": 0.5993692278862,
      "learning_rate": 0.00012548179871520342,
      "loss": 0.0809,
      "step": 2089
    },
    {
      "epoch": 1.118875502008032,
      "grad_norm": 19.730968475341797,
      "learning_rate": 0.00012544610992148468,
      "loss": 0.2631,
      "step": 2090
    },
    {
      "epoch": 1.1194109772423024,
      "grad_norm": 5.416526794433594,
      "learning_rate": 0.00012541042112776589,
      "loss": 0.109,
      "step": 2091
    },
    {
      "epoch": 1.119946452476573,
      "grad_norm": 4.214034557342529,
      "learning_rate": 0.00012537473233404712,
      "loss": 0.0872,
      "step": 2092
    },
    {
      "epoch": 1.1204819277108433,
      "grad_norm": 4.886559009552002,
      "learning_rate": 0.00012533904354032835,
      "loss": 0.1467,
      "step": 2093
    },
    {
      "epoch": 1.1210174029451139,
      "grad_norm": 0.5305997133255005,
      "learning_rate": 0.00012530335474660956,
      "loss": 0.0162,
      "step": 2094
    },
    {
      "epoch": 1.1215528781793842,
      "grad_norm": 3.2413206100463867,
      "learning_rate": 0.0001252676659528908,
      "loss": 0.1979,
      "step": 2095
    },
    {
      "epoch": 1.1220883534136545,
      "grad_norm": 2.601630926132202,
      "learning_rate": 0.00012523197715917203,
      "loss": 0.0374,
      "step": 2096
    },
    {
      "epoch": 1.122623828647925,
      "grad_norm": 0.5968976020812988,
      "learning_rate": 0.00012519628836545326,
      "loss": 0.0688,
      "step": 2097
    },
    {
      "epoch": 1.1231593038821954,
      "grad_norm": 1.7760969400405884,
      "learning_rate": 0.00012516059957173447,
      "loss": 0.0643,
      "step": 2098
    },
    {
      "epoch": 1.123694779116466,
      "grad_norm": 0.963712751865387,
      "learning_rate": 0.00012512491077801573,
      "loss": 0.0226,
      "step": 2099
    },
    {
      "epoch": 1.1242302543507363,
      "grad_norm": 0.6509532928466797,
      "learning_rate": 0.00012508922198429693,
      "loss": 0.0707,
      "step": 2100
    },
    {
      "epoch": 1.1247657295850066,
      "grad_norm": 1.0043591260910034,
      "learning_rate": 0.00012505353319057817,
      "loss": 0.0776,
      "step": 2101
    },
    {
      "epoch": 1.1253012048192772,
      "grad_norm": 0.9363867044448853,
      "learning_rate": 0.0001250178443968594,
      "loss": 0.1577,
      "step": 2102
    },
    {
      "epoch": 1.1258366800535475,
      "grad_norm": 1.2409191131591797,
      "learning_rate": 0.0001249821556031406,
      "loss": 0.1307,
      "step": 2103
    },
    {
      "epoch": 1.1263721552878179,
      "grad_norm": 1.323626160621643,
      "learning_rate": 0.00012494646680942187,
      "loss": 0.0479,
      "step": 2104
    },
    {
      "epoch": 1.1269076305220884,
      "grad_norm": 7.595800399780273,
      "learning_rate": 0.00012491077801570307,
      "loss": 0.061,
      "step": 2105
    },
    {
      "epoch": 1.1274431057563588,
      "grad_norm": 0.5792321562767029,
      "learning_rate": 0.0001248750892219843,
      "loss": 0.1023,
      "step": 2106
    },
    {
      "epoch": 1.127978580990629,
      "grad_norm": 13.149088859558105,
      "learning_rate": 0.00012483940042826554,
      "loss": 0.1422,
      "step": 2107
    },
    {
      "epoch": 1.1285140562248996,
      "grad_norm": 1.7196474075317383,
      "learning_rate": 0.00012480371163454677,
      "loss": 0.0591,
      "step": 2108
    },
    {
      "epoch": 1.12904953145917,
      "grad_norm": 0.6191992163658142,
      "learning_rate": 0.00012476802284082798,
      "loss": 0.097,
      "step": 2109
    },
    {
      "epoch": 1.1295850066934405,
      "grad_norm": 0.5641098618507385,
      "learning_rate": 0.0001247323340471092,
      "loss": 0.055,
      "step": 2110
    },
    {
      "epoch": 1.1301204819277109,
      "grad_norm": 0.836245596408844,
      "learning_rate": 0.00012469664525339045,
      "loss": 0.0607,
      "step": 2111
    },
    {
      "epoch": 1.1306559571619812,
      "grad_norm": 0.9016752243041992,
      "learning_rate": 0.00012466095645967165,
      "loss": 0.0352,
      "step": 2112
    },
    {
      "epoch": 1.1311914323962518,
      "grad_norm": 6.01246976852417,
      "learning_rate": 0.0001246252676659529,
      "loss": 0.2159,
      "step": 2113
    },
    {
      "epoch": 1.131726907630522,
      "grad_norm": 0.5183223485946655,
      "learning_rate": 0.00012458957887223412,
      "loss": 0.0793,
      "step": 2114
    },
    {
      "epoch": 1.1322623828647924,
      "grad_norm": 1.5961834192276,
      "learning_rate": 0.00012455389007851535,
      "loss": 0.1047,
      "step": 2115
    },
    {
      "epoch": 1.132797858099063,
      "grad_norm": 5.77179479598999,
      "learning_rate": 0.00012451820128479659,
      "loss": 0.0966,
      "step": 2116
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.17053242027759552,
      "learning_rate": 0.00012448251249107782,
      "loss": 0.0246,
      "step": 2117
    },
    {
      "epoch": 1.1338688085676036,
      "grad_norm": 0.6898409128189087,
      "learning_rate": 0.00012444682369735903,
      "loss": 0.1372,
      "step": 2118
    },
    {
      "epoch": 1.1344042838018742,
      "grad_norm": 0.519650936126709,
      "learning_rate": 0.00012441113490364026,
      "loss": 0.0809,
      "step": 2119
    },
    {
      "epoch": 1.1349397590361445,
      "grad_norm": 0.34168002009391785,
      "learning_rate": 0.0001243754461099215,
      "loss": 0.0456,
      "step": 2120
    },
    {
      "epoch": 1.135475234270415,
      "grad_norm": 0.6397912502288818,
      "learning_rate": 0.0001243397573162027,
      "loss": 0.0687,
      "step": 2121
    },
    {
      "epoch": 1.1360107095046854,
      "grad_norm": 4.510692119598389,
      "learning_rate": 0.00012430406852248396,
      "loss": 0.1265,
      "step": 2122
    },
    {
      "epoch": 1.1365461847389557,
      "grad_norm": 4.226253509521484,
      "learning_rate": 0.00012426837972876517,
      "loss": 0.0678,
      "step": 2123
    },
    {
      "epoch": 1.1370816599732263,
      "grad_norm": 0.6142866611480713,
      "learning_rate": 0.0001242326909350464,
      "loss": 0.0683,
      "step": 2124
    },
    {
      "epoch": 1.1376171352074966,
      "grad_norm": 0.7040982842445374,
      "learning_rate": 0.00012419700214132763,
      "loss": 0.1135,
      "step": 2125
    },
    {
      "epoch": 1.1381526104417672,
      "grad_norm": 0.6735996603965759,
      "learning_rate": 0.00012416131334760887,
      "loss": 0.0292,
      "step": 2126
    },
    {
      "epoch": 1.1386880856760375,
      "grad_norm": 5.641808032989502,
      "learning_rate": 0.00012412562455389007,
      "loss": 0.1562,
      "step": 2127
    },
    {
      "epoch": 1.1392235609103079,
      "grad_norm": 0.3729953169822693,
      "learning_rate": 0.0001240899357601713,
      "loss": 0.0479,
      "step": 2128
    },
    {
      "epoch": 1.1397590361445784,
      "grad_norm": 0.5588295459747314,
      "learning_rate": 0.00012405424696645254,
      "loss": 0.1234,
      "step": 2129
    },
    {
      "epoch": 1.1402945113788487,
      "grad_norm": 0.18600088357925415,
      "learning_rate": 0.00012401855817273377,
      "loss": 0.0138,
      "step": 2130
    },
    {
      "epoch": 1.140829986613119,
      "grad_norm": 0.9780454635620117,
      "learning_rate": 0.000123982869379015,
      "loss": 0.077,
      "step": 2131
    },
    {
      "epoch": 1.1413654618473896,
      "grad_norm": 0.7633370161056519,
      "learning_rate": 0.0001239471805852962,
      "loss": 0.1242,
      "step": 2132
    },
    {
      "epoch": 1.14190093708166,
      "grad_norm": 0.6231510639190674,
      "learning_rate": 0.00012391149179157747,
      "loss": 0.1003,
      "step": 2133
    },
    {
      "epoch": 1.1424364123159303,
      "grad_norm": 11.492042541503906,
      "learning_rate": 0.00012387580299785868,
      "loss": 0.1851,
      "step": 2134
    },
    {
      "epoch": 1.1429718875502008,
      "grad_norm": 0.29012972116470337,
      "learning_rate": 0.0001238401142041399,
      "loss": 0.022,
      "step": 2135
    },
    {
      "epoch": 1.1435073627844712,
      "grad_norm": 1.0468640327453613,
      "learning_rate": 0.00012380442541042115,
      "loss": 0.1187,
      "step": 2136
    },
    {
      "epoch": 1.1440428380187417,
      "grad_norm": 1.7220196723937988,
      "learning_rate": 0.00012376873661670235,
      "loss": 0.1971,
      "step": 2137
    },
    {
      "epoch": 1.144578313253012,
      "grad_norm": 0.6715887784957886,
      "learning_rate": 0.00012373304782298359,
      "loss": 0.0759,
      "step": 2138
    },
    {
      "epoch": 1.1451137884872824,
      "grad_norm": 1.4803413152694702,
      "learning_rate": 0.00012369735902926482,
      "loss": 0.08,
      "step": 2139
    },
    {
      "epoch": 1.145649263721553,
      "grad_norm": 0.22438496351242065,
      "learning_rate": 0.00012366167023554605,
      "loss": 0.0308,
      "step": 2140
    },
    {
      "epoch": 1.1461847389558233,
      "grad_norm": 1.7894937992095947,
      "learning_rate": 0.00012362598144182726,
      "loss": 0.2135,
      "step": 2141
    },
    {
      "epoch": 1.1467202141900936,
      "grad_norm": 2.6398136615753174,
      "learning_rate": 0.00012359029264810852,
      "loss": 0.236,
      "step": 2142
    },
    {
      "epoch": 1.1472556894243642,
      "grad_norm": 2.0374772548675537,
      "learning_rate": 0.00012355460385438973,
      "loss": 0.1341,
      "step": 2143
    },
    {
      "epoch": 1.1477911646586345,
      "grad_norm": 4.460954666137695,
      "learning_rate": 0.00012351891506067096,
      "loss": 0.2092,
      "step": 2144
    },
    {
      "epoch": 1.1483266398929048,
      "grad_norm": 2.4468390941619873,
      "learning_rate": 0.0001234832262669522,
      "loss": 0.1038,
      "step": 2145
    },
    {
      "epoch": 1.1488621151271754,
      "grad_norm": 0.623221218585968,
      "learning_rate": 0.0001234475374732334,
      "loss": 0.1638,
      "step": 2146
    },
    {
      "epoch": 1.1493975903614457,
      "grad_norm": 3.1160888671875,
      "learning_rate": 0.00012341184867951463,
      "loss": 0.1215,
      "step": 2147
    },
    {
      "epoch": 1.1499330655957163,
      "grad_norm": 4.601621627807617,
      "learning_rate": 0.00012337615988579587,
      "loss": 0.1838,
      "step": 2148
    },
    {
      "epoch": 1.1504685408299866,
      "grad_norm": 0.5315081477165222,
      "learning_rate": 0.0001233404710920771,
      "loss": 0.0569,
      "step": 2149
    },
    {
      "epoch": 1.151004016064257,
      "grad_norm": 0.3582342863082886,
      "learning_rate": 0.0001233047822983583,
      "loss": 0.0958,
      "step": 2150
    },
    {
      "epoch": 1.1515394912985275,
      "grad_norm": 0.44465047121047974,
      "learning_rate": 0.00012326909350463957,
      "loss": 0.1392,
      "step": 2151
    },
    {
      "epoch": 1.1520749665327978,
      "grad_norm": 1.2901926040649414,
      "learning_rate": 0.00012323340471092077,
      "loss": 0.1258,
      "step": 2152
    },
    {
      "epoch": 1.1526104417670684,
      "grad_norm": 0.5517399311065674,
      "learning_rate": 0.000123197715917202,
      "loss": 0.0718,
      "step": 2153
    },
    {
      "epoch": 1.1531459170013387,
      "grad_norm": 0.6275817155838013,
      "learning_rate": 0.00012316202712348324,
      "loss": 0.0975,
      "step": 2154
    },
    {
      "epoch": 1.153681392235609,
      "grad_norm": 3.501629114151001,
      "learning_rate": 0.00012312633832976445,
      "loss": 0.2228,
      "step": 2155
    },
    {
      "epoch": 1.1542168674698796,
      "grad_norm": 1.0421851873397827,
      "learning_rate": 0.00012309064953604568,
      "loss": 0.1885,
      "step": 2156
    },
    {
      "epoch": 1.15475234270415,
      "grad_norm": 1.7128827571868896,
      "learning_rate": 0.0001230549607423269,
      "loss": 0.0954,
      "step": 2157
    },
    {
      "epoch": 1.1552878179384203,
      "grad_norm": 6.778240203857422,
      "learning_rate": 0.00012301927194860815,
      "loss": 0.1837,
      "step": 2158
    },
    {
      "epoch": 1.1558232931726908,
      "grad_norm": 0.7761740684509277,
      "learning_rate": 0.00012298358315488938,
      "loss": 0.1213,
      "step": 2159
    },
    {
      "epoch": 1.1563587684069612,
      "grad_norm": 0.6823260188102722,
      "learning_rate": 0.0001229478943611706,
      "loss": 0.1434,
      "step": 2160
    },
    {
      "epoch": 1.1568942436412315,
      "grad_norm": 0.4746623933315277,
      "learning_rate": 0.00012291220556745182,
      "loss": 0.0838,
      "step": 2161
    },
    {
      "epoch": 1.157429718875502,
      "grad_norm": 0.6452252864837646,
      "learning_rate": 0.00012287651677373305,
      "loss": 0.0775,
      "step": 2162
    },
    {
      "epoch": 1.1579651941097724,
      "grad_norm": 0.705312192440033,
      "learning_rate": 0.00012284082798001429,
      "loss": 0.155,
      "step": 2163
    },
    {
      "epoch": 1.158500669344043,
      "grad_norm": 0.8660988211631775,
      "learning_rate": 0.0001228051391862955,
      "loss": 0.1179,
      "step": 2164
    },
    {
      "epoch": 1.1590361445783133,
      "grad_norm": 0.38852447271347046,
      "learning_rate": 0.00012276945039257675,
      "loss": 0.0318,
      "step": 2165
    },
    {
      "epoch": 1.1595716198125836,
      "grad_norm": 0.42962995171546936,
      "learning_rate": 0.00012273376159885796,
      "loss": 0.1104,
      "step": 2166
    },
    {
      "epoch": 1.1601070950468542,
      "grad_norm": 0.4016266167163849,
      "learning_rate": 0.0001226980728051392,
      "loss": 0.0403,
      "step": 2167
    },
    {
      "epoch": 1.1606425702811245,
      "grad_norm": 1.7231943607330322,
      "learning_rate": 0.00012266238401142043,
      "loss": 0.0792,
      "step": 2168
    },
    {
      "epoch": 1.1611780455153948,
      "grad_norm": 0.40653306245803833,
      "learning_rate": 0.00012262669521770166,
      "loss": 0.0592,
      "step": 2169
    },
    {
      "epoch": 1.1617135207496654,
      "grad_norm": 0.5709068179130554,
      "learning_rate": 0.00012259100642398287,
      "loss": 0.0861,
      "step": 2170
    },
    {
      "epoch": 1.1622489959839357,
      "grad_norm": 0.752661406993866,
      "learning_rate": 0.0001225553176302641,
      "loss": 0.0591,
      "step": 2171
    },
    {
      "epoch": 1.162784471218206,
      "grad_norm": 0.7629634141921997,
      "learning_rate": 0.00012251962883654533,
      "loss": 0.1107,
      "step": 2172
    },
    {
      "epoch": 1.1633199464524766,
      "grad_norm": 0.7959577441215515,
      "learning_rate": 0.00012248394004282654,
      "loss": 0.0691,
      "step": 2173
    },
    {
      "epoch": 1.163855421686747,
      "grad_norm": 0.3533419072628021,
      "learning_rate": 0.0001224482512491078,
      "loss": 0.044,
      "step": 2174
    },
    {
      "epoch": 1.1643908969210175,
      "grad_norm": 0.5215753316879272,
      "learning_rate": 0.000122412562455389,
      "loss": 0.0958,
      "step": 2175
    },
    {
      "epoch": 1.1649263721552878,
      "grad_norm": 0.7113275527954102,
      "learning_rate": 0.00012237687366167024,
      "loss": 0.0732,
      "step": 2176
    },
    {
      "epoch": 1.1654618473895582,
      "grad_norm": 3.1830615997314453,
      "learning_rate": 0.00012234118486795147,
      "loss": 0.2567,
      "step": 2177
    },
    {
      "epoch": 1.1659973226238287,
      "grad_norm": 0.5730276703834534,
      "learning_rate": 0.0001223054960742327,
      "loss": 0.138,
      "step": 2178
    },
    {
      "epoch": 1.166532797858099,
      "grad_norm": 0.6410807371139526,
      "learning_rate": 0.0001222698072805139,
      "loss": 0.1492,
      "step": 2179
    },
    {
      "epoch": 1.1670682730923696,
      "grad_norm": 0.462098091840744,
      "learning_rate": 0.00012223411848679515,
      "loss": 0.0654,
      "step": 2180
    },
    {
      "epoch": 1.16760374832664,
      "grad_norm": 0.5389657020568848,
      "learning_rate": 0.00012219842969307638,
      "loss": 0.1125,
      "step": 2181
    },
    {
      "epoch": 1.1681392235609103,
      "grad_norm": 0.28009921312332153,
      "learning_rate": 0.00012216274089935759,
      "loss": 0.0437,
      "step": 2182
    },
    {
      "epoch": 1.1686746987951806,
      "grad_norm": 0.8180115818977356,
      "learning_rate": 0.00012212705210563885,
      "loss": 0.0681,
      "step": 2183
    },
    {
      "epoch": 1.1692101740294512,
      "grad_norm": 0.7682604193687439,
      "learning_rate": 0.00012209136331192005,
      "loss": 0.0702,
      "step": 2184
    },
    {
      "epoch": 1.1697456492637215,
      "grad_norm": 0.8364284038543701,
      "learning_rate": 0.0001220556745182013,
      "loss": 0.1275,
      "step": 2185
    },
    {
      "epoch": 1.170281124497992,
      "grad_norm": 0.7426795363426208,
      "learning_rate": 0.00012201998572448252,
      "loss": 0.0975,
      "step": 2186
    },
    {
      "epoch": 1.1708165997322624,
      "grad_norm": 1.5880272388458252,
      "learning_rate": 0.00012198429693076374,
      "loss": 0.0362,
      "step": 2187
    },
    {
      "epoch": 1.1713520749665327,
      "grad_norm": 0.8719965219497681,
      "learning_rate": 0.00012194860813704499,
      "loss": 0.1228,
      "step": 2188
    },
    {
      "epoch": 1.1718875502008033,
      "grad_norm": 0.6934506893157959,
      "learning_rate": 0.0001219129193433262,
      "loss": 0.1662,
      "step": 2189
    },
    {
      "epoch": 1.1724230254350736,
      "grad_norm": 0.6802822351455688,
      "learning_rate": 0.00012187723054960743,
      "loss": 0.0925,
      "step": 2190
    },
    {
      "epoch": 1.1729585006693442,
      "grad_norm": 5.226284980773926,
      "learning_rate": 0.00012184154175588867,
      "loss": 0.1278,
      "step": 2191
    },
    {
      "epoch": 1.1734939759036145,
      "grad_norm": 0.9103341102600098,
      "learning_rate": 0.00012180585296216989,
      "loss": 0.1491,
      "step": 2192
    },
    {
      "epoch": 1.1740294511378848,
      "grad_norm": 3.3124184608459473,
      "learning_rate": 0.00012177016416845111,
      "loss": 0.1376,
      "step": 2193
    },
    {
      "epoch": 1.1745649263721554,
      "grad_norm": 1.4606386423110962,
      "learning_rate": 0.00012173447537473235,
      "loss": 0.1826,
      "step": 2194
    },
    {
      "epoch": 1.1751004016064257,
      "grad_norm": 0.737045407295227,
      "learning_rate": 0.00012169878658101357,
      "loss": 0.0852,
      "step": 2195
    },
    {
      "epoch": 1.175635876840696,
      "grad_norm": 0.7482176423072815,
      "learning_rate": 0.00012166309778729479,
      "loss": 0.0165,
      "step": 2196
    },
    {
      "epoch": 1.1761713520749666,
      "grad_norm": 1.042503833770752,
      "learning_rate": 0.00012162740899357603,
      "loss": 0.0995,
      "step": 2197
    },
    {
      "epoch": 1.176706827309237,
      "grad_norm": 1.280431866645813,
      "learning_rate": 0.00012159172019985725,
      "loss": 0.1269,
      "step": 2198
    },
    {
      "epoch": 1.1772423025435073,
      "grad_norm": 0.9482187628746033,
      "learning_rate": 0.00012155603140613847,
      "loss": 0.1638,
      "step": 2199
    },
    {
      "epoch": 1.1777777777777778,
      "grad_norm": 1.00591242313385,
      "learning_rate": 0.00012152034261241972,
      "loss": 0.1188,
      "step": 2200
    },
    {
      "epoch": 1.1783132530120481,
      "grad_norm": 0.7773052453994751,
      "learning_rate": 0.00012148465381870094,
      "loss": 0.0929,
      "step": 2201
    },
    {
      "epoch": 1.1788487282463187,
      "grad_norm": 2.2251904010772705,
      "learning_rate": 0.00012144896502498216,
      "loss": 0.1224,
      "step": 2202
    },
    {
      "epoch": 1.179384203480589,
      "grad_norm": 0.5422545671463013,
      "learning_rate": 0.00012141327623126339,
      "loss": 0.0614,
      "step": 2203
    },
    {
      "epoch": 1.1799196787148594,
      "grad_norm": 0.5655293464660645,
      "learning_rate": 0.00012137758743754461,
      "loss": 0.043,
      "step": 2204
    },
    {
      "epoch": 1.18045515394913,
      "grad_norm": 0.5340176224708557,
      "learning_rate": 0.00012134189864382583,
      "loss": 0.0464,
      "step": 2205
    },
    {
      "epoch": 1.1809906291834003,
      "grad_norm": 0.8077865242958069,
      "learning_rate": 0.00012130620985010708,
      "loss": 0.1264,
      "step": 2206
    },
    {
      "epoch": 1.1815261044176706,
      "grad_norm": 0.6028401255607605,
      "learning_rate": 0.0001212705210563883,
      "loss": 0.0369,
      "step": 2207
    },
    {
      "epoch": 1.1820615796519411,
      "grad_norm": 0.9159555435180664,
      "learning_rate": 0.00012123483226266952,
      "loss": 0.1214,
      "step": 2208
    },
    {
      "epoch": 1.1825970548862115,
      "grad_norm": 2.1721553802490234,
      "learning_rate": 0.00012119914346895077,
      "loss": 0.0804,
      "step": 2209
    },
    {
      "epoch": 1.1831325301204818,
      "grad_norm": 1.5303977727890015,
      "learning_rate": 0.00012116345467523199,
      "loss": 0.1757,
      "step": 2210
    },
    {
      "epoch": 1.1836680053547524,
      "grad_norm": 0.5257555246353149,
      "learning_rate": 0.0001211277658815132,
      "loss": 0.0523,
      "step": 2211
    },
    {
      "epoch": 1.1842034805890227,
      "grad_norm": 0.8328247666358948,
      "learning_rate": 0.00012109207708779444,
      "loss": 0.1292,
      "step": 2212
    },
    {
      "epoch": 1.1847389558232932,
      "grad_norm": 0.6649461388587952,
      "learning_rate": 0.00012105638829407566,
      "loss": 0.1194,
      "step": 2213
    },
    {
      "epoch": 1.1852744310575636,
      "grad_norm": 1.2056958675384521,
      "learning_rate": 0.0001210206995003569,
      "loss": 0.167,
      "step": 2214
    },
    {
      "epoch": 1.185809906291834,
      "grad_norm": 0.8721201419830322,
      "learning_rate": 0.00012098501070663813,
      "loss": 0.0737,
      "step": 2215
    },
    {
      "epoch": 1.1863453815261045,
      "grad_norm": 0.7648637294769287,
      "learning_rate": 0.00012094932191291935,
      "loss": 0.1498,
      "step": 2216
    },
    {
      "epoch": 1.1868808567603748,
      "grad_norm": 0.6450560092926025,
      "learning_rate": 0.00012091363311920059,
      "loss": 0.1129,
      "step": 2217
    },
    {
      "epoch": 1.1874163319946454,
      "grad_norm": 2.1491775512695312,
      "learning_rate": 0.00012087794432548181,
      "loss": 0.1733,
      "step": 2218
    },
    {
      "epoch": 1.1879518072289157,
      "grad_norm": 0.5566316843032837,
      "learning_rate": 0.00012084225553176303,
      "loss": 0.066,
      "step": 2219
    },
    {
      "epoch": 1.188487282463186,
      "grad_norm": 0.767387866973877,
      "learning_rate": 0.00012080656673804427,
      "loss": 0.1623,
      "step": 2220
    },
    {
      "epoch": 1.1890227576974566,
      "grad_norm": 1.7990164756774902,
      "learning_rate": 0.00012077087794432549,
      "loss": 0.2329,
      "step": 2221
    },
    {
      "epoch": 1.189558232931727,
      "grad_norm": 0.6558026075363159,
      "learning_rate": 0.0001207351891506067,
      "loss": 0.0466,
      "step": 2222
    },
    {
      "epoch": 1.1900937081659972,
      "grad_norm": 0.645628035068512,
      "learning_rate": 0.00012069950035688795,
      "loss": 0.0781,
      "step": 2223
    },
    {
      "epoch": 1.1906291834002678,
      "grad_norm": 0.6193550825119019,
      "learning_rate": 0.00012066381156316917,
      "loss": 0.0977,
      "step": 2224
    },
    {
      "epoch": 1.1911646586345381,
      "grad_norm": 1.7645529508590698,
      "learning_rate": 0.00012062812276945039,
      "loss": 0.1371,
      "step": 2225
    },
    {
      "epoch": 1.1917001338688085,
      "grad_norm": 0.47548067569732666,
      "learning_rate": 0.00012059243397573164,
      "loss": 0.1002,
      "step": 2226
    },
    {
      "epoch": 1.192235609103079,
      "grad_norm": 0.47065818309783936,
      "learning_rate": 0.00012055674518201286,
      "loss": 0.1362,
      "step": 2227
    },
    {
      "epoch": 1.1927710843373494,
      "grad_norm": 1.030134916305542,
      "learning_rate": 0.00012052105638829408,
      "loss": 0.1067,
      "step": 2228
    },
    {
      "epoch": 1.19330655957162,
      "grad_norm": 0.5183367729187012,
      "learning_rate": 0.00012048536759457531,
      "loss": 0.108,
      "step": 2229
    },
    {
      "epoch": 1.1938420348058902,
      "grad_norm": 1.0158761739730835,
      "learning_rate": 0.00012044967880085653,
      "loss": 0.153,
      "step": 2230
    },
    {
      "epoch": 1.1943775100401606,
      "grad_norm": 2.741692066192627,
      "learning_rate": 0.00012041399000713775,
      "loss": 0.0485,
      "step": 2231
    },
    {
      "epoch": 1.1949129852744311,
      "grad_norm": 0.4894540011882782,
      "learning_rate": 0.000120378301213419,
      "loss": 0.0625,
      "step": 2232
    },
    {
      "epoch": 1.1954484605087015,
      "grad_norm": 1.0261579751968384,
      "learning_rate": 0.00012034261241970022,
      "loss": 0.0928,
      "step": 2233
    },
    {
      "epoch": 1.1959839357429718,
      "grad_norm": 0.5298573970794678,
      "learning_rate": 0.00012030692362598144,
      "loss": 0.0732,
      "step": 2234
    },
    {
      "epoch": 1.1965194109772423,
      "grad_norm": 0.4396470785140991,
      "learning_rate": 0.00012027123483226269,
      "loss": 0.0558,
      "step": 2235
    },
    {
      "epoch": 1.1970548862115127,
      "grad_norm": 0.4038689136505127,
      "learning_rate": 0.0001202355460385439,
      "loss": 0.0641,
      "step": 2236
    },
    {
      "epoch": 1.197590361445783,
      "grad_norm": 0.4698563516139984,
      "learning_rate": 0.00012019985724482513,
      "loss": 0.0629,
      "step": 2237
    },
    {
      "epoch": 1.1981258366800536,
      "grad_norm": 12.911954879760742,
      "learning_rate": 0.00012016416845110636,
      "loss": 0.1432,
      "step": 2238
    },
    {
      "epoch": 1.198661311914324,
      "grad_norm": 0.33903244137763977,
      "learning_rate": 0.00012012847965738758,
      "loss": 0.07,
      "step": 2239
    },
    {
      "epoch": 1.1991967871485945,
      "grad_norm": 0.5061615109443665,
      "learning_rate": 0.00012009279086366883,
      "loss": 0.0945,
      "step": 2240
    },
    {
      "epoch": 1.1997322623828648,
      "grad_norm": 0.4456866979598999,
      "learning_rate": 0.00012005710206995005,
      "loss": 0.0423,
      "step": 2241
    },
    {
      "epoch": 1.2002677376171351,
      "grad_norm": 0.37813839316368103,
      "learning_rate": 0.00012002141327623127,
      "loss": 0.0907,
      "step": 2242
    },
    {
      "epoch": 1.2008032128514057,
      "grad_norm": 0.5802229046821594,
      "learning_rate": 0.00011998572448251251,
      "loss": 0.1292,
      "step": 2243
    },
    {
      "epoch": 1.201338688085676,
      "grad_norm": 0.5797330737113953,
      "learning_rate": 0.00011995003568879373,
      "loss": 0.1214,
      "step": 2244
    },
    {
      "epoch": 1.2018741633199466,
      "grad_norm": 0.5892624258995056,
      "learning_rate": 0.00011991434689507495,
      "loss": 0.0586,
      "step": 2245
    },
    {
      "epoch": 1.202409638554217,
      "grad_norm": 0.4388095736503601,
      "learning_rate": 0.00011987865810135619,
      "loss": 0.0812,
      "step": 2246
    },
    {
      "epoch": 1.2029451137884872,
      "grad_norm": 0.9001673460006714,
      "learning_rate": 0.0001198429693076374,
      "loss": 0.1205,
      "step": 2247
    },
    {
      "epoch": 1.2034805890227578,
      "grad_norm": 0.8486156463623047,
      "learning_rate": 0.00011980728051391863,
      "loss": 0.2627,
      "step": 2248
    },
    {
      "epoch": 1.2040160642570281,
      "grad_norm": 0.6445284485816956,
      "learning_rate": 0.00011977159172019987,
      "loss": 0.1179,
      "step": 2249
    },
    {
      "epoch": 1.2045515394912985,
      "grad_norm": 0.2806343734264374,
      "learning_rate": 0.00011973590292648109,
      "loss": 0.0345,
      "step": 2250
    },
    {
      "epoch": 1.205087014725569,
      "grad_norm": 0.6171635985374451,
      "learning_rate": 0.00011970021413276231,
      "loss": 0.0978,
      "step": 2251
    },
    {
      "epoch": 1.2056224899598393,
      "grad_norm": 0.8255059123039246,
      "learning_rate": 0.00011966452533904356,
      "loss": 0.2135,
      "step": 2252
    },
    {
      "epoch": 1.2061579651941097,
      "grad_norm": 0.8326955437660217,
      "learning_rate": 0.00011962883654532478,
      "loss": 0.0778,
      "step": 2253
    },
    {
      "epoch": 1.2066934404283802,
      "grad_norm": 0.5721989870071411,
      "learning_rate": 0.000119593147751606,
      "loss": 0.1761,
      "step": 2254
    },
    {
      "epoch": 1.2072289156626506,
      "grad_norm": 0.7824882864952087,
      "learning_rate": 0.00011955745895788723,
      "loss": 0.1307,
      "step": 2255
    },
    {
      "epoch": 1.2077643908969211,
      "grad_norm": 0.548427939414978,
      "learning_rate": 0.00011952177016416845,
      "loss": 0.063,
      "step": 2256
    },
    {
      "epoch": 1.2082998661311914,
      "grad_norm": 0.5212033987045288,
      "learning_rate": 0.00011948608137044967,
      "loss": 0.1196,
      "step": 2257
    },
    {
      "epoch": 1.2088353413654618,
      "grad_norm": 0.6201544404029846,
      "learning_rate": 0.00011945039257673092,
      "loss": 0.1046,
      "step": 2258
    },
    {
      "epoch": 1.2093708165997323,
      "grad_norm": 4.070842266082764,
      "learning_rate": 0.00011941470378301214,
      "loss": 0.1396,
      "step": 2259
    },
    {
      "epoch": 1.2099062918340027,
      "grad_norm": 0.5772285461425781,
      "learning_rate": 0.00011937901498929336,
      "loss": 0.1108,
      "step": 2260
    },
    {
      "epoch": 1.210441767068273,
      "grad_norm": 0.9113491773605347,
      "learning_rate": 0.0001193433261955746,
      "loss": 0.0586,
      "step": 2261
    },
    {
      "epoch": 1.2109772423025436,
      "grad_norm": 2.2013752460479736,
      "learning_rate": 0.00011930763740185583,
      "loss": 0.0134,
      "step": 2262
    },
    {
      "epoch": 1.2115127175368139,
      "grad_norm": 0.7197171449661255,
      "learning_rate": 0.00011927194860813705,
      "loss": 0.0787,
      "step": 2263
    },
    {
      "epoch": 1.2120481927710842,
      "grad_norm": 1.079020380973816,
      "learning_rate": 0.00011923625981441828,
      "loss": 0.246,
      "step": 2264
    },
    {
      "epoch": 1.2125836680053548,
      "grad_norm": 3.442110300064087,
      "learning_rate": 0.0001192005710206995,
      "loss": 0.1945,
      "step": 2265
    },
    {
      "epoch": 1.213119143239625,
      "grad_norm": 1.2508556842803955,
      "learning_rate": 0.00011916488222698072,
      "loss": 0.1274,
      "step": 2266
    },
    {
      "epoch": 1.2136546184738957,
      "grad_norm": 0.3460962176322937,
      "learning_rate": 0.00011912919343326197,
      "loss": 0.0531,
      "step": 2267
    },
    {
      "epoch": 1.214190093708166,
      "grad_norm": 0.3766161799430847,
      "learning_rate": 0.00011909350463954319,
      "loss": 0.0734,
      "step": 2268
    },
    {
      "epoch": 1.2147255689424363,
      "grad_norm": 0.7146509885787964,
      "learning_rate": 0.00011905781584582443,
      "loss": 0.1146,
      "step": 2269
    },
    {
      "epoch": 1.2152610441767069,
      "grad_norm": 0.5902477502822876,
      "learning_rate": 0.00011902212705210565,
      "loss": 0.1255,
      "step": 2270
    },
    {
      "epoch": 1.2157965194109772,
      "grad_norm": 0.45404452085494995,
      "learning_rate": 0.00011898643825838687,
      "loss": 0.073,
      "step": 2271
    },
    {
      "epoch": 1.2163319946452478,
      "grad_norm": 0.5392524600028992,
      "learning_rate": 0.0001189507494646681,
      "loss": 0.0794,
      "step": 2272
    },
    {
      "epoch": 1.216867469879518,
      "grad_norm": 1.0608712434768677,
      "learning_rate": 0.00011891506067094933,
      "loss": 0.1426,
      "step": 2273
    },
    {
      "epoch": 1.2174029451137884,
      "grad_norm": 0.3818434178829193,
      "learning_rate": 0.00011887937187723055,
      "loss": 0.0947,
      "step": 2274
    },
    {
      "epoch": 1.217938420348059,
      "grad_norm": 0.4577045142650604,
      "learning_rate": 0.00011884368308351179,
      "loss": 0.0515,
      "step": 2275
    },
    {
      "epoch": 1.2184738955823293,
      "grad_norm": 0.5549942851066589,
      "learning_rate": 0.00011880799428979301,
      "loss": 0.1528,
      "step": 2276
    },
    {
      "epoch": 1.2190093708165997,
      "grad_norm": 5.120337009429932,
      "learning_rate": 0.00011877230549607423,
      "loss": 0.0876,
      "step": 2277
    },
    {
      "epoch": 1.2195448460508702,
      "grad_norm": 10.066218376159668,
      "learning_rate": 0.00011873661670235548,
      "loss": 0.1175,
      "step": 2278
    },
    {
      "epoch": 1.2200803212851405,
      "grad_norm": 0.7694584727287292,
      "learning_rate": 0.0001187009279086367,
      "loss": 0.1169,
      "step": 2279
    },
    {
      "epoch": 1.2206157965194109,
      "grad_norm": 0.3826743960380554,
      "learning_rate": 0.00011866523911491792,
      "loss": 0.0864,
      "step": 2280
    },
    {
      "epoch": 1.2211512717536814,
      "grad_norm": 0.8921605348587036,
      "learning_rate": 0.00011862955032119915,
      "loss": 0.0606,
      "step": 2281
    },
    {
      "epoch": 1.2216867469879518,
      "grad_norm": 0.7080063223838806,
      "learning_rate": 0.00011859386152748037,
      "loss": 0.101,
      "step": 2282
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 0.11951299011707306,
      "learning_rate": 0.00011855817273376159,
      "loss": 0.005,
      "step": 2283
    },
    {
      "epoch": 1.2227576974564927,
      "grad_norm": 0.9709359407424927,
      "learning_rate": 0.00011852248394004284,
      "loss": 0.1263,
      "step": 2284
    },
    {
      "epoch": 1.223293172690763,
      "grad_norm": 0.5201207399368286,
      "learning_rate": 0.00011848679514632406,
      "loss": 0.0892,
      "step": 2285
    },
    {
      "epoch": 1.2238286479250335,
      "grad_norm": 0.7237004637718201,
      "learning_rate": 0.00011845110635260528,
      "loss": 0.1435,
      "step": 2286
    },
    {
      "epoch": 1.2243641231593039,
      "grad_norm": 0.4414769113063812,
      "learning_rate": 0.00011841541755888653,
      "loss": 0.1018,
      "step": 2287
    },
    {
      "epoch": 1.2248995983935742,
      "grad_norm": 0.5228033065795898,
      "learning_rate": 0.00011837972876516775,
      "loss": 0.0958,
      "step": 2288
    },
    {
      "epoch": 1.2254350736278448,
      "grad_norm": 3.584292411804199,
      "learning_rate": 0.00011834403997144897,
      "loss": 0.1593,
      "step": 2289
    },
    {
      "epoch": 1.225970548862115,
      "grad_norm": 0.6006929278373718,
      "learning_rate": 0.0001183083511777302,
      "loss": 0.0861,
      "step": 2290
    },
    {
      "epoch": 1.2265060240963854,
      "grad_norm": 0.41127797961235046,
      "learning_rate": 0.00011827266238401142,
      "loss": 0.0757,
      "step": 2291
    },
    {
      "epoch": 1.227041499330656,
      "grad_norm": 0.6817023754119873,
      "learning_rate": 0.00011823697359029264,
      "loss": 0.0768,
      "step": 2292
    },
    {
      "epoch": 1.2275769745649263,
      "grad_norm": 0.6515899896621704,
      "learning_rate": 0.00011820128479657389,
      "loss": 0.1016,
      "step": 2293
    },
    {
      "epoch": 1.2281124497991969,
      "grad_norm": 0.27722278237342834,
      "learning_rate": 0.0001181655960028551,
      "loss": 0.029,
      "step": 2294
    },
    {
      "epoch": 1.2286479250334672,
      "grad_norm": 1.056686282157898,
      "learning_rate": 0.00011812990720913635,
      "loss": 0.1071,
      "step": 2295
    },
    {
      "epoch": 1.2291834002677375,
      "grad_norm": 0.5428939461708069,
      "learning_rate": 0.00011809421841541757,
      "loss": 0.0853,
      "step": 2296
    },
    {
      "epoch": 1.229718875502008,
      "grad_norm": 0.9307891130447388,
      "learning_rate": 0.00011805852962169879,
      "loss": 0.1189,
      "step": 2297
    },
    {
      "epoch": 1.2302543507362784,
      "grad_norm": 2.1473047733306885,
      "learning_rate": 0.00011802284082798003,
      "loss": 0.1213,
      "step": 2298
    },
    {
      "epoch": 1.230789825970549,
      "grad_norm": 0.4770399332046509,
      "learning_rate": 0.00011798715203426125,
      "loss": 0.0576,
      "step": 2299
    },
    {
      "epoch": 1.2313253012048193,
      "grad_norm": 0.5086423754692078,
      "learning_rate": 0.00011795146324054247,
      "loss": 0.0562,
      "step": 2300
    },
    {
      "epoch": 1.2318607764390896,
      "grad_norm": 0.4099157154560089,
      "learning_rate": 0.00011791577444682371,
      "loss": 0.109,
      "step": 2301
    },
    {
      "epoch": 1.2323962516733602,
      "grad_norm": 0.519138753414154,
      "learning_rate": 0.00011788008565310493,
      "loss": 0.1072,
      "step": 2302
    },
    {
      "epoch": 1.2329317269076305,
      "grad_norm": 0.6459906697273254,
      "learning_rate": 0.00011784439685938615,
      "loss": 0.1332,
      "step": 2303
    },
    {
      "epoch": 1.2334672021419009,
      "grad_norm": 0.7256982922554016,
      "learning_rate": 0.0001178087080656674,
      "loss": 0.1223,
      "step": 2304
    },
    {
      "epoch": 1.2340026773761714,
      "grad_norm": 0.6978033185005188,
      "learning_rate": 0.00011777301927194862,
      "loss": 0.0849,
      "step": 2305
    },
    {
      "epoch": 1.2345381526104418,
      "grad_norm": 0.47463446855545044,
      "learning_rate": 0.00011773733047822984,
      "loss": 0.0896,
      "step": 2306
    },
    {
      "epoch": 1.235073627844712,
      "grad_norm": 3.1032462120056152,
      "learning_rate": 0.00011770164168451107,
      "loss": 0.0563,
      "step": 2307
    },
    {
      "epoch": 1.2356091030789826,
      "grad_norm": 0.7332308888435364,
      "learning_rate": 0.00011766595289079229,
      "loss": 0.2231,
      "step": 2308
    },
    {
      "epoch": 1.236144578313253,
      "grad_norm": 0.3042541444301605,
      "learning_rate": 0.00011763026409707351,
      "loss": 0.0635,
      "step": 2309
    },
    {
      "epoch": 1.2366800535475235,
      "grad_norm": 0.4193481504917145,
      "learning_rate": 0.00011759457530335476,
      "loss": 0.0586,
      "step": 2310
    },
    {
      "epoch": 1.2372155287817939,
      "grad_norm": 0.4658161997795105,
      "learning_rate": 0.00011755888650963598,
      "loss": 0.0792,
      "step": 2311
    },
    {
      "epoch": 1.2377510040160642,
      "grad_norm": 0.47963523864746094,
      "learning_rate": 0.0001175231977159172,
      "loss": 0.1266,
      "step": 2312
    },
    {
      "epoch": 1.2382864792503347,
      "grad_norm": 0.4453890025615692,
      "learning_rate": 0.00011748750892219845,
      "loss": 0.1035,
      "step": 2313
    },
    {
      "epoch": 1.238821954484605,
      "grad_norm": 0.4738151729106903,
      "learning_rate": 0.00011745182012847967,
      "loss": 0.1284,
      "step": 2314
    },
    {
      "epoch": 1.2393574297188754,
      "grad_norm": 0.47912728786468506,
      "learning_rate": 0.00011741613133476089,
      "loss": 0.1,
      "step": 2315
    },
    {
      "epoch": 1.239892904953146,
      "grad_norm": 0.5497127771377563,
      "learning_rate": 0.00011738044254104212,
      "loss": 0.0185,
      "step": 2316
    },
    {
      "epoch": 1.2404283801874163,
      "grad_norm": 0.48886391520500183,
      "learning_rate": 0.00011734475374732334,
      "loss": 0.0798,
      "step": 2317
    },
    {
      "epoch": 1.2409638554216866,
      "grad_norm": 0.36408114433288574,
      "learning_rate": 0.00011730906495360456,
      "loss": 0.0486,
      "step": 2318
    },
    {
      "epoch": 1.2414993306559572,
      "grad_norm": 0.5395285487174988,
      "learning_rate": 0.0001172733761598858,
      "loss": 0.1225,
      "step": 2319
    },
    {
      "epoch": 1.2420348058902275,
      "grad_norm": 0.21937385201454163,
      "learning_rate": 0.00011723768736616703,
      "loss": 0.038,
      "step": 2320
    },
    {
      "epoch": 1.242570281124498,
      "grad_norm": 0.6802325248718262,
      "learning_rate": 0.00011720199857244825,
      "loss": 0.0582,
      "step": 2321
    },
    {
      "epoch": 1.2431057563587684,
      "grad_norm": 0.391752153635025,
      "learning_rate": 0.00011716630977872949,
      "loss": 0.064,
      "step": 2322
    },
    {
      "epoch": 1.2436412315930387,
      "grad_norm": 0.7006322145462036,
      "learning_rate": 0.00011713062098501071,
      "loss": 0.0951,
      "step": 2323
    },
    {
      "epoch": 1.2441767068273093,
      "grad_norm": 0.6128446459770203,
      "learning_rate": 0.00011709493219129195,
      "loss": 0.0291,
      "step": 2324
    },
    {
      "epoch": 1.2447121820615796,
      "grad_norm": 0.5722881555557251,
      "learning_rate": 0.00011705924339757317,
      "loss": 0.0647,
      "step": 2325
    },
    {
      "epoch": 1.2452476572958502,
      "grad_norm": 0.7212793827056885,
      "learning_rate": 0.00011702355460385439,
      "loss": 0.0822,
      "step": 2326
    },
    {
      "epoch": 1.2457831325301205,
      "grad_norm": 0.5252508521080017,
      "learning_rate": 0.00011698786581013563,
      "loss": 0.0508,
      "step": 2327
    },
    {
      "epoch": 1.2463186077643909,
      "grad_norm": 0.7781971096992493,
      "learning_rate": 0.00011695217701641685,
      "loss": 0.1192,
      "step": 2328
    },
    {
      "epoch": 1.2468540829986614,
      "grad_norm": 0.39907148480415344,
      "learning_rate": 0.00011691648822269807,
      "loss": 0.1036,
      "step": 2329
    },
    {
      "epoch": 1.2473895582329317,
      "grad_norm": 0.43771642446517944,
      "learning_rate": 0.00011688079942897932,
      "loss": 0.0814,
      "step": 2330
    },
    {
      "epoch": 1.247925033467202,
      "grad_norm": 0.8486381769180298,
      "learning_rate": 0.00011684511063526054,
      "loss": 0.0847,
      "step": 2331
    },
    {
      "epoch": 1.2484605087014726,
      "grad_norm": 0.535280704498291,
      "learning_rate": 0.00011680942184154176,
      "loss": 0.1019,
      "step": 2332
    },
    {
      "epoch": 1.248995983935743,
      "grad_norm": 0.7947676777839661,
      "learning_rate": 0.00011677373304782299,
      "loss": 0.0704,
      "step": 2333
    },
    {
      "epoch": 1.2495314591700133,
      "grad_norm": 0.47753024101257324,
      "learning_rate": 0.00011673804425410421,
      "loss": 0.0498,
      "step": 2334
    },
    {
      "epoch": 1.2500669344042838,
      "grad_norm": 0.6110979914665222,
      "learning_rate": 0.00011670235546038543,
      "loss": 0.0856,
      "step": 2335
    },
    {
      "epoch": 1.2506024096385542,
      "grad_norm": 1.4577856063842773,
      "learning_rate": 0.00011666666666666668,
      "loss": 0.1058,
      "step": 2336
    },
    {
      "epoch": 1.2511378848728247,
      "grad_norm": 0.6727633476257324,
      "learning_rate": 0.0001166309778729479,
      "loss": 0.1019,
      "step": 2337
    },
    {
      "epoch": 1.251673360107095,
      "grad_norm": 0.7221118807792664,
      "learning_rate": 0.00011659528907922912,
      "loss": 0.1572,
      "step": 2338
    },
    {
      "epoch": 1.2522088353413654,
      "grad_norm": 0.7357471585273743,
      "learning_rate": 0.00011655960028551037,
      "loss": 0.0771,
      "step": 2339
    },
    {
      "epoch": 1.252744310575636,
      "grad_norm": 0.5289751291275024,
      "learning_rate": 0.00011652391149179159,
      "loss": 0.08,
      "step": 2340
    },
    {
      "epoch": 1.2532797858099063,
      "grad_norm": 0.932175874710083,
      "learning_rate": 0.0001164882226980728,
      "loss": 0.1105,
      "step": 2341
    },
    {
      "epoch": 1.2538152610441768,
      "grad_norm": 2.6826369762420654,
      "learning_rate": 0.00011645253390435404,
      "loss": 0.0866,
      "step": 2342
    },
    {
      "epoch": 1.2543507362784472,
      "grad_norm": 0.6102812886238098,
      "learning_rate": 0.00011641684511063526,
      "loss": 0.121,
      "step": 2343
    },
    {
      "epoch": 1.2548862115127175,
      "grad_norm": 0.7059262990951538,
      "learning_rate": 0.00011638115631691648,
      "loss": 0.0917,
      "step": 2344
    },
    {
      "epoch": 1.2554216867469878,
      "grad_norm": 0.49364620447158813,
      "learning_rate": 0.00011634546752319773,
      "loss": 0.0629,
      "step": 2345
    },
    {
      "epoch": 1.2559571619812584,
      "grad_norm": 0.3358619511127472,
      "learning_rate": 0.00011630977872947895,
      "loss": 0.0856,
      "step": 2346
    },
    {
      "epoch": 1.2564926372155287,
      "grad_norm": 0.48398080468177795,
      "learning_rate": 0.00011627408993576017,
      "loss": 0.0613,
      "step": 2347
    },
    {
      "epoch": 1.2570281124497993,
      "grad_norm": 1.5741500854492188,
      "learning_rate": 0.00011623840114204141,
      "loss": 0.1942,
      "step": 2348
    },
    {
      "epoch": 1.2575635876840696,
      "grad_norm": 0.5139914155006409,
      "learning_rate": 0.00011620271234832263,
      "loss": 0.0961,
      "step": 2349
    },
    {
      "epoch": 1.25809906291834,
      "grad_norm": 0.5200440287590027,
      "learning_rate": 0.00011616702355460387,
      "loss": 0.0456,
      "step": 2350
    },
    {
      "epoch": 1.2586345381526105,
      "grad_norm": 0.24222932755947113,
      "learning_rate": 0.00011613133476088509,
      "loss": 0.0697,
      "step": 2351
    },
    {
      "epoch": 1.2591700133868808,
      "grad_norm": 1.3174344301223755,
      "learning_rate": 0.0001160956459671663,
      "loss": 0.1732,
      "step": 2352
    },
    {
      "epoch": 1.2597054886211514,
      "grad_norm": 0.55230313539505,
      "learning_rate": 0.00011605995717344755,
      "loss": 0.0119,
      "step": 2353
    },
    {
      "epoch": 1.2602409638554217,
      "grad_norm": 0.822839617729187,
      "learning_rate": 0.00011602426837972877,
      "loss": 0.1333,
      "step": 2354
    },
    {
      "epoch": 1.260776439089692,
      "grad_norm": 0.5658901333808899,
      "learning_rate": 0.00011598857958600999,
      "loss": 0.0946,
      "step": 2355
    },
    {
      "epoch": 1.2613119143239624,
      "grad_norm": 0.5155389904975891,
      "learning_rate": 0.00011595289079229124,
      "loss": 0.0565,
      "step": 2356
    },
    {
      "epoch": 1.261847389558233,
      "grad_norm": 0.4646870195865631,
      "learning_rate": 0.00011591720199857246,
      "loss": 0.0703,
      "step": 2357
    },
    {
      "epoch": 1.2623828647925033,
      "grad_norm": 1.5280811786651611,
      "learning_rate": 0.00011588151320485368,
      "loss": 0.1571,
      "step": 2358
    },
    {
      "epoch": 1.2629183400267738,
      "grad_norm": 0.5597587823867798,
      "learning_rate": 0.00011584582441113491,
      "loss": 0.0842,
      "step": 2359
    },
    {
      "epoch": 1.2634538152610442,
      "grad_norm": 2.604888439178467,
      "learning_rate": 0.00011581013561741613,
      "loss": 0.0852,
      "step": 2360
    },
    {
      "epoch": 1.2639892904953145,
      "grad_norm": 0.9068875312805176,
      "learning_rate": 0.00011577444682369735,
      "loss": 0.096,
      "step": 2361
    },
    {
      "epoch": 1.264524765729585,
      "grad_norm": 0.9299049973487854,
      "learning_rate": 0.0001157387580299786,
      "loss": 0.1517,
      "step": 2362
    },
    {
      "epoch": 1.2650602409638554,
      "grad_norm": 0.5777910947799683,
      "learning_rate": 0.00011570306923625982,
      "loss": 0.0628,
      "step": 2363
    },
    {
      "epoch": 1.265595716198126,
      "grad_norm": 0.5731183886528015,
      "learning_rate": 0.00011566738044254104,
      "loss": 0.062,
      "step": 2364
    },
    {
      "epoch": 1.2661311914323963,
      "grad_norm": 0.528269350528717,
      "learning_rate": 0.00011563169164882229,
      "loss": 0.0655,
      "step": 2365
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.5596484541893005,
      "learning_rate": 0.0001155960028551035,
      "loss": 0.1045,
      "step": 2366
    },
    {
      "epoch": 1.2672021419009372,
      "grad_norm": 0.4087652266025543,
      "learning_rate": 0.00011556031406138473,
      "loss": 0.0654,
      "step": 2367
    },
    {
      "epoch": 1.2677376171352075,
      "grad_norm": 0.6889753341674805,
      "learning_rate": 0.00011552462526766596,
      "loss": 0.1439,
      "step": 2368
    },
    {
      "epoch": 1.268273092369478,
      "grad_norm": 0.9790465831756592,
      "learning_rate": 0.00011548893647394718,
      "loss": 0.08,
      "step": 2369
    },
    {
      "epoch": 1.2688085676037484,
      "grad_norm": 0.5814647078514099,
      "learning_rate": 0.0001154532476802284,
      "loss": 0.0856,
      "step": 2370
    },
    {
      "epoch": 1.2693440428380187,
      "grad_norm": 0.6024813055992126,
      "learning_rate": 0.00011541755888650965,
      "loss": 0.089,
      "step": 2371
    },
    {
      "epoch": 1.269879518072289,
      "grad_norm": 0.794360339641571,
      "learning_rate": 0.00011538187009279087,
      "loss": 0.117,
      "step": 2372
    },
    {
      "epoch": 1.2704149933065596,
      "grad_norm": 0.29196423292160034,
      "learning_rate": 0.00011534618129907209,
      "loss": 0.0543,
      "step": 2373
    },
    {
      "epoch": 1.27095046854083,
      "grad_norm": 0.668165922164917,
      "learning_rate": 0.00011531049250535333,
      "loss": 0.088,
      "step": 2374
    },
    {
      "epoch": 1.2714859437751005,
      "grad_norm": 0.5228837728500366,
      "learning_rate": 0.00011527480371163455,
      "loss": 0.1029,
      "step": 2375
    },
    {
      "epoch": 1.2720214190093708,
      "grad_norm": 0.7867467403411865,
      "learning_rate": 0.00011523911491791577,
      "loss": 0.0851,
      "step": 2376
    },
    {
      "epoch": 1.2725568942436412,
      "grad_norm": 0.9184578061103821,
      "learning_rate": 0.000115203426124197,
      "loss": 0.1068,
      "step": 2377
    },
    {
      "epoch": 1.2730923694779117,
      "grad_norm": 1.4198873043060303,
      "learning_rate": 0.00011516773733047823,
      "loss": 0.1251,
      "step": 2378
    },
    {
      "epoch": 1.273627844712182,
      "grad_norm": 0.4177301526069641,
      "learning_rate": 0.00011513204853675947,
      "loss": 0.0729,
      "step": 2379
    },
    {
      "epoch": 1.2741633199464526,
      "grad_norm": 0.7998168468475342,
      "learning_rate": 0.00011509635974304069,
      "loss": 0.1479,
      "step": 2380
    },
    {
      "epoch": 1.274698795180723,
      "grad_norm": 0.40354403853416443,
      "learning_rate": 0.00011506067094932191,
      "loss": 0.0833,
      "step": 2381
    },
    {
      "epoch": 1.2752342704149933,
      "grad_norm": 1.3974727392196655,
      "learning_rate": 0.00011502498215560316,
      "loss": 0.0614,
      "step": 2382
    },
    {
      "epoch": 1.2757697456492636,
      "grad_norm": 0.5188230276107788,
      "learning_rate": 0.00011498929336188438,
      "loss": 0.0942,
      "step": 2383
    },
    {
      "epoch": 1.2763052208835342,
      "grad_norm": 0.41423290967941284,
      "learning_rate": 0.0001149536045681656,
      "loss": 0.0658,
      "step": 2384
    },
    {
      "epoch": 1.2768406961178045,
      "grad_norm": 0.76816326379776,
      "learning_rate": 0.00011491791577444683,
      "loss": 0.1294,
      "step": 2385
    },
    {
      "epoch": 1.277376171352075,
      "grad_norm": 0.521821916103363,
      "learning_rate": 0.00011488222698072805,
      "loss": 0.0836,
      "step": 2386
    },
    {
      "epoch": 1.2779116465863454,
      "grad_norm": 1.0871795415878296,
      "learning_rate": 0.00011484653818700927,
      "loss": 0.1559,
      "step": 2387
    },
    {
      "epoch": 1.2784471218206157,
      "grad_norm": 0.2225906252861023,
      "learning_rate": 0.00011481084939329052,
      "loss": 0.0133,
      "step": 2388
    },
    {
      "epoch": 1.2789825970548863,
      "grad_norm": 3.3364057540893555,
      "learning_rate": 0.00011477516059957174,
      "loss": 0.125,
      "step": 2389
    },
    {
      "epoch": 1.2795180722891566,
      "grad_norm": 0.9818789958953857,
      "learning_rate": 0.00011473947180585296,
      "loss": 0.0813,
      "step": 2390
    },
    {
      "epoch": 1.2800535475234271,
      "grad_norm": 0.7450295686721802,
      "learning_rate": 0.0001147037830121342,
      "loss": 0.1426,
      "step": 2391
    },
    {
      "epoch": 1.2805890227576975,
      "grad_norm": 1.0301722288131714,
      "learning_rate": 0.00011466809421841543,
      "loss": 0.1984,
      "step": 2392
    },
    {
      "epoch": 1.2811244979919678,
      "grad_norm": 0.47492751479148865,
      "learning_rate": 0.00011463240542469665,
      "loss": 0.0921,
      "step": 2393
    },
    {
      "epoch": 1.2816599732262384,
      "grad_norm": 0.527910590171814,
      "learning_rate": 0.00011459671663097788,
      "loss": 0.0765,
      "step": 2394
    },
    {
      "epoch": 1.2821954484605087,
      "grad_norm": 0.3720874488353729,
      "learning_rate": 0.0001145610278372591,
      "loss": 0.0702,
      "step": 2395
    },
    {
      "epoch": 1.282730923694779,
      "grad_norm": 0.5813242793083191,
      "learning_rate": 0.00011452533904354032,
      "loss": 0.0885,
      "step": 2396
    },
    {
      "epoch": 1.2832663989290496,
      "grad_norm": 0.35401248931884766,
      "learning_rate": 0.00011448965024982157,
      "loss": 0.0296,
      "step": 2397
    },
    {
      "epoch": 1.28380187416332,
      "grad_norm": 0.6113190054893494,
      "learning_rate": 0.00011445396145610279,
      "loss": 0.0985,
      "step": 2398
    },
    {
      "epoch": 1.2843373493975903,
      "grad_norm": 0.5489823222160339,
      "learning_rate": 0.000114418272662384,
      "loss": 0.0582,
      "step": 2399
    },
    {
      "epoch": 1.2848728246318608,
      "grad_norm": 0.38714927434921265,
      "learning_rate": 0.00011438258386866525,
      "loss": 0.0544,
      "step": 2400
    },
    {
      "epoch": 1.2854082998661311,
      "grad_norm": 0.774834930896759,
      "learning_rate": 0.00011434689507494647,
      "loss": 0.1302,
      "step": 2401
    },
    {
      "epoch": 1.2859437751004017,
      "grad_norm": 4.429082870483398,
      "learning_rate": 0.00011431120628122769,
      "loss": 0.0413,
      "step": 2402
    },
    {
      "epoch": 1.286479250334672,
      "grad_norm": 0.39308711886405945,
      "learning_rate": 0.00011427551748750893,
      "loss": 0.0511,
      "step": 2403
    },
    {
      "epoch": 1.2870147255689424,
      "grad_norm": 0.507168173789978,
      "learning_rate": 0.00011423982869379015,
      "loss": 0.0722,
      "step": 2404
    },
    {
      "epoch": 1.287550200803213,
      "grad_norm": 0.5711986422538757,
      "learning_rate": 0.00011420413990007139,
      "loss": 0.0694,
      "step": 2405
    },
    {
      "epoch": 1.2880856760374833,
      "grad_norm": 1.9200150966644287,
      "learning_rate": 0.00011416845110635261,
      "loss": 0.0851,
      "step": 2406
    },
    {
      "epoch": 1.2886211512717538,
      "grad_norm": 0.5814935564994812,
      "learning_rate": 0.00011413276231263383,
      "loss": 0.0843,
      "step": 2407
    },
    {
      "epoch": 1.2891566265060241,
      "grad_norm": 0.6368359923362732,
      "learning_rate": 0.00011409707351891508,
      "loss": 0.1193,
      "step": 2408
    },
    {
      "epoch": 1.2896921017402945,
      "grad_norm": 0.323034405708313,
      "learning_rate": 0.0001140613847251963,
      "loss": 0.052,
      "step": 2409
    },
    {
      "epoch": 1.2902275769745648,
      "grad_norm": 0.395387202501297,
      "learning_rate": 0.00011402569593147752,
      "loss": 0.065,
      "step": 2410
    },
    {
      "epoch": 1.2907630522088354,
      "grad_norm": 0.6862974762916565,
      "learning_rate": 0.00011399000713775875,
      "loss": 0.1124,
      "step": 2411
    },
    {
      "epoch": 1.2912985274431057,
      "grad_norm": 0.867723822593689,
      "learning_rate": 0.00011395431834403997,
      "loss": 0.1122,
      "step": 2412
    },
    {
      "epoch": 1.2918340026773762,
      "grad_norm": 0.47741881012916565,
      "learning_rate": 0.00011391862955032119,
      "loss": 0.0686,
      "step": 2413
    },
    {
      "epoch": 1.2923694779116466,
      "grad_norm": 0.5730419754981995,
      "learning_rate": 0.00011388294075660244,
      "loss": 0.0882,
      "step": 2414
    },
    {
      "epoch": 1.292904953145917,
      "grad_norm": 0.5705853700637817,
      "learning_rate": 0.00011384725196288366,
      "loss": 0.1057,
      "step": 2415
    },
    {
      "epoch": 1.2934404283801875,
      "grad_norm": 1.3611074686050415,
      "learning_rate": 0.00011381156316916488,
      "loss": 0.1673,
      "step": 2416
    },
    {
      "epoch": 1.2939759036144578,
      "grad_norm": 0.6354864239692688,
      "learning_rate": 0.00011377587437544613,
      "loss": 0.0719,
      "step": 2417
    },
    {
      "epoch": 1.2945113788487284,
      "grad_norm": 0.8189004063606262,
      "learning_rate": 0.00011374018558172735,
      "loss": 0.126,
      "step": 2418
    },
    {
      "epoch": 1.2950468540829987,
      "grad_norm": 0.41990119218826294,
      "learning_rate": 0.00011370449678800857,
      "loss": 0.0244,
      "step": 2419
    },
    {
      "epoch": 1.295582329317269,
      "grad_norm": 0.6216450333595276,
      "learning_rate": 0.0001136688079942898,
      "loss": 0.0905,
      "step": 2420
    },
    {
      "epoch": 1.2961178045515394,
      "grad_norm": 0.8715250492095947,
      "learning_rate": 0.00011363311920057103,
      "loss": 0.0927,
      "step": 2421
    },
    {
      "epoch": 1.29665327978581,
      "grad_norm": 0.6027854084968567,
      "learning_rate": 0.00011359743040685225,
      "loss": 0.0906,
      "step": 2422
    },
    {
      "epoch": 1.2971887550200802,
      "grad_norm": 0.8574787974357605,
      "learning_rate": 0.00011356174161313349,
      "loss": 0.1245,
      "step": 2423
    },
    {
      "epoch": 1.2977242302543508,
      "grad_norm": 0.48260918259620667,
      "learning_rate": 0.0001135260528194147,
      "loss": 0.0643,
      "step": 2424
    },
    {
      "epoch": 1.2982597054886211,
      "grad_norm": 0.5185198783874512,
      "learning_rate": 0.00011349036402569593,
      "loss": 0.0757,
      "step": 2425
    },
    {
      "epoch": 1.2987951807228915,
      "grad_norm": 0.5938003659248352,
      "learning_rate": 0.00011345467523197717,
      "loss": 0.0733,
      "step": 2426
    },
    {
      "epoch": 1.299330655957162,
      "grad_norm": 1.477653980255127,
      "learning_rate": 0.00011341898643825839,
      "loss": 0.0797,
      "step": 2427
    },
    {
      "epoch": 1.2998661311914324,
      "grad_norm": 0.9068747758865356,
      "learning_rate": 0.00011338329764453961,
      "loss": 0.1729,
      "step": 2428
    },
    {
      "epoch": 1.300401606425703,
      "grad_norm": 0.6031740307807922,
      "learning_rate": 0.00011334760885082086,
      "loss": 0.0819,
      "step": 2429
    },
    {
      "epoch": 1.3009370816599732,
      "grad_norm": 0.47393599152565,
      "learning_rate": 0.00011331192005710208,
      "loss": 0.0555,
      "step": 2430
    },
    {
      "epoch": 1.3014725568942436,
      "grad_norm": 0.6826847195625305,
      "learning_rate": 0.0001132762312633833,
      "loss": 0.0744,
      "step": 2431
    },
    {
      "epoch": 1.3020080321285141,
      "grad_norm": 0.40259888768196106,
      "learning_rate": 0.00011324054246966453,
      "loss": 0.0554,
      "step": 2432
    },
    {
      "epoch": 1.3025435073627845,
      "grad_norm": 4.604338645935059,
      "learning_rate": 0.00011320485367594575,
      "loss": 0.0533,
      "step": 2433
    },
    {
      "epoch": 1.303078982597055,
      "grad_norm": 1.0695154666900635,
      "learning_rate": 0.000113169164882227,
      "loss": 0.1501,
      "step": 2434
    },
    {
      "epoch": 1.3036144578313253,
      "grad_norm": 0.5135989785194397,
      "learning_rate": 0.00011313347608850822,
      "loss": 0.077,
      "step": 2435
    },
    {
      "epoch": 1.3041499330655957,
      "grad_norm": 0.5122585892677307,
      "learning_rate": 0.00011309778729478944,
      "loss": 0.0879,
      "step": 2436
    },
    {
      "epoch": 1.304685408299866,
      "grad_norm": 0.5496468544006348,
      "learning_rate": 0.00011306209850107069,
      "loss": 0.0989,
      "step": 2437
    },
    {
      "epoch": 1.3052208835341366,
      "grad_norm": 1.425479769706726,
      "learning_rate": 0.0001130264097073519,
      "loss": 0.1574,
      "step": 2438
    },
    {
      "epoch": 1.305756358768407,
      "grad_norm": 0.8269385099411011,
      "learning_rate": 0.00011299072091363313,
      "loss": 0.0743,
      "step": 2439
    },
    {
      "epoch": 1.3062918340026775,
      "grad_norm": 0.6683432459831238,
      "learning_rate": 0.00011295503211991436,
      "loss": 0.1563,
      "step": 2440
    },
    {
      "epoch": 1.3068273092369478,
      "grad_norm": 0.2566215395927429,
      "learning_rate": 0.00011291934332619558,
      "loss": 0.0159,
      "step": 2441
    },
    {
      "epoch": 1.3073627844712181,
      "grad_norm": 0.47528231143951416,
      "learning_rate": 0.0001128836545324768,
      "loss": 0.0868,
      "step": 2442
    },
    {
      "epoch": 1.3078982597054887,
      "grad_norm": 3.32802152633667,
      "learning_rate": 0.00011284796573875805,
      "loss": 0.1687,
      "step": 2443
    },
    {
      "epoch": 1.308433734939759,
      "grad_norm": 0.4982925057411194,
      "learning_rate": 0.00011281227694503927,
      "loss": 0.0715,
      "step": 2444
    },
    {
      "epoch": 1.3089692101740296,
      "grad_norm": 0.5317457318305969,
      "learning_rate": 0.00011277658815132049,
      "loss": 0.0534,
      "step": 2445
    },
    {
      "epoch": 1.3095046854083,
      "grad_norm": 0.7577972412109375,
      "learning_rate": 0.00011274089935760173,
      "loss": 0.1419,
      "step": 2446
    },
    {
      "epoch": 1.3100401606425702,
      "grad_norm": 0.6074895262718201,
      "learning_rate": 0.00011270521056388295,
      "loss": 0.1061,
      "step": 2447
    },
    {
      "epoch": 1.3105756358768406,
      "grad_norm": 1.2995316982269287,
      "learning_rate": 0.00011266952177016417,
      "loss": 0.08,
      "step": 2448
    },
    {
      "epoch": 1.3111111111111111,
      "grad_norm": 0.5650871992111206,
      "learning_rate": 0.0001126338329764454,
      "loss": 0.0731,
      "step": 2449
    },
    {
      "epoch": 1.3116465863453814,
      "grad_norm": 0.47048160433769226,
      "learning_rate": 0.00011259814418272663,
      "loss": 0.0825,
      "step": 2450
    },
    {
      "epoch": 1.312182061579652,
      "grad_norm": 0.27079281210899353,
      "learning_rate": 0.00011256245538900785,
      "loss": 0.0295,
      "step": 2451
    },
    {
      "epoch": 1.3127175368139223,
      "grad_norm": 0.5023869276046753,
      "learning_rate": 0.00011252676659528909,
      "loss": 0.102,
      "step": 2452
    },
    {
      "epoch": 1.3132530120481927,
      "grad_norm": 0.8220795392990112,
      "learning_rate": 0.00011249107780157031,
      "loss": 0.0973,
      "step": 2453
    },
    {
      "epoch": 1.3137884872824632,
      "grad_norm": 0.3050673007965088,
      "learning_rate": 0.00011245538900785153,
      "loss": 0.0479,
      "step": 2454
    },
    {
      "epoch": 1.3143239625167336,
      "grad_norm": 0.8258593678474426,
      "learning_rate": 0.00011241970021413278,
      "loss": 0.158,
      "step": 2455
    },
    {
      "epoch": 1.3148594377510041,
      "grad_norm": 0.7872294187545776,
      "learning_rate": 0.000112384011420414,
      "loss": 0.1557,
      "step": 2456
    },
    {
      "epoch": 1.3153949129852744,
      "grad_norm": 0.482533723115921,
      "learning_rate": 0.00011234832262669522,
      "loss": 0.13,
      "step": 2457
    },
    {
      "epoch": 1.3159303882195448,
      "grad_norm": 0.5908344984054565,
      "learning_rate": 0.00011231263383297645,
      "loss": 0.0805,
      "step": 2458
    },
    {
      "epoch": 1.3164658634538153,
      "grad_norm": 0.5789834260940552,
      "learning_rate": 0.00011227694503925767,
      "loss": 0.091,
      "step": 2459
    },
    {
      "epoch": 1.3170013386880857,
      "grad_norm": 0.711105227470398,
      "learning_rate": 0.00011224125624553892,
      "loss": 0.1189,
      "step": 2460
    },
    {
      "epoch": 1.3175368139223562,
      "grad_norm": 0.5490458607673645,
      "learning_rate": 0.00011220556745182014,
      "loss": 0.0665,
      "step": 2461
    },
    {
      "epoch": 1.3180722891566266,
      "grad_norm": 0.5875626802444458,
      "learning_rate": 0.00011216987865810136,
      "loss": 0.0821,
      "step": 2462
    },
    {
      "epoch": 1.3186077643908969,
      "grad_norm": 0.8853978514671326,
      "learning_rate": 0.0001121341898643826,
      "loss": 0.1063,
      "step": 2463
    },
    {
      "epoch": 1.3191432396251672,
      "grad_norm": 6.836893081665039,
      "learning_rate": 0.00011209850107066383,
      "loss": 0.128,
      "step": 2464
    },
    {
      "epoch": 1.3196787148594378,
      "grad_norm": 0.5788316130638123,
      "learning_rate": 0.00011206281227694505,
      "loss": 0.0828,
      "step": 2465
    },
    {
      "epoch": 1.320214190093708,
      "grad_norm": 0.7436196804046631,
      "learning_rate": 0.00011202712348322628,
      "loss": 0.1135,
      "step": 2466
    },
    {
      "epoch": 1.3207496653279787,
      "grad_norm": 0.6544486880302429,
      "learning_rate": 0.0001119914346895075,
      "loss": 0.1087,
      "step": 2467
    },
    {
      "epoch": 1.321285140562249,
      "grad_norm": 0.3371763229370117,
      "learning_rate": 0.00011195574589578872,
      "loss": 0.0445,
      "step": 2468
    },
    {
      "epoch": 1.3218206157965193,
      "grad_norm": 1.1357614994049072,
      "learning_rate": 0.00011192005710206997,
      "loss": 0.0372,
      "step": 2469
    },
    {
      "epoch": 1.3223560910307899,
      "grad_norm": 0.35572317242622375,
      "learning_rate": 0.00011188436830835119,
      "loss": 0.0409,
      "step": 2470
    },
    {
      "epoch": 1.3228915662650602,
      "grad_norm": 0.9393980503082275,
      "learning_rate": 0.0001118486795146324,
      "loss": 0.1023,
      "step": 2471
    },
    {
      "epoch": 1.3234270414993308,
      "grad_norm": 0.7293535470962524,
      "learning_rate": 0.00011181299072091365,
      "loss": 0.1156,
      "step": 2472
    },
    {
      "epoch": 1.323962516733601,
      "grad_norm": 0.8612937927246094,
      "learning_rate": 0.00011177730192719487,
      "loss": 0.0636,
      "step": 2473
    },
    {
      "epoch": 1.3244979919678714,
      "grad_norm": 0.40509530901908875,
      "learning_rate": 0.00011174161313347609,
      "loss": 0.0298,
      "step": 2474
    },
    {
      "epoch": 1.3250334672021418,
      "grad_norm": 1.0852800607681274,
      "learning_rate": 0.00011170592433975733,
      "loss": 0.1292,
      "step": 2475
    },
    {
      "epoch": 1.3255689424364123,
      "grad_norm": 0.37170490622520447,
      "learning_rate": 0.00011167023554603855,
      "loss": 0.0502,
      "step": 2476
    },
    {
      "epoch": 1.3261044176706827,
      "grad_norm": 0.6805973052978516,
      "learning_rate": 0.00011163454675231977,
      "loss": 0.047,
      "step": 2477
    },
    {
      "epoch": 1.3266398929049532,
      "grad_norm": 1.5411885976791382,
      "learning_rate": 0.00011159885795860101,
      "loss": 0.1424,
      "step": 2478
    },
    {
      "epoch": 1.3271753681392235,
      "grad_norm": 0.520133376121521,
      "learning_rate": 0.00011156316916488223,
      "loss": 0.0905,
      "step": 2479
    },
    {
      "epoch": 1.3277108433734939,
      "grad_norm": 4.236061096191406,
      "learning_rate": 0.00011152748037116345,
      "loss": 0.205,
      "step": 2480
    },
    {
      "epoch": 1.3282463186077644,
      "grad_norm": 0.5779414176940918,
      "learning_rate": 0.0001114917915774447,
      "loss": 0.1127,
      "step": 2481
    },
    {
      "epoch": 1.3287817938420348,
      "grad_norm": 0.6323580145835876,
      "learning_rate": 0.00011145610278372592,
      "loss": 0.1281,
      "step": 2482
    },
    {
      "epoch": 1.3293172690763053,
      "grad_norm": 0.7955129742622375,
      "learning_rate": 0.00011142041399000714,
      "loss": 0.1272,
      "step": 2483
    },
    {
      "epoch": 1.3298527443105757,
      "grad_norm": 0.6940439343452454,
      "learning_rate": 0.00011138472519628837,
      "loss": 0.0785,
      "step": 2484
    },
    {
      "epoch": 1.330388219544846,
      "grad_norm": 0.5880032777786255,
      "learning_rate": 0.00011134903640256959,
      "loss": 0.0879,
      "step": 2485
    },
    {
      "epoch": 1.3309236947791165,
      "grad_norm": 0.483894944190979,
      "learning_rate": 0.00011131334760885081,
      "loss": 0.0444,
      "step": 2486
    },
    {
      "epoch": 1.3314591700133869,
      "grad_norm": 0.6929129362106323,
      "learning_rate": 0.00011127765881513206,
      "loss": 0.0954,
      "step": 2487
    },
    {
      "epoch": 1.3319946452476574,
      "grad_norm": 0.7732737064361572,
      "learning_rate": 0.00011124197002141328,
      "loss": 0.1083,
      "step": 2488
    },
    {
      "epoch": 1.3325301204819278,
      "grad_norm": 0.9299578666687012,
      "learning_rate": 0.00011120628122769453,
      "loss": 0.1331,
      "step": 2489
    },
    {
      "epoch": 1.333065595716198,
      "grad_norm": 0.5998607277870178,
      "learning_rate": 0.00011117059243397575,
      "loss": 0.0778,
      "step": 2490
    },
    {
      "epoch": 1.3336010709504684,
      "grad_norm": 1.0692278146743774,
      "learning_rate": 0.00011113490364025697,
      "loss": 0.1622,
      "step": 2491
    },
    {
      "epoch": 1.334136546184739,
      "grad_norm": 0.6259150505065918,
      "learning_rate": 0.0001110992148465382,
      "loss": 0.0766,
      "step": 2492
    },
    {
      "epoch": 1.3346720214190093,
      "grad_norm": 0.6071008443832397,
      "learning_rate": 0.00011106352605281942,
      "loss": 0.1098,
      "step": 2493
    },
    {
      "epoch": 1.3352074966532799,
      "grad_norm": 1.4061695337295532,
      "learning_rate": 0.00011102783725910064,
      "loss": 0.2092,
      "step": 2494
    },
    {
      "epoch": 1.3357429718875502,
      "grad_norm": 0.5504341125488281,
      "learning_rate": 0.00011099214846538189,
      "loss": 0.0618,
      "step": 2495
    },
    {
      "epoch": 1.3362784471218205,
      "grad_norm": 4.546267032623291,
      "learning_rate": 0.0001109564596716631,
      "loss": 0.1359,
      "step": 2496
    },
    {
      "epoch": 1.336813922356091,
      "grad_norm": 0.3520589768886566,
      "learning_rate": 0.00011092077087794433,
      "loss": 0.1085,
      "step": 2497
    },
    {
      "epoch": 1.3373493975903614,
      "grad_norm": 0.3568224012851715,
      "learning_rate": 0.00011088508208422557,
      "loss": 0.0483,
      "step": 2498
    },
    {
      "epoch": 1.337884872824632,
      "grad_norm": 1.0637164115905762,
      "learning_rate": 0.00011084939329050679,
      "loss": 0.2373,
      "step": 2499
    },
    {
      "epoch": 1.3384203480589023,
      "grad_norm": 1.5104707479476929,
      "learning_rate": 0.00011081370449678801,
      "loss": 0.1337,
      "step": 2500
    },
    {
      "epoch": 1.3389558232931726,
      "grad_norm": 0.7994252443313599,
      "learning_rate": 0.00011077801570306925,
      "loss": 0.1131,
      "step": 2501
    },
    {
      "epoch": 1.339491298527443,
      "grad_norm": 0.48908287286758423,
      "learning_rate": 0.00011074232690935047,
      "loss": 0.0592,
      "step": 2502
    },
    {
      "epoch": 1.3400267737617135,
      "grad_norm": 0.38036778569221497,
      "learning_rate": 0.00011070663811563169,
      "loss": 0.0762,
      "step": 2503
    },
    {
      "epoch": 1.3405622489959839,
      "grad_norm": 1.9771753549575806,
      "learning_rate": 0.00011067094932191293,
      "loss": 0.1304,
      "step": 2504
    },
    {
      "epoch": 1.3410977242302544,
      "grad_norm": 0.5992327928543091,
      "learning_rate": 0.00011063526052819415,
      "loss": 0.1043,
      "step": 2505
    },
    {
      "epoch": 1.3416331994645248,
      "grad_norm": 0.5206900835037231,
      "learning_rate": 0.00011059957173447537,
      "loss": 0.0543,
      "step": 2506
    },
    {
      "epoch": 1.342168674698795,
      "grad_norm": 0.6778508424758911,
      "learning_rate": 0.00011056388294075662,
      "loss": 0.1227,
      "step": 2507
    },
    {
      "epoch": 1.3427041499330656,
      "grad_norm": 0.6130694150924683,
      "learning_rate": 0.00011052819414703784,
      "loss": 0.0878,
      "step": 2508
    },
    {
      "epoch": 1.343239625167336,
      "grad_norm": 0.4206755459308624,
      "learning_rate": 0.00011049250535331906,
      "loss": 0.0772,
      "step": 2509
    },
    {
      "epoch": 1.3437751004016065,
      "grad_norm": 0.8331471681594849,
      "learning_rate": 0.00011045681655960029,
      "loss": 0.1316,
      "step": 2510
    },
    {
      "epoch": 1.3443105756358769,
      "grad_norm": 0.47520506381988525,
      "learning_rate": 0.00011042112776588151,
      "loss": 0.1108,
      "step": 2511
    },
    {
      "epoch": 1.3448460508701472,
      "grad_norm": 0.6504639983177185,
      "learning_rate": 0.00011038543897216273,
      "loss": 0.0878,
      "step": 2512
    },
    {
      "epoch": 1.3453815261044177,
      "grad_norm": 0.31939229369163513,
      "learning_rate": 0.00011034975017844398,
      "loss": 0.0633,
      "step": 2513
    },
    {
      "epoch": 1.345917001338688,
      "grad_norm": 0.9602958559989929,
      "learning_rate": 0.0001103140613847252,
      "loss": 0.1687,
      "step": 2514
    },
    {
      "epoch": 1.3464524765729586,
      "grad_norm": 0.8863173127174377,
      "learning_rate": 0.00011027837259100645,
      "loss": 0.1504,
      "step": 2515
    },
    {
      "epoch": 1.346987951807229,
      "grad_norm": 0.8232526779174805,
      "learning_rate": 0.00011024268379728767,
      "loss": 0.1747,
      "step": 2516
    },
    {
      "epoch": 1.3475234270414993,
      "grad_norm": 0.6529924869537354,
      "learning_rate": 0.00011020699500356889,
      "loss": 0.1372,
      "step": 2517
    },
    {
      "epoch": 1.3480589022757696,
      "grad_norm": 0.8188884258270264,
      "learning_rate": 0.00011017130620985012,
      "loss": 0.171,
      "step": 2518
    },
    {
      "epoch": 1.3485943775100402,
      "grad_norm": 0.3779015839099884,
      "learning_rate": 0.00011013561741613134,
      "loss": 0.062,
      "step": 2519
    },
    {
      "epoch": 1.3491298527443105,
      "grad_norm": 0.5816189646720886,
      "learning_rate": 0.00011009992862241256,
      "loss": 0.1181,
      "step": 2520
    },
    {
      "epoch": 1.349665327978581,
      "grad_norm": 0.45725271105766296,
      "learning_rate": 0.0001100642398286938,
      "loss": 0.0659,
      "step": 2521
    },
    {
      "epoch": 1.3502008032128514,
      "grad_norm": 0.4764506220817566,
      "learning_rate": 0.00011002855103497503,
      "loss": 0.1174,
      "step": 2522
    },
    {
      "epoch": 1.3507362784471217,
      "grad_norm": 0.24054977297782898,
      "learning_rate": 0.00010999286224125625,
      "loss": 0.0206,
      "step": 2523
    },
    {
      "epoch": 1.3512717536813923,
      "grad_norm": 0.6206253170967102,
      "learning_rate": 0.00010995717344753749,
      "loss": 0.0476,
      "step": 2524
    },
    {
      "epoch": 1.3518072289156626,
      "grad_norm": 0.5975182056427002,
      "learning_rate": 0.00010992148465381871,
      "loss": 0.0636,
      "step": 2525
    },
    {
      "epoch": 1.3523427041499332,
      "grad_norm": 0.6280166506767273,
      "learning_rate": 0.00010988579586009993,
      "loss": 0.0486,
      "step": 2526
    },
    {
      "epoch": 1.3528781793842035,
      "grad_norm": 6.1705145835876465,
      "learning_rate": 0.00010985010706638117,
      "loss": 0.1894,
      "step": 2527
    },
    {
      "epoch": 1.3534136546184738,
      "grad_norm": 0.3578217327594757,
      "learning_rate": 0.00010981441827266239,
      "loss": 0.0538,
      "step": 2528
    },
    {
      "epoch": 1.3539491298527442,
      "grad_norm": 0.7610140442848206,
      "learning_rate": 0.0001097787294789436,
      "loss": 0.0709,
      "step": 2529
    },
    {
      "epoch": 1.3544846050870147,
      "grad_norm": 0.5611214637756348,
      "learning_rate": 0.00010974304068522485,
      "loss": 0.0646,
      "step": 2530
    },
    {
      "epoch": 1.355020080321285,
      "grad_norm": 0.4931831955909729,
      "learning_rate": 0.00010970735189150607,
      "loss": 0.0835,
      "step": 2531
    },
    {
      "epoch": 1.3555555555555556,
      "grad_norm": 0.5415558815002441,
      "learning_rate": 0.00010967166309778729,
      "loss": 0.106,
      "step": 2532
    },
    {
      "epoch": 1.356091030789826,
      "grad_norm": 0.6769899129867554,
      "learning_rate": 0.00010963597430406854,
      "loss": 0.0917,
      "step": 2533
    },
    {
      "epoch": 1.3566265060240963,
      "grad_norm": 0.778826892375946,
      "learning_rate": 0.00010960028551034976,
      "loss": 0.0811,
      "step": 2534
    },
    {
      "epoch": 1.3571619812583668,
      "grad_norm": 1.3479512929916382,
      "learning_rate": 0.00010956459671663098,
      "loss": 0.1842,
      "step": 2535
    },
    {
      "epoch": 1.3576974564926372,
      "grad_norm": 0.8438314199447632,
      "learning_rate": 0.00010952890792291221,
      "loss": 0.1131,
      "step": 2536
    },
    {
      "epoch": 1.3582329317269077,
      "grad_norm": 0.06956448405981064,
      "learning_rate": 0.00010949321912919343,
      "loss": 0.004,
      "step": 2537
    },
    {
      "epoch": 1.358768406961178,
      "grad_norm": 2.7950308322906494,
      "learning_rate": 0.00010945753033547465,
      "loss": 0.1261,
      "step": 2538
    },
    {
      "epoch": 1.3593038821954484,
      "grad_norm": 0.5174477100372314,
      "learning_rate": 0.0001094218415417559,
      "loss": 0.0596,
      "step": 2539
    },
    {
      "epoch": 1.359839357429719,
      "grad_norm": 0.5123087763786316,
      "learning_rate": 0.00010938615274803712,
      "loss": 0.1333,
      "step": 2540
    },
    {
      "epoch": 1.3603748326639893,
      "grad_norm": 0.3742491900920868,
      "learning_rate": 0.00010935046395431834,
      "loss": 0.0395,
      "step": 2541
    },
    {
      "epoch": 1.3609103078982598,
      "grad_norm": 0.6641174554824829,
      "learning_rate": 0.00010931477516059959,
      "loss": 0.0889,
      "step": 2542
    },
    {
      "epoch": 1.3614457831325302,
      "grad_norm": 0.5926246643066406,
      "learning_rate": 0.0001092790863668808,
      "loss": 0.0561,
      "step": 2543
    },
    {
      "epoch": 1.3619812583668005,
      "grad_norm": 0.6575318574905396,
      "learning_rate": 0.00010924339757316204,
      "loss": 0.0967,
      "step": 2544
    },
    {
      "epoch": 1.3625167336010708,
      "grad_norm": 0.4007470905780792,
      "learning_rate": 0.00010920770877944326,
      "loss": 0.0379,
      "step": 2545
    },
    {
      "epoch": 1.3630522088353414,
      "grad_norm": 0.7095683813095093,
      "learning_rate": 0.00010917201998572448,
      "loss": 0.1575,
      "step": 2546
    },
    {
      "epoch": 1.3635876840696117,
      "grad_norm": 0.46115145087242126,
      "learning_rate": 0.00010913633119200573,
      "loss": 0.0623,
      "step": 2547
    },
    {
      "epoch": 1.3641231593038823,
      "grad_norm": 4.581390857696533,
      "learning_rate": 0.00010910064239828695,
      "loss": 0.1264,
      "step": 2548
    },
    {
      "epoch": 1.3646586345381526,
      "grad_norm": 0.5357968807220459,
      "learning_rate": 0.00010906495360456817,
      "loss": 0.0847,
      "step": 2549
    },
    {
      "epoch": 1.365194109772423,
      "grad_norm": 0.7251750230789185,
      "learning_rate": 0.00010902926481084941,
      "loss": 0.1097,
      "step": 2550
    },
    {
      "epoch": 1.3657295850066935,
      "grad_norm": 0.482973575592041,
      "learning_rate": 0.00010899357601713063,
      "loss": 0.0409,
      "step": 2551
    },
    {
      "epoch": 1.3662650602409638,
      "grad_norm": 1.02064847946167,
      "learning_rate": 0.00010895788722341185,
      "loss": 0.1473,
      "step": 2552
    },
    {
      "epoch": 1.3668005354752344,
      "grad_norm": 0.9408085346221924,
      "learning_rate": 0.00010892219842969309,
      "loss": 0.1361,
      "step": 2553
    },
    {
      "epoch": 1.3673360107095047,
      "grad_norm": 2.240839958190918,
      "learning_rate": 0.0001088865096359743,
      "loss": 0.2652,
      "step": 2554
    },
    {
      "epoch": 1.367871485943775,
      "grad_norm": 3.1832175254821777,
      "learning_rate": 0.00010885082084225553,
      "loss": 0.1215,
      "step": 2555
    },
    {
      "epoch": 1.3684069611780454,
      "grad_norm": 0.4018554985523224,
      "learning_rate": 0.00010881513204853677,
      "loss": 0.0668,
      "step": 2556
    },
    {
      "epoch": 1.368942436412316,
      "grad_norm": 1.4280816316604614,
      "learning_rate": 0.00010877944325481799,
      "loss": 0.1208,
      "step": 2557
    },
    {
      "epoch": 1.3694779116465863,
      "grad_norm": 0.6207471489906311,
      "learning_rate": 0.00010874375446109921,
      "loss": 0.0628,
      "step": 2558
    },
    {
      "epoch": 1.3700133868808568,
      "grad_norm": 0.778910756111145,
      "learning_rate": 0.00010870806566738046,
      "loss": 0.1019,
      "step": 2559
    },
    {
      "epoch": 1.3705488621151272,
      "grad_norm": 0.458151638507843,
      "learning_rate": 0.00010867237687366168,
      "loss": 0.1155,
      "step": 2560
    },
    {
      "epoch": 1.3710843373493975,
      "grad_norm": 0.7934529185295105,
      "learning_rate": 0.0001086366880799429,
      "loss": 0.094,
      "step": 2561
    },
    {
      "epoch": 1.371619812583668,
      "grad_norm": 0.403146892786026,
      "learning_rate": 0.00010860099928622413,
      "loss": 0.0402,
      "step": 2562
    },
    {
      "epoch": 1.3721552878179384,
      "grad_norm": 0.5777016878128052,
      "learning_rate": 0.00010856531049250535,
      "loss": 0.0827,
      "step": 2563
    },
    {
      "epoch": 1.372690763052209,
      "grad_norm": 0.6476079821586609,
      "learning_rate": 0.00010852962169878657,
      "loss": 0.1456,
      "step": 2564
    },
    {
      "epoch": 1.3732262382864793,
      "grad_norm": 0.8932437300682068,
      "learning_rate": 0.00010849393290506782,
      "loss": 0.0853,
      "step": 2565
    },
    {
      "epoch": 1.3737617135207496,
      "grad_norm": 0.3904031813144684,
      "learning_rate": 0.00010845824411134904,
      "loss": 0.0478,
      "step": 2566
    },
    {
      "epoch": 1.3742971887550202,
      "grad_norm": 0.5901893973350525,
      "learning_rate": 0.00010842255531763026,
      "loss": 0.1051,
      "step": 2567
    },
    {
      "epoch": 1.3748326639892905,
      "grad_norm": 0.4176467955112457,
      "learning_rate": 0.0001083868665239115,
      "loss": 0.0582,
      "step": 2568
    },
    {
      "epoch": 1.375368139223561,
      "grad_norm": 0.7199776768684387,
      "learning_rate": 0.00010835117773019273,
      "loss": 0.1067,
      "step": 2569
    },
    {
      "epoch": 1.3759036144578314,
      "grad_norm": 4.448449611663818,
      "learning_rate": 0.00010831548893647396,
      "loss": 0.0694,
      "step": 2570
    },
    {
      "epoch": 1.3764390896921017,
      "grad_norm": 0.35628315806388855,
      "learning_rate": 0.00010827980014275518,
      "loss": 0.0456,
      "step": 2571
    },
    {
      "epoch": 1.376974564926372,
      "grad_norm": 0.805203378200531,
      "learning_rate": 0.0001082441113490364,
      "loss": 0.1126,
      "step": 2572
    },
    {
      "epoch": 1.3775100401606426,
      "grad_norm": 0.5119379162788391,
      "learning_rate": 0.00010820842255531765,
      "loss": 0.0955,
      "step": 2573
    },
    {
      "epoch": 1.378045515394913,
      "grad_norm": 0.43800464272499084,
      "learning_rate": 0.00010817273376159887,
      "loss": 0.0894,
      "step": 2574
    },
    {
      "epoch": 1.3785809906291835,
      "grad_norm": 1.0495600700378418,
      "learning_rate": 0.00010813704496788009,
      "loss": 0.181,
      "step": 2575
    },
    {
      "epoch": 1.3791164658634538,
      "grad_norm": 5.043267250061035,
      "learning_rate": 0.00010810135617416133,
      "loss": 0.1566,
      "step": 2576
    },
    {
      "epoch": 1.3796519410977242,
      "grad_norm": 0.4195019006729126,
      "learning_rate": 0.00010806566738044255,
      "loss": 0.0262,
      "step": 2577
    },
    {
      "epoch": 1.3801874163319947,
      "grad_norm": 0.4589062035083771,
      "learning_rate": 0.00010802997858672377,
      "loss": 0.0541,
      "step": 2578
    },
    {
      "epoch": 1.380722891566265,
      "grad_norm": 0.7661675214767456,
      "learning_rate": 0.000107994289793005,
      "loss": 0.1233,
      "step": 2579
    },
    {
      "epoch": 1.3812583668005356,
      "grad_norm": 0.4291232228279114,
      "learning_rate": 0.00010795860099928623,
      "loss": 0.0542,
      "step": 2580
    },
    {
      "epoch": 1.381793842034806,
      "grad_norm": 0.7701641321182251,
      "learning_rate": 0.00010792291220556744,
      "loss": 0.1693,
      "step": 2581
    },
    {
      "epoch": 1.3823293172690763,
      "grad_norm": 0.8246810436248779,
      "learning_rate": 0.00010788722341184869,
      "loss": 0.1159,
      "step": 2582
    },
    {
      "epoch": 1.3828647925033466,
      "grad_norm": 0.716820240020752,
      "learning_rate": 0.00010785153461812991,
      "loss": 0.1322,
      "step": 2583
    },
    {
      "epoch": 1.3834002677376172,
      "grad_norm": 2.2381680011749268,
      "learning_rate": 0.00010781584582441113,
      "loss": 0.1282,
      "step": 2584
    },
    {
      "epoch": 1.3839357429718875,
      "grad_norm": 0.35029950737953186,
      "learning_rate": 0.00010778015703069238,
      "loss": 0.0694,
      "step": 2585
    },
    {
      "epoch": 1.384471218206158,
      "grad_norm": 0.740481436252594,
      "learning_rate": 0.0001077444682369736,
      "loss": 0.0981,
      "step": 2586
    },
    {
      "epoch": 1.3850066934404284,
      "grad_norm": 2.7700822353363037,
      "learning_rate": 0.00010770877944325482,
      "loss": 0.0648,
      "step": 2587
    },
    {
      "epoch": 1.3855421686746987,
      "grad_norm": 0.572407066822052,
      "learning_rate": 0.00010767309064953605,
      "loss": 0.0678,
      "step": 2588
    },
    {
      "epoch": 1.3860776439089693,
      "grad_norm": 0.4272821843624115,
      "learning_rate": 0.00010763740185581727,
      "loss": 0.0502,
      "step": 2589
    },
    {
      "epoch": 1.3866131191432396,
      "grad_norm": 0.4445403218269348,
      "learning_rate": 0.00010760171306209849,
      "loss": 0.0619,
      "step": 2590
    },
    {
      "epoch": 1.3871485943775101,
      "grad_norm": 0.45497214794158936,
      "learning_rate": 0.00010756602426837974,
      "loss": 0.0668,
      "step": 2591
    },
    {
      "epoch": 1.3876840696117805,
      "grad_norm": 0.3185330033302307,
      "learning_rate": 0.00010753033547466096,
      "loss": 0.0422,
      "step": 2592
    },
    {
      "epoch": 1.3882195448460508,
      "grad_norm": 0.41901248693466187,
      "learning_rate": 0.00010749464668094218,
      "loss": 0.0651,
      "step": 2593
    },
    {
      "epoch": 1.3887550200803214,
      "grad_norm": 0.630475640296936,
      "learning_rate": 0.00010745895788722343,
      "loss": 0.1075,
      "step": 2594
    },
    {
      "epoch": 1.3892904953145917,
      "grad_norm": 0.6211848855018616,
      "learning_rate": 0.00010742326909350465,
      "loss": 0.1299,
      "step": 2595
    },
    {
      "epoch": 1.389825970548862,
      "grad_norm": 0.7391391396522522,
      "learning_rate": 0.00010738758029978587,
      "loss": 0.1047,
      "step": 2596
    },
    {
      "epoch": 1.3903614457831326,
      "grad_norm": 0.6553267240524292,
      "learning_rate": 0.0001073518915060671,
      "loss": 0.0748,
      "step": 2597
    },
    {
      "epoch": 1.390896921017403,
      "grad_norm": 0.5985710620880127,
      "learning_rate": 0.00010731620271234832,
      "loss": 0.0963,
      "step": 2598
    },
    {
      "epoch": 1.3914323962516733,
      "grad_norm": 0.983257532119751,
      "learning_rate": 0.00010728051391862957,
      "loss": 0.1461,
      "step": 2599
    },
    {
      "epoch": 1.3919678714859438,
      "grad_norm": 0.7580745220184326,
      "learning_rate": 0.00010724482512491079,
      "loss": 0.1007,
      "step": 2600
    },
    {
      "epoch": 1.3925033467202141,
      "grad_norm": 0.35206320881843567,
      "learning_rate": 0.000107209136331192,
      "loss": 0.043,
      "step": 2601
    },
    {
      "epoch": 1.3930388219544847,
      "grad_norm": 0.6128973364830017,
      "learning_rate": 0.00010717344753747325,
      "loss": 0.0936,
      "step": 2602
    },
    {
      "epoch": 1.393574297188755,
      "grad_norm": 0.4760861396789551,
      "learning_rate": 0.00010713775874375447,
      "loss": 0.098,
      "step": 2603
    },
    {
      "epoch": 1.3941097724230254,
      "grad_norm": 3.403099298477173,
      "learning_rate": 0.00010710206995003569,
      "loss": 0.1672,
      "step": 2604
    },
    {
      "epoch": 1.394645247657296,
      "grad_norm": 0.4125214219093323,
      "learning_rate": 0.00010706638115631693,
      "loss": 0.0626,
      "step": 2605
    },
    {
      "epoch": 1.3951807228915662,
      "grad_norm": 0.5500143766403198,
      "learning_rate": 0.00010703069236259815,
      "loss": 0.1112,
      "step": 2606
    },
    {
      "epoch": 1.3957161981258368,
      "grad_norm": 0.7597690224647522,
      "learning_rate": 0.00010699500356887936,
      "loss": 0.1451,
      "step": 2607
    },
    {
      "epoch": 1.3962516733601071,
      "grad_norm": 0.673709511756897,
      "learning_rate": 0.00010695931477516061,
      "loss": 0.1411,
      "step": 2608
    },
    {
      "epoch": 1.3967871485943775,
      "grad_norm": 0.5514441728591919,
      "learning_rate": 0.00010692362598144183,
      "loss": 0.0834,
      "step": 2609
    },
    {
      "epoch": 1.3973226238286478,
      "grad_norm": 0.5447200536727905,
      "learning_rate": 0.00010688793718772305,
      "loss": 0.0956,
      "step": 2610
    },
    {
      "epoch": 1.3978580990629184,
      "grad_norm": 0.5486788749694824,
      "learning_rate": 0.0001068522483940043,
      "loss": 0.0719,
      "step": 2611
    },
    {
      "epoch": 1.3983935742971887,
      "grad_norm": 0.6995422840118408,
      "learning_rate": 0.00010681655960028552,
      "loss": 0.079,
      "step": 2612
    },
    {
      "epoch": 1.3989290495314592,
      "grad_norm": 0.5028332471847534,
      "learning_rate": 0.00010678087080656674,
      "loss": 0.1063,
      "step": 2613
    },
    {
      "epoch": 1.3994645247657296,
      "grad_norm": 0.5899252891540527,
      "learning_rate": 0.00010674518201284797,
      "loss": 0.0791,
      "step": 2614
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.26468342542648315,
      "learning_rate": 0.00010670949321912919,
      "loss": 0.0515,
      "step": 2615
    },
    {
      "epoch": 1.4005354752342705,
      "grad_norm": 0.5094183087348938,
      "learning_rate": 0.00010667380442541041,
      "loss": 0.0838,
      "step": 2616
    },
    {
      "epoch": 1.4010709504685408,
      "grad_norm": 0.29509568214416504,
      "learning_rate": 0.00010663811563169166,
      "loss": 0.0368,
      "step": 2617
    },
    {
      "epoch": 1.4016064257028114,
      "grad_norm": 0.8012913465499878,
      "learning_rate": 0.00010660242683797288,
      "loss": 0.1395,
      "step": 2618
    },
    {
      "epoch": 1.4021419009370817,
      "grad_norm": 0.5557563900947571,
      "learning_rate": 0.0001065667380442541,
      "loss": 0.1878,
      "step": 2619
    },
    {
      "epoch": 1.402677376171352,
      "grad_norm": 0.5414882898330688,
      "learning_rate": 0.00010653104925053535,
      "loss": 0.1116,
      "step": 2620
    },
    {
      "epoch": 1.4032128514056224,
      "grad_norm": 1.9271509647369385,
      "learning_rate": 0.00010649536045681657,
      "loss": 0.1056,
      "step": 2621
    },
    {
      "epoch": 1.403748326639893,
      "grad_norm": 7.233813762664795,
      "learning_rate": 0.00010645967166309778,
      "loss": 0.1445,
      "step": 2622
    },
    {
      "epoch": 1.4042838018741632,
      "grad_norm": 0.5855475664138794,
      "learning_rate": 0.00010642398286937902,
      "loss": 0.1376,
      "step": 2623
    },
    {
      "epoch": 1.4048192771084338,
      "grad_norm": 0.6037400960922241,
      "learning_rate": 0.00010638829407566024,
      "loss": 0.1695,
      "step": 2624
    },
    {
      "epoch": 1.4053547523427041,
      "grad_norm": 0.3747997581958771,
      "learning_rate": 0.00010635260528194149,
      "loss": 0.0874,
      "step": 2625
    },
    {
      "epoch": 1.4058902275769745,
      "grad_norm": 0.7184897065162659,
      "learning_rate": 0.0001063169164882227,
      "loss": 0.1925,
      "step": 2626
    },
    {
      "epoch": 1.406425702811245,
      "grad_norm": 0.5229958295822144,
      "learning_rate": 0.00010628122769450392,
      "loss": 0.0827,
      "step": 2627
    },
    {
      "epoch": 1.4069611780455153,
      "grad_norm": 0.32647329568862915,
      "learning_rate": 0.00010624553890078517,
      "loss": 0.0512,
      "step": 2628
    },
    {
      "epoch": 1.407496653279786,
      "grad_norm": 0.39074864983558655,
      "learning_rate": 0.00010620985010706639,
      "loss": 0.0545,
      "step": 2629
    },
    {
      "epoch": 1.4080321285140562,
      "grad_norm": 0.32196417450904846,
      "learning_rate": 0.00010617416131334761,
      "loss": 0.0299,
      "step": 2630
    },
    {
      "epoch": 1.4085676037483266,
      "grad_norm": 0.8284107446670532,
      "learning_rate": 0.00010613847251962885,
      "loss": 0.1323,
      "step": 2631
    },
    {
      "epoch": 1.4091030789825971,
      "grad_norm": 0.48368141055107117,
      "learning_rate": 0.00010610278372591007,
      "loss": 0.0854,
      "step": 2632
    },
    {
      "epoch": 1.4096385542168675,
      "grad_norm": 0.23823386430740356,
      "learning_rate": 0.00010606709493219128,
      "loss": 0.0324,
      "step": 2633
    },
    {
      "epoch": 1.410174029451138,
      "grad_norm": 0.6861953139305115,
      "learning_rate": 0.00010603140613847253,
      "loss": 0.113,
      "step": 2634
    },
    {
      "epoch": 1.4107095046854083,
      "grad_norm": 0.6526820063591003,
      "learning_rate": 0.00010599571734475375,
      "loss": 0.0807,
      "step": 2635
    },
    {
      "epoch": 1.4112449799196787,
      "grad_norm": 0.5918386578559875,
      "learning_rate": 0.00010596002855103497,
      "loss": 0.1137,
      "step": 2636
    },
    {
      "epoch": 1.411780455153949,
      "grad_norm": 0.5523427128791809,
      "learning_rate": 0.00010592433975731622,
      "loss": 0.107,
      "step": 2637
    },
    {
      "epoch": 1.4123159303882196,
      "grad_norm": 0.8320667147636414,
      "learning_rate": 0.00010588865096359744,
      "loss": 0.139,
      "step": 2638
    },
    {
      "epoch": 1.41285140562249,
      "grad_norm": 0.2859957814216614,
      "learning_rate": 0.00010585296216987866,
      "loss": 0.0578,
      "step": 2639
    },
    {
      "epoch": 1.4133868808567605,
      "grad_norm": 0.49788784980773926,
      "learning_rate": 0.00010581727337615989,
      "loss": 0.0731,
      "step": 2640
    },
    {
      "epoch": 1.4139223560910308,
      "grad_norm": 0.3810053765773773,
      "learning_rate": 0.00010578158458244111,
      "loss": 0.069,
      "step": 2641
    },
    {
      "epoch": 1.4144578313253011,
      "grad_norm": 0.6007817387580872,
      "learning_rate": 0.00010574589578872233,
      "loss": 0.085,
      "step": 2642
    },
    {
      "epoch": 1.4149933065595717,
      "grad_norm": 1.7283233404159546,
      "learning_rate": 0.00010571020699500358,
      "loss": 0.1265,
      "step": 2643
    },
    {
      "epoch": 1.415528781793842,
      "grad_norm": 0.8220987915992737,
      "learning_rate": 0.0001056745182012848,
      "loss": 0.0943,
      "step": 2644
    },
    {
      "epoch": 1.4160642570281126,
      "grad_norm": 0.7808987498283386,
      "learning_rate": 0.00010563882940756602,
      "loss": 0.1246,
      "step": 2645
    },
    {
      "epoch": 1.416599732262383,
      "grad_norm": 0.8473033308982849,
      "learning_rate": 0.00010560314061384727,
      "loss": 0.1269,
      "step": 2646
    },
    {
      "epoch": 1.4171352074966532,
      "grad_norm": 0.7055055499076843,
      "learning_rate": 0.00010556745182012849,
      "loss": 0.1315,
      "step": 2647
    },
    {
      "epoch": 1.4176706827309236,
      "grad_norm": 0.5780289173126221,
      "learning_rate": 0.0001055317630264097,
      "loss": 0.0953,
      "step": 2648
    },
    {
      "epoch": 1.4182061579651941,
      "grad_norm": 0.6155160665512085,
      "learning_rate": 0.00010549607423269094,
      "loss": 0.1223,
      "step": 2649
    },
    {
      "epoch": 1.4187416331994644,
      "grad_norm": 1.0273497104644775,
      "learning_rate": 0.00010546038543897216,
      "loss": 0.1186,
      "step": 2650
    },
    {
      "epoch": 1.419277108433735,
      "grad_norm": 4.7485480308532715,
      "learning_rate": 0.00010542469664525338,
      "loss": 0.1189,
      "step": 2651
    },
    {
      "epoch": 1.4198125836680053,
      "grad_norm": 1.8866112232208252,
      "learning_rate": 0.00010538900785153463,
      "loss": 0.0667,
      "step": 2652
    },
    {
      "epoch": 1.4203480589022757,
      "grad_norm": 0.6059733629226685,
      "learning_rate": 0.00010535331905781584,
      "loss": 0.1388,
      "step": 2653
    },
    {
      "epoch": 1.4208835341365462,
      "grad_norm": 0.612150251865387,
      "learning_rate": 0.00010531763026409709,
      "loss": 0.0929,
      "step": 2654
    },
    {
      "epoch": 1.4214190093708166,
      "grad_norm": 0.890557587146759,
      "learning_rate": 0.00010528194147037831,
      "loss": 0.1383,
      "step": 2655
    },
    {
      "epoch": 1.421954484605087,
      "grad_norm": 2.9954001903533936,
      "learning_rate": 0.00010524625267665953,
      "loss": 0.0666,
      "step": 2656
    },
    {
      "epoch": 1.4224899598393574,
      "grad_norm": 0.8623095750808716,
      "learning_rate": 0.00010521056388294077,
      "loss": 0.1228,
      "step": 2657
    },
    {
      "epoch": 1.4230254350736278,
      "grad_norm": 4.549439430236816,
      "learning_rate": 0.00010517487508922198,
      "loss": 0.0907,
      "step": 2658
    },
    {
      "epoch": 1.4235609103078983,
      "grad_norm": 0.3167523741722107,
      "learning_rate": 0.0001051391862955032,
      "loss": 0.0412,
      "step": 2659
    },
    {
      "epoch": 1.4240963855421687,
      "grad_norm": 0.39011362195014954,
      "learning_rate": 0.00010510349750178445,
      "loss": 0.0159,
      "step": 2660
    },
    {
      "epoch": 1.4246318607764392,
      "grad_norm": 0.5602947473526001,
      "learning_rate": 0.00010506780870806567,
      "loss": 0.0483,
      "step": 2661
    },
    {
      "epoch": 1.4251673360107096,
      "grad_norm": 0.9822659492492676,
      "learning_rate": 0.00010503211991434689,
      "loss": 0.1912,
      "step": 2662
    },
    {
      "epoch": 1.4257028112449799,
      "grad_norm": 0.6698852181434631,
      "learning_rate": 0.00010499643112062814,
      "loss": 0.0869,
      "step": 2663
    },
    {
      "epoch": 1.4262382864792502,
      "grad_norm": 1.0714290142059326,
      "learning_rate": 0.00010496074232690936,
      "loss": 0.1728,
      "step": 2664
    },
    {
      "epoch": 1.4267737617135208,
      "grad_norm": 1.694903016090393,
      "learning_rate": 0.00010492505353319058,
      "loss": 0.1427,
      "step": 2665
    },
    {
      "epoch": 1.427309236947791,
      "grad_norm": 0.5685879588127136,
      "learning_rate": 0.00010488936473947181,
      "loss": 0.0723,
      "step": 2666
    },
    {
      "epoch": 1.4278447121820617,
      "grad_norm": 0.8374356627464294,
      "learning_rate": 0.00010485367594575303,
      "loss": 0.1593,
      "step": 2667
    },
    {
      "epoch": 1.428380187416332,
      "grad_norm": 0.6851289868354797,
      "learning_rate": 0.00010481798715203427,
      "loss": 0.1053,
      "step": 2668
    },
    {
      "epoch": 1.4289156626506023,
      "grad_norm": 0.5411627888679504,
      "learning_rate": 0.0001047822983583155,
      "loss": 0.1167,
      "step": 2669
    },
    {
      "epoch": 1.4294511378848729,
      "grad_norm": 0.548608124256134,
      "learning_rate": 0.00010474660956459672,
      "loss": 0.0496,
      "step": 2670
    },
    {
      "epoch": 1.4299866131191432,
      "grad_norm": 0.5440146327018738,
      "learning_rate": 0.00010471092077087794,
      "loss": 0.0687,
      "step": 2671
    },
    {
      "epoch": 1.4305220883534138,
      "grad_norm": 0.8791679739952087,
      "learning_rate": 0.00010467523197715919,
      "loss": 0.1352,
      "step": 2672
    },
    {
      "epoch": 1.431057563587684,
      "grad_norm": 2.541233539581299,
      "learning_rate": 0.0001046395431834404,
      "loss": 0.1182,
      "step": 2673
    },
    {
      "epoch": 1.4315930388219544,
      "grad_norm": 0.5532950758934021,
      "learning_rate": 0.00010460385438972162,
      "loss": 0.0619,
      "step": 2674
    },
    {
      "epoch": 1.4321285140562248,
      "grad_norm": 0.6268812417984009,
      "learning_rate": 0.00010456816559600286,
      "loss": 0.0716,
      "step": 2675
    },
    {
      "epoch": 1.4326639892904953,
      "grad_norm": 0.46842873096466064,
      "learning_rate": 0.00010453247680228409,
      "loss": 0.0835,
      "step": 2676
    },
    {
      "epoch": 1.4331994645247657,
      "grad_norm": 0.39625129103660583,
      "learning_rate": 0.00010449678800856531,
      "loss": 0.027,
      "step": 2677
    },
    {
      "epoch": 1.4337349397590362,
      "grad_norm": 1.3066154718399048,
      "learning_rate": 0.00010446109921484655,
      "loss": 0.1424,
      "step": 2678
    },
    {
      "epoch": 1.4342704149933065,
      "grad_norm": 0.5222008228302002,
      "learning_rate": 0.00010442541042112776,
      "loss": 0.1014,
      "step": 2679
    },
    {
      "epoch": 1.4348058902275769,
      "grad_norm": 0.6236961483955383,
      "learning_rate": 0.00010438972162740901,
      "loss": 0.0662,
      "step": 2680
    },
    {
      "epoch": 1.4353413654618474,
      "grad_norm": 0.5971802473068237,
      "learning_rate": 0.00010435403283369023,
      "loss": 0.0995,
      "step": 2681
    },
    {
      "epoch": 1.4358768406961178,
      "grad_norm": 2.20432710647583,
      "learning_rate": 0.00010431834403997145,
      "loss": 0.0985,
      "step": 2682
    },
    {
      "epoch": 1.4364123159303883,
      "grad_norm": 1.0022155046463013,
      "learning_rate": 0.00010428265524625269,
      "loss": 0.1507,
      "step": 2683
    },
    {
      "epoch": 1.4369477911646586,
      "grad_norm": 0.8902365565299988,
      "learning_rate": 0.00010424696645253392,
      "loss": 0.133,
      "step": 2684
    },
    {
      "epoch": 1.437483266398929,
      "grad_norm": 0.6073533892631531,
      "learning_rate": 0.00010421127765881514,
      "loss": 0.0604,
      "step": 2685
    },
    {
      "epoch": 1.4380187416331995,
      "grad_norm": 0.7545893788337708,
      "learning_rate": 0.00010417558886509637,
      "loss": 0.1111,
      "step": 2686
    },
    {
      "epoch": 1.4385542168674699,
      "grad_norm": 0.9658102989196777,
      "learning_rate": 0.00010413990007137759,
      "loss": 0.14,
      "step": 2687
    },
    {
      "epoch": 1.4390896921017404,
      "grad_norm": 0.6510087251663208,
      "learning_rate": 0.00010410421127765881,
      "loss": 0.0711,
      "step": 2688
    },
    {
      "epoch": 1.4396251673360108,
      "grad_norm": 0.5766235589981079,
      "learning_rate": 0.00010406852248394006,
      "loss": 0.04,
      "step": 2689
    },
    {
      "epoch": 1.440160642570281,
      "grad_norm": 0.7395593523979187,
      "learning_rate": 0.00010403283369022128,
      "loss": 0.1774,
      "step": 2690
    },
    {
      "epoch": 1.4406961178045514,
      "grad_norm": 0.9871740937232971,
      "learning_rate": 0.0001039971448965025,
      "loss": 0.1606,
      "step": 2691
    },
    {
      "epoch": 1.441231593038822,
      "grad_norm": 0.5918561220169067,
      "learning_rate": 0.00010396145610278375,
      "loss": 0.094,
      "step": 2692
    },
    {
      "epoch": 1.4417670682730923,
      "grad_norm": 0.6997690200805664,
      "learning_rate": 0.00010392576730906497,
      "loss": 0.1086,
      "step": 2693
    },
    {
      "epoch": 1.4423025435073629,
      "grad_norm": 1.4967584609985352,
      "learning_rate": 0.00010389007851534618,
      "loss": 0.1097,
      "step": 2694
    },
    {
      "epoch": 1.4428380187416332,
      "grad_norm": 1.173499584197998,
      "learning_rate": 0.00010385438972162742,
      "loss": 0.209,
      "step": 2695
    },
    {
      "epoch": 1.4433734939759035,
      "grad_norm": 0.5235021710395813,
      "learning_rate": 0.00010381870092790864,
      "loss": 0.0755,
      "step": 2696
    },
    {
      "epoch": 1.443908969210174,
      "grad_norm": 0.7961246967315674,
      "learning_rate": 0.00010378301213418986,
      "loss": 0.0658,
      "step": 2697
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 0.5678207874298096,
      "learning_rate": 0.0001037473233404711,
      "loss": 0.0643,
      "step": 2698
    },
    {
      "epoch": 1.444979919678715,
      "grad_norm": 0.9558597803115845,
      "learning_rate": 0.00010371163454675232,
      "loss": 0.0883,
      "step": 2699
    },
    {
      "epoch": 1.4455153949129853,
      "grad_norm": 0.6214250326156616,
      "learning_rate": 0.00010367594575303354,
      "loss": 0.0636,
      "step": 2700
    },
    {
      "epoch": 1.4460508701472556,
      "grad_norm": 5.265183925628662,
      "learning_rate": 0.00010364025695931479,
      "loss": 0.1078,
      "step": 2701
    },
    {
      "epoch": 1.446586345381526,
      "grad_norm": 0.5498404502868652,
      "learning_rate": 0.00010360456816559601,
      "loss": 0.0785,
      "step": 2702
    },
    {
      "epoch": 1.4471218206157965,
      "grad_norm": 1.1490142345428467,
      "learning_rate": 0.00010356887937187723,
      "loss": 0.1992,
      "step": 2703
    },
    {
      "epoch": 1.4476572958500669,
      "grad_norm": 1.067639708518982,
      "learning_rate": 0.00010353319057815846,
      "loss": 0.0871,
      "step": 2704
    },
    {
      "epoch": 1.4481927710843374,
      "grad_norm": 0.6273492574691772,
      "learning_rate": 0.00010349750178443968,
      "loss": 0.0594,
      "step": 2705
    },
    {
      "epoch": 1.4487282463186077,
      "grad_norm": 0.8604168891906738,
      "learning_rate": 0.0001034618129907209,
      "loss": 0.1352,
      "step": 2706
    },
    {
      "epoch": 1.449263721552878,
      "grad_norm": 0.8785693645477295,
      "learning_rate": 0.00010342612419700215,
      "loss": 0.0798,
      "step": 2707
    },
    {
      "epoch": 1.4497991967871486,
      "grad_norm": 4.388928413391113,
      "learning_rate": 0.00010339043540328337,
      "loss": 0.1779,
      "step": 2708
    },
    {
      "epoch": 1.450334672021419,
      "grad_norm": 0.7349056601524353,
      "learning_rate": 0.00010335474660956462,
      "loss": 0.1134,
      "step": 2709
    },
    {
      "epoch": 1.4508701472556895,
      "grad_norm": 0.4178164601325989,
      "learning_rate": 0.00010331905781584584,
      "loss": 0.0698,
      "step": 2710
    },
    {
      "epoch": 1.4514056224899599,
      "grad_norm": 1.0120872259140015,
      "learning_rate": 0.00010328336902212706,
      "loss": 0.0607,
      "step": 2711
    },
    {
      "epoch": 1.4519410977242302,
      "grad_norm": 0.5550978183746338,
      "learning_rate": 0.00010324768022840829,
      "loss": 0.0558,
      "step": 2712
    },
    {
      "epoch": 1.4524765729585007,
      "grad_norm": 0.496338814496994,
      "learning_rate": 0.00010321199143468951,
      "loss": 0.0502,
      "step": 2713
    },
    {
      "epoch": 1.453012048192771,
      "grad_norm": 0.9509196877479553,
      "learning_rate": 0.00010317630264097073,
      "loss": 0.168,
      "step": 2714
    },
    {
      "epoch": 1.4535475234270416,
      "grad_norm": 0.38344669342041016,
      "learning_rate": 0.00010314061384725198,
      "loss": 0.031,
      "step": 2715
    },
    {
      "epoch": 1.454082998661312,
      "grad_norm": 0.7336297631263733,
      "learning_rate": 0.0001031049250535332,
      "loss": 0.087,
      "step": 2716
    },
    {
      "epoch": 1.4546184738955823,
      "grad_norm": 0.5208529829978943,
      "learning_rate": 0.00010306923625981442,
      "loss": 0.0737,
      "step": 2717
    },
    {
      "epoch": 1.4551539491298526,
      "grad_norm": 0.47832927107810974,
      "learning_rate": 0.00010303354746609567,
      "loss": 0.0498,
      "step": 2718
    },
    {
      "epoch": 1.4556894243641232,
      "grad_norm": 1.0763983726501465,
      "learning_rate": 0.00010299785867237689,
      "loss": 0.1475,
      "step": 2719
    },
    {
      "epoch": 1.4562248995983935,
      "grad_norm": 0.7940011620521545,
      "learning_rate": 0.0001029621698786581,
      "loss": 0.1151,
      "step": 2720
    },
    {
      "epoch": 1.456760374832664,
      "grad_norm": 0.36034053564071655,
      "learning_rate": 0.00010292648108493934,
      "loss": 0.0241,
      "step": 2721
    },
    {
      "epoch": 1.4572958500669344,
      "grad_norm": 0.8402244448661804,
      "learning_rate": 0.00010289079229122056,
      "loss": 0.0741,
      "step": 2722
    },
    {
      "epoch": 1.4578313253012047,
      "grad_norm": 0.610702633857727,
      "learning_rate": 0.00010285510349750178,
      "loss": 0.0942,
      "step": 2723
    },
    {
      "epoch": 1.4583668005354753,
      "grad_norm": 0.668017566204071,
      "learning_rate": 0.00010281941470378303,
      "loss": 0.0975,
      "step": 2724
    },
    {
      "epoch": 1.4589022757697456,
      "grad_norm": 0.7969233989715576,
      "learning_rate": 0.00010278372591006424,
      "loss": 0.106,
      "step": 2725
    },
    {
      "epoch": 1.4594377510040162,
      "grad_norm": 0.7304818034172058,
      "learning_rate": 0.00010274803711634546,
      "loss": 0.1257,
      "step": 2726
    },
    {
      "epoch": 1.4599732262382865,
      "grad_norm": 1.1912267208099365,
      "learning_rate": 0.00010271234832262671,
      "loss": 0.1921,
      "step": 2727
    },
    {
      "epoch": 1.4605087014725568,
      "grad_norm": 0.3109908401966095,
      "learning_rate": 0.00010267665952890793,
      "loss": 0.0321,
      "step": 2728
    },
    {
      "epoch": 1.4610441767068272,
      "grad_norm": 0.6616087555885315,
      "learning_rate": 0.00010264097073518915,
      "loss": 0.0707,
      "step": 2729
    },
    {
      "epoch": 1.4615796519410977,
      "grad_norm": 0.9209445118904114,
      "learning_rate": 0.00010260528194147038,
      "loss": 0.1709,
      "step": 2730
    },
    {
      "epoch": 1.462115127175368,
      "grad_norm": 0.3176302909851074,
      "learning_rate": 0.0001025695931477516,
      "loss": 0.0218,
      "step": 2731
    },
    {
      "epoch": 1.4626506024096386,
      "grad_norm": 0.5406153202056885,
      "learning_rate": 0.00010253390435403282,
      "loss": 0.0784,
      "step": 2732
    },
    {
      "epoch": 1.463186077643909,
      "grad_norm": 0.773575484752655,
      "learning_rate": 0.00010249821556031407,
      "loss": 0.0705,
      "step": 2733
    },
    {
      "epoch": 1.4637215528781793,
      "grad_norm": 1.1379590034484863,
      "learning_rate": 0.00010246252676659529,
      "loss": 0.112,
      "step": 2734
    },
    {
      "epoch": 1.4642570281124498,
      "grad_norm": 0.9660289883613586,
      "learning_rate": 0.00010242683797287654,
      "loss": 0.2038,
      "step": 2735
    },
    {
      "epoch": 1.4647925033467202,
      "grad_norm": 1.0810918807983398,
      "learning_rate": 0.00010239114917915776,
      "loss": 0.0632,
      "step": 2736
    },
    {
      "epoch": 1.4653279785809907,
      "grad_norm": 0.9140413999557495,
      "learning_rate": 0.00010235546038543898,
      "loss": 0.1057,
      "step": 2737
    },
    {
      "epoch": 1.465863453815261,
      "grad_norm": 0.7296349406242371,
      "learning_rate": 0.00010231977159172021,
      "loss": 0.105,
      "step": 2738
    },
    {
      "epoch": 1.4663989290495314,
      "grad_norm": 1.082483172416687,
      "learning_rate": 0.00010228408279800143,
      "loss": 0.1281,
      "step": 2739
    },
    {
      "epoch": 1.466934404283802,
      "grad_norm": 0.6731275320053101,
      "learning_rate": 0.00010224839400428265,
      "loss": 0.0679,
      "step": 2740
    },
    {
      "epoch": 1.4674698795180723,
      "grad_norm": 1.091210126876831,
      "learning_rate": 0.0001022127052105639,
      "loss": 0.1144,
      "step": 2741
    },
    {
      "epoch": 1.4680053547523428,
      "grad_norm": 0.6718275547027588,
      "learning_rate": 0.00010217701641684512,
      "loss": 0.1107,
      "step": 2742
    },
    {
      "epoch": 1.4685408299866132,
      "grad_norm": 2.6598634719848633,
      "learning_rate": 0.00010214132762312634,
      "loss": 0.0562,
      "step": 2743
    },
    {
      "epoch": 1.4690763052208835,
      "grad_norm": 0.5879944562911987,
      "learning_rate": 0.00010210563882940759,
      "loss": 0.1185,
      "step": 2744
    },
    {
      "epoch": 1.4696117804551538,
      "grad_norm": 0.7235562801361084,
      "learning_rate": 0.0001020699500356888,
      "loss": 0.1045,
      "step": 2745
    },
    {
      "epoch": 1.4701472556894244,
      "grad_norm": 1.286169409751892,
      "learning_rate": 0.00010203426124197002,
      "loss": 0.1212,
      "step": 2746
    },
    {
      "epoch": 1.4706827309236947,
      "grad_norm": 1.19460129737854,
      "learning_rate": 0.00010199857244825126,
      "loss": 0.1014,
      "step": 2747
    },
    {
      "epoch": 1.4712182061579653,
      "grad_norm": 0.8088759779930115,
      "learning_rate": 0.00010196288365453248,
      "loss": 0.1057,
      "step": 2748
    },
    {
      "epoch": 1.4717536813922356,
      "grad_norm": 0.4214332103729248,
      "learning_rate": 0.0001019271948608137,
      "loss": 0.037,
      "step": 2749
    },
    {
      "epoch": 1.472289156626506,
      "grad_norm": 0.7722069025039673,
      "learning_rate": 0.00010189150606709495,
      "loss": 0.0961,
      "step": 2750
    },
    {
      "epoch": 1.4728246318607765,
      "grad_norm": 0.6989212036132812,
      "learning_rate": 0.00010185581727337616,
      "loss": 0.1294,
      "step": 2751
    },
    {
      "epoch": 1.4733601070950468,
      "grad_norm": 2.9921228885650635,
      "learning_rate": 0.00010182012847965738,
      "loss": 0.1437,
      "step": 2752
    },
    {
      "epoch": 1.4738955823293174,
      "grad_norm": 0.5176851153373718,
      "learning_rate": 0.00010178443968593863,
      "loss": 0.0845,
      "step": 2753
    },
    {
      "epoch": 1.4744310575635877,
      "grad_norm": 0.3777034282684326,
      "learning_rate": 0.00010174875089221985,
      "loss": 0.0558,
      "step": 2754
    },
    {
      "epoch": 1.474966532797858,
      "grad_norm": 1.1264493465423584,
      "learning_rate": 0.00010171306209850107,
      "loss": 0.1596,
      "step": 2755
    },
    {
      "epoch": 1.4755020080321284,
      "grad_norm": 0.7682713866233826,
      "learning_rate": 0.0001016773733047823,
      "loss": 0.077,
      "step": 2756
    },
    {
      "epoch": 1.476037483266399,
      "grad_norm": 1.2308130264282227,
      "learning_rate": 0.00010164168451106352,
      "loss": 0.1052,
      "step": 2757
    },
    {
      "epoch": 1.4765729585006693,
      "grad_norm": 0.7507281303405762,
      "learning_rate": 0.00010160599571734474,
      "loss": 0.0632,
      "step": 2758
    },
    {
      "epoch": 1.4771084337349398,
      "grad_norm": 0.4424080550670624,
      "learning_rate": 0.00010157030692362599,
      "loss": 0.0526,
      "step": 2759
    },
    {
      "epoch": 1.4776439089692102,
      "grad_norm": 0.8497499823570251,
      "learning_rate": 0.00010153461812990721,
      "loss": 0.0739,
      "step": 2760
    },
    {
      "epoch": 1.4781793842034805,
      "grad_norm": 0.6541553735733032,
      "learning_rate": 0.00010149892933618843,
      "loss": 0.1359,
      "step": 2761
    },
    {
      "epoch": 1.478714859437751,
      "grad_norm": 1.1790032386779785,
      "learning_rate": 0.00010146324054246968,
      "loss": 0.1724,
      "step": 2762
    },
    {
      "epoch": 1.4792503346720214,
      "grad_norm": 0.8387104868888855,
      "learning_rate": 0.0001014275517487509,
      "loss": 0.1641,
      "step": 2763
    },
    {
      "epoch": 1.479785809906292,
      "grad_norm": 0.7677648067474365,
      "learning_rate": 0.00010139186295503213,
      "loss": 0.0772,
      "step": 2764
    },
    {
      "epoch": 1.4803212851405623,
      "grad_norm": 0.6566424369812012,
      "learning_rate": 0.00010135617416131335,
      "loss": 0.087,
      "step": 2765
    },
    {
      "epoch": 1.4808567603748326,
      "grad_norm": 0.7966621518135071,
      "learning_rate": 0.00010132048536759457,
      "loss": 0.0868,
      "step": 2766
    },
    {
      "epoch": 1.4813922356091032,
      "grad_norm": 0.6351320743560791,
      "learning_rate": 0.00010128479657387582,
      "loss": 0.1065,
      "step": 2767
    },
    {
      "epoch": 1.4819277108433735,
      "grad_norm": 6.337430953979492,
      "learning_rate": 0.00010124910778015704,
      "loss": 0.1416,
      "step": 2768
    },
    {
      "epoch": 1.482463186077644,
      "grad_norm": 0.972923219203949,
      "learning_rate": 0.00010121341898643826,
      "loss": 0.077,
      "step": 2769
    },
    {
      "epoch": 1.4829986613119144,
      "grad_norm": 0.7548040747642517,
      "learning_rate": 0.0001011777301927195,
      "loss": 0.0472,
      "step": 2770
    },
    {
      "epoch": 1.4835341365461847,
      "grad_norm": 1.0012962818145752,
      "learning_rate": 0.00010114204139900072,
      "loss": 0.1123,
      "step": 2771
    },
    {
      "epoch": 1.484069611780455,
      "grad_norm": 0.9005132913589478,
      "learning_rate": 0.00010110635260528194,
      "loss": 0.1378,
      "step": 2772
    },
    {
      "epoch": 1.4846050870147256,
      "grad_norm": 0.5632866024971008,
      "learning_rate": 0.00010107066381156318,
      "loss": 0.0812,
      "step": 2773
    },
    {
      "epoch": 1.485140562248996,
      "grad_norm": 0.879328191280365,
      "learning_rate": 0.0001010349750178444,
      "loss": 0.138,
      "step": 2774
    },
    {
      "epoch": 1.4856760374832665,
      "grad_norm": 1.0140634775161743,
      "learning_rate": 0.00010099928622412562,
      "loss": 0.1233,
      "step": 2775
    },
    {
      "epoch": 1.4862115127175368,
      "grad_norm": 0.6948472261428833,
      "learning_rate": 0.00010096359743040686,
      "loss": 0.1147,
      "step": 2776
    },
    {
      "epoch": 1.4867469879518072,
      "grad_norm": 0.6348589062690735,
      "learning_rate": 0.00010092790863668808,
      "loss": 0.0564,
      "step": 2777
    },
    {
      "epoch": 1.4872824631860777,
      "grad_norm": 0.7255092263221741,
      "learning_rate": 0.0001008922198429693,
      "loss": 0.0864,
      "step": 2778
    },
    {
      "epoch": 1.487817938420348,
      "grad_norm": 0.6162142753601074,
      "learning_rate": 0.00010085653104925055,
      "loss": 0.0793,
      "step": 2779
    },
    {
      "epoch": 1.4883534136546186,
      "grad_norm": 1.1370959281921387,
      "learning_rate": 0.00010082084225553177,
      "loss": 0.1339,
      "step": 2780
    },
    {
      "epoch": 1.488888888888889,
      "grad_norm": 0.6769638657569885,
      "learning_rate": 0.00010078515346181299,
      "loss": 0.0549,
      "step": 2781
    },
    {
      "epoch": 1.4894243641231593,
      "grad_norm": 0.8302573561668396,
      "learning_rate": 0.00010074946466809422,
      "loss": 0.0428,
      "step": 2782
    },
    {
      "epoch": 1.4899598393574296,
      "grad_norm": 1.1515121459960938,
      "learning_rate": 0.00010071377587437544,
      "loss": 0.1425,
      "step": 2783
    },
    {
      "epoch": 1.4904953145917001,
      "grad_norm": 0.8705613017082214,
      "learning_rate": 0.00010067808708065666,
      "loss": 0.0877,
      "step": 2784
    },
    {
      "epoch": 1.4910307898259705,
      "grad_norm": 0.8149858713150024,
      "learning_rate": 0.00010064239828693791,
      "loss": 0.0963,
      "step": 2785
    },
    {
      "epoch": 1.491566265060241,
      "grad_norm": 0.8512398600578308,
      "learning_rate": 0.00010060670949321913,
      "loss": 0.0772,
      "step": 2786
    },
    {
      "epoch": 1.4921017402945114,
      "grad_norm": 0.7888523936271667,
      "learning_rate": 0.00010057102069950035,
      "loss": 0.1039,
      "step": 2787
    },
    {
      "epoch": 1.4926372155287817,
      "grad_norm": 1.0848995447158813,
      "learning_rate": 0.0001005353319057816,
      "loss": 0.1834,
      "step": 2788
    },
    {
      "epoch": 1.4931726907630523,
      "grad_norm": 0.8645389676094055,
      "learning_rate": 0.00010049964311206282,
      "loss": 0.0936,
      "step": 2789
    },
    {
      "epoch": 1.4937081659973226,
      "grad_norm": 0.6070827841758728,
      "learning_rate": 0.00010046395431834405,
      "loss": 0.0897,
      "step": 2790
    },
    {
      "epoch": 1.4942436412315931,
      "grad_norm": 0.4739658236503601,
      "learning_rate": 0.00010042826552462527,
      "loss": 0.0289,
      "step": 2791
    },
    {
      "epoch": 1.4947791164658635,
      "grad_norm": 1.026889443397522,
      "learning_rate": 0.00010039257673090649,
      "loss": 0.15,
      "step": 2792
    },
    {
      "epoch": 1.4953145917001338,
      "grad_norm": 0.5436292886734009,
      "learning_rate": 0.00010035688793718774,
      "loss": 0.0809,
      "step": 2793
    },
    {
      "epoch": 1.4958500669344044,
      "grad_norm": 1.5108354091644287,
      "learning_rate": 0.00010032119914346896,
      "loss": 0.146,
      "step": 2794
    },
    {
      "epoch": 1.4963855421686747,
      "grad_norm": 1.2028440237045288,
      "learning_rate": 0.00010028551034975018,
      "loss": 0.1024,
      "step": 2795
    },
    {
      "epoch": 1.496921017402945,
      "grad_norm": 1.3625712394714355,
      "learning_rate": 0.00010024982155603143,
      "loss": 0.1669,
      "step": 2796
    },
    {
      "epoch": 1.4974564926372156,
      "grad_norm": 0.8376135230064392,
      "learning_rate": 0.00010021413276231264,
      "loss": 0.0894,
      "step": 2797
    },
    {
      "epoch": 1.497991967871486,
      "grad_norm": 0.8666458129882812,
      "learning_rate": 0.00010017844396859386,
      "loss": 0.1029,
      "step": 2798
    },
    {
      "epoch": 1.4985274431057563,
      "grad_norm": 1.213455080986023,
      "learning_rate": 0.0001001427551748751,
      "loss": 0.1564,
      "step": 2799
    },
    {
      "epoch": 1.4990629183400268,
      "grad_norm": 0.733617901802063,
      "learning_rate": 0.00010010706638115632,
      "loss": 0.0789,
      "step": 2800
    },
    {
      "epoch": 1.4995983935742971,
      "grad_norm": 0.5219176411628723,
      "learning_rate": 0.00010007137758743754,
      "loss": 0.0519,
      "step": 2801
    },
    {
      "epoch": 1.5001338688085677,
      "grad_norm": 0.4877176582813263,
      "learning_rate": 0.00010003568879371878,
      "loss": 0.0517,
      "step": 2802
    },
    {
      "epoch": 1.500669344042838,
      "grad_norm": 0.6327006220817566,
      "learning_rate": 0.0001,
      "loss": 0.0681,
      "step": 2803
    },
    {
      "epoch": 1.5012048192771084,
      "grad_norm": 0.6770922541618347,
      "learning_rate": 9.996431120628124e-05,
      "loss": 0.0761,
      "step": 2804
    },
    {
      "epoch": 1.5017402945113787,
      "grad_norm": 0.7551183700561523,
      "learning_rate": 9.992862241256246e-05,
      "loss": 0.0771,
      "step": 2805
    },
    {
      "epoch": 1.5022757697456492,
      "grad_norm": 0.33335497975349426,
      "learning_rate": 9.989293361884369e-05,
      "loss": 0.0435,
      "step": 2806
    },
    {
      "epoch": 1.5028112449799198,
      "grad_norm": 0.4320622384548187,
      "learning_rate": 9.985724482512492e-05,
      "loss": 0.0345,
      "step": 2807
    },
    {
      "epoch": 1.5033467202141901,
      "grad_norm": 0.7679417133331299,
      "learning_rate": 9.982155603140614e-05,
      "loss": 0.1158,
      "step": 2808
    },
    {
      "epoch": 1.5038821954484605,
      "grad_norm": 0.49234503507614136,
      "learning_rate": 9.978586723768736e-05,
      "loss": 0.047,
      "step": 2809
    },
    {
      "epoch": 1.5044176706827308,
      "grad_norm": 0.7112907767295837,
      "learning_rate": 9.97501784439686e-05,
      "loss": 0.0783,
      "step": 2810
    },
    {
      "epoch": 1.5049531459170014,
      "grad_norm": 1.0081590414047241,
      "learning_rate": 9.971448965024983e-05,
      "loss": 0.1053,
      "step": 2811
    },
    {
      "epoch": 1.505488621151272,
      "grad_norm": 0.5662107467651367,
      "learning_rate": 9.967880085653105e-05,
      "loss": 0.0616,
      "step": 2812
    },
    {
      "epoch": 1.5060240963855422,
      "grad_norm": 0.4017393887042999,
      "learning_rate": 9.964311206281228e-05,
      "loss": 0.0318,
      "step": 2813
    },
    {
      "epoch": 1.5065595716198126,
      "grad_norm": 3.7582714557647705,
      "learning_rate": 9.960742326909352e-05,
      "loss": 0.0904,
      "step": 2814
    },
    {
      "epoch": 1.507095046854083,
      "grad_norm": 0.5273412466049194,
      "learning_rate": 9.957173447537474e-05,
      "loss": 0.0672,
      "step": 2815
    },
    {
      "epoch": 1.5076305220883535,
      "grad_norm": 0.7670683860778809,
      "learning_rate": 9.953604568165597e-05,
      "loss": 0.0786,
      "step": 2816
    },
    {
      "epoch": 1.5081659973226238,
      "grad_norm": 0.45385366678237915,
      "learning_rate": 9.950035688793719e-05,
      "loss": 0.0601,
      "step": 2817
    },
    {
      "epoch": 1.5087014725568944,
      "grad_norm": 1.1197788715362549,
      "learning_rate": 9.946466809421841e-05,
      "loss": 0.1088,
      "step": 2818
    },
    {
      "epoch": 1.5092369477911647,
      "grad_norm": 1.836422085762024,
      "learning_rate": 9.942897930049964e-05,
      "loss": 0.2157,
      "step": 2819
    },
    {
      "epoch": 1.509772423025435,
      "grad_norm": 0.710646390914917,
      "learning_rate": 9.939329050678088e-05,
      "loss": 0.0877,
      "step": 2820
    },
    {
      "epoch": 1.5103078982597054,
      "grad_norm": 0.7691617012023926,
      "learning_rate": 9.93576017130621e-05,
      "loss": 0.0397,
      "step": 2821
    },
    {
      "epoch": 1.510843373493976,
      "grad_norm": 0.5403712391853333,
      "learning_rate": 9.932191291934333e-05,
      "loss": 0.0712,
      "step": 2822
    },
    {
      "epoch": 1.5113788487282465,
      "grad_norm": 0.597920298576355,
      "learning_rate": 9.928622412562456e-05,
      "loss": 0.0779,
      "step": 2823
    },
    {
      "epoch": 1.5119143239625168,
      "grad_norm": 0.6627821326255798,
      "learning_rate": 9.92505353319058e-05,
      "loss": 0.1132,
      "step": 2824
    },
    {
      "epoch": 1.5124497991967871,
      "grad_norm": 0.5326328277587891,
      "learning_rate": 9.921484653818702e-05,
      "loss": 0.0693,
      "step": 2825
    },
    {
      "epoch": 1.5129852744310575,
      "grad_norm": 0.5509093403816223,
      "learning_rate": 9.917915774446824e-05,
      "loss": 0.046,
      "step": 2826
    },
    {
      "epoch": 1.513520749665328,
      "grad_norm": 0.5087181329727173,
      "learning_rate": 9.914346895074947e-05,
      "loss": 0.062,
      "step": 2827
    },
    {
      "epoch": 1.5140562248995983,
      "grad_norm": 0.547663152217865,
      "learning_rate": 9.910778015703069e-05,
      "loss": 0.0372,
      "step": 2828
    },
    {
      "epoch": 1.514591700133869,
      "grad_norm": 0.2715167999267578,
      "learning_rate": 9.907209136331192e-05,
      "loss": 0.0429,
      "step": 2829
    },
    {
      "epoch": 1.5151271753681392,
      "grad_norm": 1.0094809532165527,
      "learning_rate": 9.903640256959316e-05,
      "loss": 0.0706,
      "step": 2830
    },
    {
      "epoch": 1.5156626506024096,
      "grad_norm": 0.631417989730835,
      "learning_rate": 9.900071377587438e-05,
      "loss": 0.0505,
      "step": 2831
    },
    {
      "epoch": 1.51619812583668,
      "grad_norm": 1.0506545305252075,
      "learning_rate": 9.896502498215561e-05,
      "loss": 0.1272,
      "step": 2832
    },
    {
      "epoch": 1.5167336010709505,
      "grad_norm": 0.4747137427330017,
      "learning_rate": 9.892933618843684e-05,
      "loss": 0.0566,
      "step": 2833
    },
    {
      "epoch": 1.517269076305221,
      "grad_norm": 0.8756275773048401,
      "learning_rate": 9.889364739471806e-05,
      "loss": 0.0808,
      "step": 2834
    },
    {
      "epoch": 1.5178045515394913,
      "grad_norm": 1.1709836721420288,
      "learning_rate": 9.885795860099928e-05,
      "loss": 0.1257,
      "step": 2835
    },
    {
      "epoch": 1.5183400267737617,
      "grad_norm": 1.3857688903808594,
      "learning_rate": 9.882226980728052e-05,
      "loss": 0.1517,
      "step": 2836
    },
    {
      "epoch": 1.518875502008032,
      "grad_norm": 3.5125038623809814,
      "learning_rate": 9.878658101356174e-05,
      "loss": 0.2032,
      "step": 2837
    },
    {
      "epoch": 1.5194109772423026,
      "grad_norm": 0.7662874460220337,
      "learning_rate": 9.875089221984297e-05,
      "loss": 0.1098,
      "step": 2838
    },
    {
      "epoch": 1.5199464524765731,
      "grad_norm": 0.8407121300697327,
      "learning_rate": 9.87152034261242e-05,
      "loss": 0.0637,
      "step": 2839
    },
    {
      "epoch": 1.5204819277108435,
      "grad_norm": 1.2901880741119385,
      "learning_rate": 9.867951463240544e-05,
      "loss": 0.1463,
      "step": 2840
    },
    {
      "epoch": 1.5210174029451138,
      "grad_norm": 1.180560827255249,
      "learning_rate": 9.864382583868666e-05,
      "loss": 0.0925,
      "step": 2841
    },
    {
      "epoch": 1.5215528781793841,
      "grad_norm": 0.6678221225738525,
      "learning_rate": 9.860813704496789e-05,
      "loss": 0.0677,
      "step": 2842
    },
    {
      "epoch": 1.5220883534136547,
      "grad_norm": 1.31845223903656,
      "learning_rate": 9.857244825124911e-05,
      "loss": 0.1865,
      "step": 2843
    },
    {
      "epoch": 1.522623828647925,
      "grad_norm": 0.9993159174919128,
      "learning_rate": 9.853675945753033e-05,
      "loss": 0.1438,
      "step": 2844
    },
    {
      "epoch": 1.5231593038821956,
      "grad_norm": 1.5118768215179443,
      "learning_rate": 9.850107066381156e-05,
      "loss": 0.1446,
      "step": 2845
    },
    {
      "epoch": 1.523694779116466,
      "grad_norm": 0.8920060396194458,
      "learning_rate": 9.84653818700928e-05,
      "loss": 0.1067,
      "step": 2846
    },
    {
      "epoch": 1.5242302543507362,
      "grad_norm": 1.0029377937316895,
      "learning_rate": 9.842969307637402e-05,
      "loss": 0.1531,
      "step": 2847
    },
    {
      "epoch": 1.5247657295850066,
      "grad_norm": 4.069204330444336,
      "learning_rate": 9.839400428265525e-05,
      "loss": 0.1413,
      "step": 2848
    },
    {
      "epoch": 1.5253012048192771,
      "grad_norm": 0.831882655620575,
      "learning_rate": 9.835831548893648e-05,
      "loss": 0.0641,
      "step": 2849
    },
    {
      "epoch": 1.5258366800535477,
      "grad_norm": 0.6598395109176636,
      "learning_rate": 9.83226266952177e-05,
      "loss": 0.1029,
      "step": 2850
    },
    {
      "epoch": 1.526372155287818,
      "grad_norm": 0.5913110971450806,
      "learning_rate": 9.828693790149894e-05,
      "loss": 0.1223,
      "step": 2851
    },
    {
      "epoch": 1.5269076305220883,
      "grad_norm": 0.7103506326675415,
      "learning_rate": 9.825124910778016e-05,
      "loss": 0.1126,
      "step": 2852
    },
    {
      "epoch": 1.5274431057563587,
      "grad_norm": 0.6479618549346924,
      "learning_rate": 9.821556031406139e-05,
      "loss": 0.1096,
      "step": 2853
    },
    {
      "epoch": 1.5279785809906292,
      "grad_norm": 1.0967961549758911,
      "learning_rate": 9.817987152034261e-05,
      "loss": 0.09,
      "step": 2854
    },
    {
      "epoch": 1.5285140562248996,
      "grad_norm": 0.704014241695404,
      "learning_rate": 9.814418272662384e-05,
      "loss": 0.1001,
      "step": 2855
    },
    {
      "epoch": 1.52904953145917,
      "grad_norm": 0.9078488945960999,
      "learning_rate": 9.810849393290508e-05,
      "loss": 0.1,
      "step": 2856
    },
    {
      "epoch": 1.5295850066934404,
      "grad_norm": 1.1708420515060425,
      "learning_rate": 9.80728051391863e-05,
      "loss": 0.146,
      "step": 2857
    },
    {
      "epoch": 1.5301204819277108,
      "grad_norm": 0.3973124027252197,
      "learning_rate": 9.803711634546753e-05,
      "loss": 0.0539,
      "step": 2858
    },
    {
      "epoch": 1.530655957161981,
      "grad_norm": 0.48258161544799805,
      "learning_rate": 9.800142755174876e-05,
      "loss": 0.0493,
      "step": 2859
    },
    {
      "epoch": 1.5311914323962517,
      "grad_norm": 0.6302805542945862,
      "learning_rate": 9.796573875802998e-05,
      "loss": 0.0651,
      "step": 2860
    },
    {
      "epoch": 1.5317269076305222,
      "grad_norm": 0.5226149559020996,
      "learning_rate": 9.79300499643112e-05,
      "loss": 0.0895,
      "step": 2861
    },
    {
      "epoch": 1.5322623828647925,
      "grad_norm": 0.4958910346031189,
      "learning_rate": 9.789436117059244e-05,
      "loss": 0.062,
      "step": 2862
    },
    {
      "epoch": 1.5327978580990629,
      "grad_norm": 0.6461722254753113,
      "learning_rate": 9.785867237687366e-05,
      "loss": 0.0813,
      "step": 2863
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.31222185492515564,
      "learning_rate": 9.782298358315489e-05,
      "loss": 0.0576,
      "step": 2864
    },
    {
      "epoch": 1.5338688085676038,
      "grad_norm": 0.8592644333839417,
      "learning_rate": 9.778729478943612e-05,
      "loss": 0.1252,
      "step": 2865
    },
    {
      "epoch": 1.5344042838018743,
      "grad_norm": 1.1153783798217773,
      "learning_rate": 9.775160599571736e-05,
      "loss": 0.2221,
      "step": 2866
    },
    {
      "epoch": 1.5349397590361447,
      "grad_norm": 0.503495454788208,
      "learning_rate": 9.771591720199858e-05,
      "loss": 0.0764,
      "step": 2867
    },
    {
      "epoch": 1.535475234270415,
      "grad_norm": 0.3755902349948883,
      "learning_rate": 9.768022840827981e-05,
      "loss": 0.0566,
      "step": 2868
    },
    {
      "epoch": 1.5360107095046853,
      "grad_norm": 0.6764099597930908,
      "learning_rate": 9.764453961456103e-05,
      "loss": 0.1016,
      "step": 2869
    },
    {
      "epoch": 1.5365461847389559,
      "grad_norm": 1.1785967350006104,
      "learning_rate": 9.760885082084225e-05,
      "loss": 0.1122,
      "step": 2870
    },
    {
      "epoch": 1.5370816599732262,
      "grad_norm": 0.47299426794052124,
      "learning_rate": 9.757316202712348e-05,
      "loss": 0.0581,
      "step": 2871
    },
    {
      "epoch": 1.5376171352074968,
      "grad_norm": 0.49190273880958557,
      "learning_rate": 9.753747323340472e-05,
      "loss": 0.0369,
      "step": 2872
    },
    {
      "epoch": 1.538152610441767,
      "grad_norm": 0.5374811887741089,
      "learning_rate": 9.750178443968594e-05,
      "loss": 0.0785,
      "step": 2873
    },
    {
      "epoch": 1.5386880856760374,
      "grad_norm": 0.4724690318107605,
      "learning_rate": 9.746609564596717e-05,
      "loss": 0.0466,
      "step": 2874
    },
    {
      "epoch": 1.5392235609103078,
      "grad_norm": 0.6177425384521484,
      "learning_rate": 9.74304068522484e-05,
      "loss": 0.1037,
      "step": 2875
    },
    {
      "epoch": 1.5397590361445783,
      "grad_norm": 0.8645059466362,
      "learning_rate": 9.739471805852962e-05,
      "loss": 0.1118,
      "step": 2876
    },
    {
      "epoch": 1.5402945113788489,
      "grad_norm": 0.779440701007843,
      "learning_rate": 9.735902926481086e-05,
      "loss": 0.1772,
      "step": 2877
    },
    {
      "epoch": 1.5408299866131192,
      "grad_norm": 0.4464128315448761,
      "learning_rate": 9.732334047109208e-05,
      "loss": 0.0643,
      "step": 2878
    },
    {
      "epoch": 1.5413654618473895,
      "grad_norm": 0.49002987146377563,
      "learning_rate": 9.728765167737331e-05,
      "loss": 0.0304,
      "step": 2879
    },
    {
      "epoch": 1.5419009370816599,
      "grad_norm": 0.6117960214614868,
      "learning_rate": 9.725196288365453e-05,
      "loss": 0.0678,
      "step": 2880
    },
    {
      "epoch": 1.5424364123159304,
      "grad_norm": 0.8457937836647034,
      "learning_rate": 9.721627408993576e-05,
      "loss": 0.1235,
      "step": 2881
    },
    {
      "epoch": 1.5429718875502008,
      "grad_norm": 0.45913025736808777,
      "learning_rate": 9.7180585296217e-05,
      "loss": 0.04,
      "step": 2882
    },
    {
      "epoch": 1.5435073627844713,
      "grad_norm": 0.6122757792472839,
      "learning_rate": 9.714489650249822e-05,
      "loss": 0.0388,
      "step": 2883
    },
    {
      "epoch": 1.5440428380187416,
      "grad_norm": 0.5470637679100037,
      "learning_rate": 9.710920770877945e-05,
      "loss": 0.0531,
      "step": 2884
    },
    {
      "epoch": 1.544578313253012,
      "grad_norm": 5.210971355438232,
      "learning_rate": 9.707351891506068e-05,
      "loss": 0.1815,
      "step": 2885
    },
    {
      "epoch": 1.5451137884872823,
      "grad_norm": 0.595363438129425,
      "learning_rate": 9.70378301213419e-05,
      "loss": 0.1021,
      "step": 2886
    },
    {
      "epoch": 1.5456492637215529,
      "grad_norm": 0.826835572719574,
      "learning_rate": 9.700214132762312e-05,
      "loss": 0.1101,
      "step": 2887
    },
    {
      "epoch": 1.5461847389558234,
      "grad_norm": 1.0272988080978394,
      "learning_rate": 9.696645253390436e-05,
      "loss": 0.1212,
      "step": 2888
    },
    {
      "epoch": 1.5467202141900938,
      "grad_norm": 0.22873415052890778,
      "learning_rate": 9.693076374018558e-05,
      "loss": 0.0146,
      "step": 2889
    },
    {
      "epoch": 1.547255689424364,
      "grad_norm": 3.238929033279419,
      "learning_rate": 9.689507494646681e-05,
      "loss": 0.0701,
      "step": 2890
    },
    {
      "epoch": 1.5477911646586344,
      "grad_norm": 0.6744180917739868,
      "learning_rate": 9.685938615274804e-05,
      "loss": 0.1022,
      "step": 2891
    },
    {
      "epoch": 1.548326639892905,
      "grad_norm": 1.2620242834091187,
      "learning_rate": 9.682369735902926e-05,
      "loss": 0.1694,
      "step": 2892
    },
    {
      "epoch": 1.5488621151271755,
      "grad_norm": 0.566122829914093,
      "learning_rate": 9.67880085653105e-05,
      "loss": 0.0261,
      "step": 2893
    },
    {
      "epoch": 1.5493975903614459,
      "grad_norm": 0.5274292230606079,
      "learning_rate": 9.675231977159173e-05,
      "loss": 0.0387,
      "step": 2894
    },
    {
      "epoch": 1.5499330655957162,
      "grad_norm": 0.8306838274002075,
      "learning_rate": 9.671663097787295e-05,
      "loss": 0.1414,
      "step": 2895
    },
    {
      "epoch": 1.5504685408299865,
      "grad_norm": 1.3072532415390015,
      "learning_rate": 9.668094218415417e-05,
      "loss": 0.1623,
      "step": 2896
    },
    {
      "epoch": 1.5510040160642569,
      "grad_norm": 1.0316663980484009,
      "learning_rate": 9.66452533904354e-05,
      "loss": 0.1856,
      "step": 2897
    },
    {
      "epoch": 1.5515394912985274,
      "grad_norm": 2.853409767150879,
      "learning_rate": 9.660956459671664e-05,
      "loss": 0.0562,
      "step": 2898
    },
    {
      "epoch": 1.552074966532798,
      "grad_norm": 1.304922342300415,
      "learning_rate": 9.657387580299786e-05,
      "loss": 0.1315,
      "step": 2899
    },
    {
      "epoch": 1.5526104417670683,
      "grad_norm": 1.213964581489563,
      "learning_rate": 9.653818700927909e-05,
      "loss": 0.1053,
      "step": 2900
    },
    {
      "epoch": 1.5531459170013386,
      "grad_norm": 1.4258618354797363,
      "learning_rate": 9.650249821556032e-05,
      "loss": 0.1697,
      "step": 2901
    },
    {
      "epoch": 1.553681392235609,
      "grad_norm": 0.46405017375946045,
      "learning_rate": 9.646680942184154e-05,
      "loss": 0.0471,
      "step": 2902
    },
    {
      "epoch": 1.5542168674698795,
      "grad_norm": 1.1108951568603516,
      "learning_rate": 9.643112062812278e-05,
      "loss": 0.0808,
      "step": 2903
    },
    {
      "epoch": 1.55475234270415,
      "grad_norm": 0.6472181677818298,
      "learning_rate": 9.6395431834404e-05,
      "loss": 0.1182,
      "step": 2904
    },
    {
      "epoch": 1.5552878179384204,
      "grad_norm": 0.9163407683372498,
      "learning_rate": 9.635974304068522e-05,
      "loss": 0.15,
      "step": 2905
    },
    {
      "epoch": 1.5558232931726907,
      "grad_norm": 0.7999005317687988,
      "learning_rate": 9.632405424696645e-05,
      "loss": 0.1218,
      "step": 2906
    },
    {
      "epoch": 1.556358768406961,
      "grad_norm": 0.9693827033042908,
      "learning_rate": 9.628836545324768e-05,
      "loss": 0.1537,
      "step": 2907
    },
    {
      "epoch": 1.5568942436412316,
      "grad_norm": 0.6121357083320618,
      "learning_rate": 9.625267665952892e-05,
      "loss": 0.074,
      "step": 2908
    },
    {
      "epoch": 1.557429718875502,
      "grad_norm": 1.312519907951355,
      "learning_rate": 9.621698786581014e-05,
      "loss": 0.0621,
      "step": 2909
    },
    {
      "epoch": 1.5579651941097725,
      "grad_norm": 0.6580442190170288,
      "learning_rate": 9.618129907209137e-05,
      "loss": 0.0796,
      "step": 2910
    },
    {
      "epoch": 1.5585006693440429,
      "grad_norm": 0.3796667754650116,
      "learning_rate": 9.61456102783726e-05,
      "loss": 0.0264,
      "step": 2911
    },
    {
      "epoch": 1.5590361445783132,
      "grad_norm": 0.5652485489845276,
      "learning_rate": 9.610992148465382e-05,
      "loss": 0.0258,
      "step": 2912
    },
    {
      "epoch": 1.5595716198125835,
      "grad_norm": 1.1078987121582031,
      "learning_rate": 9.607423269093504e-05,
      "loss": 0.1364,
      "step": 2913
    },
    {
      "epoch": 1.560107095046854,
      "grad_norm": 1.175321102142334,
      "learning_rate": 9.603854389721628e-05,
      "loss": 0.123,
      "step": 2914
    },
    {
      "epoch": 1.5606425702811246,
      "grad_norm": 0.6220240592956543,
      "learning_rate": 9.60028551034975e-05,
      "loss": 0.0549,
      "step": 2915
    },
    {
      "epoch": 1.561178045515395,
      "grad_norm": 0.48027798533439636,
      "learning_rate": 9.596716630977873e-05,
      "loss": 0.0563,
      "step": 2916
    },
    {
      "epoch": 1.5617135207496653,
      "grad_norm": 0.5796412229537964,
      "learning_rate": 9.593147751605996e-05,
      "loss": 0.0617,
      "step": 2917
    },
    {
      "epoch": 1.5622489959839356,
      "grad_norm": 0.8552273511886597,
      "learning_rate": 9.589578872234118e-05,
      "loss": 0.0598,
      "step": 2918
    },
    {
      "epoch": 1.5627844712182062,
      "grad_norm": 0.6100403070449829,
      "learning_rate": 9.586009992862242e-05,
      "loss": 0.0906,
      "step": 2919
    },
    {
      "epoch": 1.5633199464524767,
      "grad_norm": 1.038619875907898,
      "learning_rate": 9.582441113490365e-05,
      "loss": 0.0851,
      "step": 2920
    },
    {
      "epoch": 1.563855421686747,
      "grad_norm": 0.5901054739952087,
      "learning_rate": 9.578872234118487e-05,
      "loss": 0.0754,
      "step": 2921
    },
    {
      "epoch": 1.5643908969210174,
      "grad_norm": 0.7594460248947144,
      "learning_rate": 9.575303354746609e-05,
      "loss": 0.1072,
      "step": 2922
    },
    {
      "epoch": 1.5649263721552877,
      "grad_norm": 1.0639138221740723,
      "learning_rate": 9.571734475374732e-05,
      "loss": 0.1581,
      "step": 2923
    },
    {
      "epoch": 1.565461847389558,
      "grad_norm": 0.6043781042098999,
      "learning_rate": 9.568165596002856e-05,
      "loss": 0.0575,
      "step": 2924
    },
    {
      "epoch": 1.5659973226238286,
      "grad_norm": 0.7429699301719666,
      "learning_rate": 9.564596716630978e-05,
      "loss": 0.084,
      "step": 2925
    },
    {
      "epoch": 1.5665327978580992,
      "grad_norm": 0.887424886226654,
      "learning_rate": 9.561027837259101e-05,
      "loss": 0.1221,
      "step": 2926
    },
    {
      "epoch": 1.5670682730923695,
      "grad_norm": 0.7716686129570007,
      "learning_rate": 9.557458957887224e-05,
      "loss": 0.1168,
      "step": 2927
    },
    {
      "epoch": 1.5676037483266398,
      "grad_norm": 2.279524564743042,
      "learning_rate": 9.553890078515346e-05,
      "loss": 0.117,
      "step": 2928
    },
    {
      "epoch": 1.5681392235609102,
      "grad_norm": 0.9832136631011963,
      "learning_rate": 9.55032119914347e-05,
      "loss": 0.1717,
      "step": 2929
    },
    {
      "epoch": 1.5686746987951807,
      "grad_norm": 1.6388176679611206,
      "learning_rate": 9.546752319771592e-05,
      "loss": 0.2248,
      "step": 2930
    },
    {
      "epoch": 1.5692101740294513,
      "grad_norm": 0.4791618585586548,
      "learning_rate": 9.543183440399715e-05,
      "loss": 0.079,
      "step": 2931
    },
    {
      "epoch": 1.5697456492637216,
      "grad_norm": 3.708186149597168,
      "learning_rate": 9.539614561027837e-05,
      "loss": 0.0861,
      "step": 2932
    },
    {
      "epoch": 1.570281124497992,
      "grad_norm": 2.6740031242370605,
      "learning_rate": 9.53604568165596e-05,
      "loss": 0.1979,
      "step": 2933
    },
    {
      "epoch": 1.5708165997322623,
      "grad_norm": 1.4262993335723877,
      "learning_rate": 9.532476802284084e-05,
      "loss": 0.15,
      "step": 2934
    },
    {
      "epoch": 1.5713520749665328,
      "grad_norm": 0.74874347448349,
      "learning_rate": 9.528907922912206e-05,
      "loss": 0.1207,
      "step": 2935
    },
    {
      "epoch": 1.5718875502008032,
      "grad_norm": 0.7829805016517639,
      "learning_rate": 9.525339043540329e-05,
      "loss": 0.0984,
      "step": 2936
    },
    {
      "epoch": 1.5724230254350737,
      "grad_norm": 0.4242613911628723,
      "learning_rate": 9.521770164168452e-05,
      "loss": 0.0379,
      "step": 2937
    },
    {
      "epoch": 1.572958500669344,
      "grad_norm": 0.9403365254402161,
      "learning_rate": 9.518201284796574e-05,
      "loss": 0.0956,
      "step": 2938
    },
    {
      "epoch": 1.5734939759036144,
      "grad_norm": 0.9325809478759766,
      "learning_rate": 9.514632405424698e-05,
      "loss": 0.1719,
      "step": 2939
    },
    {
      "epoch": 1.5740294511378847,
      "grad_norm": 0.309393972158432,
      "learning_rate": 9.51106352605282e-05,
      "loss": 0.0249,
      "step": 2940
    },
    {
      "epoch": 1.5745649263721553,
      "grad_norm": 0.6617863774299622,
      "learning_rate": 9.507494646680942e-05,
      "loss": 0.0698,
      "step": 2941
    },
    {
      "epoch": 1.5751004016064258,
      "grad_norm": 0.32908275723457336,
      "learning_rate": 9.503925767309065e-05,
      "loss": 0.0538,
      "step": 2942
    },
    {
      "epoch": 1.5756358768406962,
      "grad_norm": 0.41997992992401123,
      "learning_rate": 9.500356887937188e-05,
      "loss": 0.0475,
      "step": 2943
    },
    {
      "epoch": 1.5761713520749665,
      "grad_norm": 0.21221016347408295,
      "learning_rate": 9.49678800856531e-05,
      "loss": 0.0191,
      "step": 2944
    },
    {
      "epoch": 1.5767068273092368,
      "grad_norm": 1.507453203201294,
      "learning_rate": 9.493219129193434e-05,
      "loss": 0.1488,
      "step": 2945
    },
    {
      "epoch": 1.5772423025435074,
      "grad_norm": 0.6324267387390137,
      "learning_rate": 9.489650249821557e-05,
      "loss": 0.088,
      "step": 2946
    },
    {
      "epoch": 1.5777777777777777,
      "grad_norm": 0.47476696968078613,
      "learning_rate": 9.486081370449679e-05,
      "loss": 0.0547,
      "step": 2947
    },
    {
      "epoch": 1.5783132530120483,
      "grad_norm": 0.4819590449333191,
      "learning_rate": 9.482512491077802e-05,
      "loss": 0.0638,
      "step": 2948
    },
    {
      "epoch": 1.5788487282463186,
      "grad_norm": 1.3720309734344482,
      "learning_rate": 9.478943611705924e-05,
      "loss": 0.1471,
      "step": 2949
    },
    {
      "epoch": 1.579384203480589,
      "grad_norm": 0.9481366872787476,
      "learning_rate": 9.475374732334048e-05,
      "loss": 0.1428,
      "step": 2950
    },
    {
      "epoch": 1.5799196787148593,
      "grad_norm": 1.035996913909912,
      "learning_rate": 9.47180585296217e-05,
      "loss": 0.1828,
      "step": 2951
    },
    {
      "epoch": 1.5804551539491298,
      "grad_norm": 0.28544849157333374,
      "learning_rate": 9.468236973590293e-05,
      "loss": 0.0261,
      "step": 2952
    },
    {
      "epoch": 1.5809906291834004,
      "grad_norm": 0.552983820438385,
      "learning_rate": 9.464668094218416e-05,
      "loss": 0.0917,
      "step": 2953
    },
    {
      "epoch": 1.5815261044176707,
      "grad_norm": 0.6596319079399109,
      "learning_rate": 9.461099214846538e-05,
      "loss": 0.065,
      "step": 2954
    },
    {
      "epoch": 1.582061579651941,
      "grad_norm": 0.5479084253311157,
      "learning_rate": 9.457530335474662e-05,
      "loss": 0.08,
      "step": 2955
    },
    {
      "epoch": 1.5825970548862114,
      "grad_norm": 0.5506578683853149,
      "learning_rate": 9.453961456102785e-05,
      "loss": 0.0908,
      "step": 2956
    },
    {
      "epoch": 1.583132530120482,
      "grad_norm": 0.49697360396385193,
      "learning_rate": 9.450392576730907e-05,
      "loss": 0.1127,
      "step": 2957
    },
    {
      "epoch": 1.5836680053547525,
      "grad_norm": 0.5162572860717773,
      "learning_rate": 9.446823697359029e-05,
      "loss": 0.0885,
      "step": 2958
    },
    {
      "epoch": 1.5842034805890228,
      "grad_norm": 4.330853462219238,
      "learning_rate": 9.443254817987152e-05,
      "loss": 0.1095,
      "step": 2959
    },
    {
      "epoch": 1.5847389558232932,
      "grad_norm": 0.595267653465271,
      "learning_rate": 9.439685938615274e-05,
      "loss": 0.0781,
      "step": 2960
    },
    {
      "epoch": 1.5852744310575635,
      "grad_norm": 0.412835955619812,
      "learning_rate": 9.436117059243398e-05,
      "loss": 0.032,
      "step": 2961
    },
    {
      "epoch": 1.585809906291834,
      "grad_norm": 0.8693007826805115,
      "learning_rate": 9.432548179871521e-05,
      "loss": 0.1144,
      "step": 2962
    },
    {
      "epoch": 1.5863453815261044,
      "grad_norm": 0.6094356179237366,
      "learning_rate": 9.428979300499644e-05,
      "loss": 0.1342,
      "step": 2963
    },
    {
      "epoch": 1.586880856760375,
      "grad_norm": 0.4551900625228882,
      "learning_rate": 9.425410421127766e-05,
      "loss": 0.0514,
      "step": 2964
    },
    {
      "epoch": 1.5874163319946453,
      "grad_norm": 0.44698217511177063,
      "learning_rate": 9.42184154175589e-05,
      "loss": 0.0605,
      "step": 2965
    },
    {
      "epoch": 1.5879518072289156,
      "grad_norm": 0.7952951192855835,
      "learning_rate": 9.418272662384012e-05,
      "loss": 0.1119,
      "step": 2966
    },
    {
      "epoch": 1.588487282463186,
      "grad_norm": 0.2724824845790863,
      "learning_rate": 9.414703783012134e-05,
      "loss": 0.0299,
      "step": 2967
    },
    {
      "epoch": 1.5890227576974565,
      "grad_norm": 0.8545852303504944,
      "learning_rate": 9.411134903640257e-05,
      "loss": 0.0871,
      "step": 2968
    },
    {
      "epoch": 1.589558232931727,
      "grad_norm": 0.5146628618240356,
      "learning_rate": 9.40756602426838e-05,
      "loss": 0.0609,
      "step": 2969
    },
    {
      "epoch": 1.5900937081659974,
      "grad_norm": 0.9398961067199707,
      "learning_rate": 9.403997144896502e-05,
      "loss": 0.1196,
      "step": 2970
    },
    {
      "epoch": 1.5906291834002677,
      "grad_norm": 0.6588450074195862,
      "learning_rate": 9.400428265524626e-05,
      "loss": 0.0951,
      "step": 2971
    },
    {
      "epoch": 1.591164658634538,
      "grad_norm": 0.5763821005821228,
      "learning_rate": 9.396859386152749e-05,
      "loss": 0.0636,
      "step": 2972
    },
    {
      "epoch": 1.5917001338688086,
      "grad_norm": 0.6151916980743408,
      "learning_rate": 9.393290506780871e-05,
      "loss": 0.0656,
      "step": 2973
    },
    {
      "epoch": 1.592235609103079,
      "grad_norm": 3.462553024291992,
      "learning_rate": 9.389721627408994e-05,
      "loss": 0.0507,
      "step": 2974
    },
    {
      "epoch": 1.5927710843373495,
      "grad_norm": 0.3175504505634308,
      "learning_rate": 9.386152748037116e-05,
      "loss": 0.0242,
      "step": 2975
    },
    {
      "epoch": 1.5933065595716198,
      "grad_norm": 0.8741804957389832,
      "learning_rate": 9.38258386866524e-05,
      "loss": 0.1059,
      "step": 2976
    },
    {
      "epoch": 1.5938420348058902,
      "grad_norm": 0.5511013269424438,
      "learning_rate": 9.379014989293362e-05,
      "loss": 0.0533,
      "step": 2977
    },
    {
      "epoch": 1.5943775100401605,
      "grad_norm": 1.9027498960494995,
      "learning_rate": 9.375446109921485e-05,
      "loss": 0.1547,
      "step": 2978
    },
    {
      "epoch": 1.594912985274431,
      "grad_norm": 1.1180195808410645,
      "learning_rate": 9.371877230549608e-05,
      "loss": 0.0831,
      "step": 2979
    },
    {
      "epoch": 1.5954484605087016,
      "grad_norm": 0.8183291554450989,
      "learning_rate": 9.36830835117773e-05,
      "loss": 0.0535,
      "step": 2980
    },
    {
      "epoch": 1.595983935742972,
      "grad_norm": 0.6235139966011047,
      "learning_rate": 9.364739471805854e-05,
      "loss": 0.0937,
      "step": 2981
    },
    {
      "epoch": 1.5965194109772423,
      "grad_norm": 1.5812917947769165,
      "learning_rate": 9.361170592433977e-05,
      "loss": 0.1151,
      "step": 2982
    },
    {
      "epoch": 1.5970548862115126,
      "grad_norm": 0.7728066444396973,
      "learning_rate": 9.357601713062099e-05,
      "loss": 0.1124,
      "step": 2983
    },
    {
      "epoch": 1.5975903614457831,
      "grad_norm": 0.5882190465927124,
      "learning_rate": 9.354032833690221e-05,
      "loss": 0.0624,
      "step": 2984
    },
    {
      "epoch": 1.5981258366800537,
      "grad_norm": 1.5409677028656006,
      "learning_rate": 9.350463954318344e-05,
      "loss": 0.1804,
      "step": 2985
    },
    {
      "epoch": 1.598661311914324,
      "grad_norm": 0.5091924071311951,
      "learning_rate": 9.346895074946466e-05,
      "loss": 0.0398,
      "step": 2986
    },
    {
      "epoch": 1.5991967871485944,
      "grad_norm": 0.8322700262069702,
      "learning_rate": 9.34332619557459e-05,
      "loss": 0.2013,
      "step": 2987
    },
    {
      "epoch": 1.5997322623828647,
      "grad_norm": 3.1265835762023926,
      "learning_rate": 9.339757316202713e-05,
      "loss": 0.1193,
      "step": 2988
    },
    {
      "epoch": 1.6002677376171353,
      "grad_norm": 0.5926802158355713,
      "learning_rate": 9.336188436830836e-05,
      "loss": 0.1109,
      "step": 2989
    },
    {
      "epoch": 1.6008032128514056,
      "grad_norm": 0.48625999689102173,
      "learning_rate": 9.332619557458958e-05,
      "loss": 0.0423,
      "step": 2990
    },
    {
      "epoch": 1.6013386880856761,
      "grad_norm": 0.9193379878997803,
      "learning_rate": 9.329050678087082e-05,
      "loss": 0.139,
      "step": 2991
    },
    {
      "epoch": 1.6018741633199465,
      "grad_norm": 1.0269453525543213,
      "learning_rate": 9.325481798715204e-05,
      "loss": 0.1297,
      "step": 2992
    },
    {
      "epoch": 1.6024096385542168,
      "grad_norm": 0.5539464950561523,
      "learning_rate": 9.321912919343326e-05,
      "loss": 0.0893,
      "step": 2993
    },
    {
      "epoch": 1.6029451137884871,
      "grad_norm": 0.9844040870666504,
      "learning_rate": 9.318344039971449e-05,
      "loss": 0.2101,
      "step": 2994
    },
    {
      "epoch": 1.6034805890227577,
      "grad_norm": 1.2263219356536865,
      "learning_rate": 9.314775160599572e-05,
      "loss": 0.0715,
      "step": 2995
    },
    {
      "epoch": 1.6040160642570283,
      "grad_norm": 0.4253959357738495,
      "learning_rate": 9.311206281227694e-05,
      "loss": 0.0434,
      "step": 2996
    },
    {
      "epoch": 1.6045515394912986,
      "grad_norm": 0.6891120076179504,
      "learning_rate": 9.307637401855818e-05,
      "loss": 0.0543,
      "step": 2997
    },
    {
      "epoch": 1.605087014725569,
      "grad_norm": 0.25966501235961914,
      "learning_rate": 9.304068522483941e-05,
      "loss": 0.0185,
      "step": 2998
    },
    {
      "epoch": 1.6056224899598392,
      "grad_norm": 0.9188346862792969,
      "learning_rate": 9.300499643112063e-05,
      "loss": 0.0914,
      "step": 2999
    },
    {
      "epoch": 1.6061579651941098,
      "grad_norm": 1.1050658226013184,
      "learning_rate": 9.296930763740186e-05,
      "loss": 0.1433,
      "step": 3000
    },
    {
      "epoch": 1.6066934404283801,
      "grad_norm": 0.9071949124336243,
      "learning_rate": 9.293361884368308e-05,
      "loss": 0.1342,
      "step": 3001
    },
    {
      "epoch": 1.6072289156626507,
      "grad_norm": 1.6323374509811401,
      "learning_rate": 9.28979300499643e-05,
      "loss": 0.1556,
      "step": 3002
    },
    {
      "epoch": 1.607764390896921,
      "grad_norm": 0.6199740767478943,
      "learning_rate": 9.286224125624554e-05,
      "loss": 0.0514,
      "step": 3003
    },
    {
      "epoch": 1.6082998661311914,
      "grad_norm": 2.5973427295684814,
      "learning_rate": 9.282655246252677e-05,
      "loss": 0.1835,
      "step": 3004
    },
    {
      "epoch": 1.6088353413654617,
      "grad_norm": 0.7499372363090515,
      "learning_rate": 9.2790863668808e-05,
      "loss": 0.0983,
      "step": 3005
    },
    {
      "epoch": 1.6093708165997322,
      "grad_norm": 6.624293804168701,
      "learning_rate": 9.275517487508922e-05,
      "loss": 0.1274,
      "step": 3006
    },
    {
      "epoch": 1.6099062918340028,
      "grad_norm": 0.81832355260849,
      "learning_rate": 9.271948608137046e-05,
      "loss": 0.1107,
      "step": 3007
    },
    {
      "epoch": 1.6104417670682731,
      "grad_norm": 2.306328058242798,
      "learning_rate": 9.268379728765169e-05,
      "loss": 0.0854,
      "step": 3008
    },
    {
      "epoch": 1.6109772423025435,
      "grad_norm": 0.4903615713119507,
      "learning_rate": 9.264810849393291e-05,
      "loss": 0.0971,
      "step": 3009
    },
    {
      "epoch": 1.6115127175368138,
      "grad_norm": 1.2448768615722656,
      "learning_rate": 9.261241970021413e-05,
      "loss": 0.1069,
      "step": 3010
    },
    {
      "epoch": 1.6120481927710844,
      "grad_norm": 0.5709693431854248,
      "learning_rate": 9.257673090649536e-05,
      "loss": 0.06,
      "step": 3011
    },
    {
      "epoch": 1.612583668005355,
      "grad_norm": 0.7201004028320312,
      "learning_rate": 9.254104211277658e-05,
      "loss": 0.1273,
      "step": 3012
    },
    {
      "epoch": 1.6131191432396252,
      "grad_norm": 0.891193687915802,
      "learning_rate": 9.250535331905782e-05,
      "loss": 0.136,
      "step": 3013
    },
    {
      "epoch": 1.6136546184738956,
      "grad_norm": 0.6351221799850464,
      "learning_rate": 9.246966452533905e-05,
      "loss": 0.073,
      "step": 3014
    },
    {
      "epoch": 1.614190093708166,
      "grad_norm": 0.7724539041519165,
      "learning_rate": 9.243397573162027e-05,
      "loss": 0.1227,
      "step": 3015
    },
    {
      "epoch": 1.6147255689424365,
      "grad_norm": 0.2952212393283844,
      "learning_rate": 9.23982869379015e-05,
      "loss": 0.03,
      "step": 3016
    },
    {
      "epoch": 1.6152610441767068,
      "grad_norm": 0.7351414561271667,
      "learning_rate": 9.236259814418274e-05,
      "loss": 0.095,
      "step": 3017
    },
    {
      "epoch": 1.6157965194109773,
      "grad_norm": 0.4202674329280853,
      "learning_rate": 9.232690935046396e-05,
      "loss": 0.0684,
      "step": 3018
    },
    {
      "epoch": 1.6163319946452477,
      "grad_norm": 0.974124014377594,
      "learning_rate": 9.229122055674518e-05,
      "loss": 0.0634,
      "step": 3019
    },
    {
      "epoch": 1.616867469879518,
      "grad_norm": 0.6413552761077881,
      "learning_rate": 9.225553176302641e-05,
      "loss": 0.0857,
      "step": 3020
    },
    {
      "epoch": 1.6174029451137883,
      "grad_norm": 0.44318485260009766,
      "learning_rate": 9.221984296930764e-05,
      "loss": 0.0445,
      "step": 3021
    },
    {
      "epoch": 1.617938420348059,
      "grad_norm": 0.9616793394088745,
      "learning_rate": 9.218415417558886e-05,
      "loss": 0.0873,
      "step": 3022
    },
    {
      "epoch": 1.6184738955823295,
      "grad_norm": 0.7885798215866089,
      "learning_rate": 9.21484653818701e-05,
      "loss": 0.1386,
      "step": 3023
    },
    {
      "epoch": 1.6190093708165998,
      "grad_norm": 0.2862757444381714,
      "learning_rate": 9.211277658815133e-05,
      "loss": 0.0207,
      "step": 3024
    },
    {
      "epoch": 1.6195448460508701,
      "grad_norm": 0.4508286714553833,
      "learning_rate": 9.207708779443255e-05,
      "loss": 0.0846,
      "step": 3025
    },
    {
      "epoch": 1.6200803212851405,
      "grad_norm": 0.38197532296180725,
      "learning_rate": 9.204139900071378e-05,
      "loss": 0.0554,
      "step": 3026
    },
    {
      "epoch": 1.620615796519411,
      "grad_norm": 1.0078456401824951,
      "learning_rate": 9.2005710206995e-05,
      "loss": 0.1266,
      "step": 3027
    },
    {
      "epoch": 1.6211512717536813,
      "grad_norm": 1.2754032611846924,
      "learning_rate": 9.197002141327622e-05,
      "loss": 0.1485,
      "step": 3028
    },
    {
      "epoch": 1.621686746987952,
      "grad_norm": 1.4049514532089233,
      "learning_rate": 9.193433261955746e-05,
      "loss": 0.1471,
      "step": 3029
    },
    {
      "epoch": 1.6222222222222222,
      "grad_norm": 0.6967653036117554,
      "learning_rate": 9.189864382583869e-05,
      "loss": 0.0473,
      "step": 3030
    },
    {
      "epoch": 1.6227576974564926,
      "grad_norm": 0.31096798181533813,
      "learning_rate": 9.186295503211992e-05,
      "loss": 0.0078,
      "step": 3031
    },
    {
      "epoch": 1.623293172690763,
      "grad_norm": 0.6235267519950867,
      "learning_rate": 9.182726623840114e-05,
      "loss": 0.0866,
      "step": 3032
    },
    {
      "epoch": 1.6238286479250335,
      "grad_norm": 2.316110610961914,
      "learning_rate": 9.179157744468238e-05,
      "loss": 0.049,
      "step": 3033
    },
    {
      "epoch": 1.624364123159304,
      "grad_norm": 1.4760560989379883,
      "learning_rate": 9.175588865096361e-05,
      "loss": 0.1433,
      "step": 3034
    },
    {
      "epoch": 1.6248995983935743,
      "grad_norm": 1.4364066123962402,
      "learning_rate": 9.172019985724483e-05,
      "loss": 0.1184,
      "step": 3035
    },
    {
      "epoch": 1.6254350736278447,
      "grad_norm": 0.6972483396530151,
      "learning_rate": 9.168451106352605e-05,
      "loss": 0.0575,
      "step": 3036
    },
    {
      "epoch": 1.625970548862115,
      "grad_norm": 0.725114643573761,
      "learning_rate": 9.164882226980728e-05,
      "loss": 0.0794,
      "step": 3037
    },
    {
      "epoch": 1.6265060240963856,
      "grad_norm": 0.4400692582130432,
      "learning_rate": 9.16131334760885e-05,
      "loss": 0.0394,
      "step": 3038
    },
    {
      "epoch": 1.6270414993306561,
      "grad_norm": 0.9012240171432495,
      "learning_rate": 9.157744468236974e-05,
      "loss": 0.0965,
      "step": 3039
    },
    {
      "epoch": 1.6275769745649264,
      "grad_norm": 0.9044619798660278,
      "learning_rate": 9.154175588865097e-05,
      "loss": 0.0677,
      "step": 3040
    },
    {
      "epoch": 1.6281124497991968,
      "grad_norm": 0.48976707458496094,
      "learning_rate": 9.150606709493219e-05,
      "loss": 0.0565,
      "step": 3041
    },
    {
      "epoch": 1.6286479250334671,
      "grad_norm": 2.8251867294311523,
      "learning_rate": 9.147037830121342e-05,
      "loss": 0.1294,
      "step": 3042
    },
    {
      "epoch": 1.6291834002677377,
      "grad_norm": 0.7115849852561951,
      "learning_rate": 9.143468950749466e-05,
      "loss": 0.0828,
      "step": 3043
    },
    {
      "epoch": 1.629718875502008,
      "grad_norm": 1.2236266136169434,
      "learning_rate": 9.139900071377588e-05,
      "loss": 0.1917,
      "step": 3044
    },
    {
      "epoch": 1.6302543507362786,
      "grad_norm": 1.029321312904358,
      "learning_rate": 9.13633119200571e-05,
      "loss": 0.1622,
      "step": 3045
    },
    {
      "epoch": 1.630789825970549,
      "grad_norm": 0.8667240738868713,
      "learning_rate": 9.132762312633833e-05,
      "loss": 0.1074,
      "step": 3046
    },
    {
      "epoch": 1.6313253012048192,
      "grad_norm": 0.8674018979072571,
      "learning_rate": 9.129193433261956e-05,
      "loss": 0.1781,
      "step": 3047
    },
    {
      "epoch": 1.6318607764390896,
      "grad_norm": 0.7474344968795776,
      "learning_rate": 9.125624553890078e-05,
      "loss": 0.1171,
      "step": 3048
    },
    {
      "epoch": 1.63239625167336,
      "grad_norm": 1.0498069524765015,
      "learning_rate": 9.122055674518202e-05,
      "loss": 0.1571,
      "step": 3049
    },
    {
      "epoch": 1.6329317269076307,
      "grad_norm": 1.0235086679458618,
      "learning_rate": 9.118486795146325e-05,
      "loss": 0.1108,
      "step": 3050
    },
    {
      "epoch": 1.633467202141901,
      "grad_norm": 0.6778652667999268,
      "learning_rate": 9.114917915774447e-05,
      "loss": 0.0831,
      "step": 3051
    },
    {
      "epoch": 1.6340026773761713,
      "grad_norm": 0.783427357673645,
      "learning_rate": 9.11134903640257e-05,
      "loss": 0.121,
      "step": 3052
    },
    {
      "epoch": 1.6345381526104417,
      "grad_norm": 0.576237678527832,
      "learning_rate": 9.107780157030692e-05,
      "loss": 0.0598,
      "step": 3053
    },
    {
      "epoch": 1.6350736278447122,
      "grad_norm": 0.8373557329177856,
      "learning_rate": 9.104211277658816e-05,
      "loss": 0.0379,
      "step": 3054
    },
    {
      "epoch": 1.6356091030789826,
      "grad_norm": 0.7162013053894043,
      "learning_rate": 9.100642398286938e-05,
      "loss": 0.0921,
      "step": 3055
    },
    {
      "epoch": 1.636144578313253,
      "grad_norm": 0.6325794458389282,
      "learning_rate": 9.097073518915061e-05,
      "loss": 0.1235,
      "step": 3056
    },
    {
      "epoch": 1.6366800535475234,
      "grad_norm": 0.6133553981781006,
      "learning_rate": 9.093504639543183e-05,
      "loss": 0.0691,
      "step": 3057
    },
    {
      "epoch": 1.6372155287817938,
      "grad_norm": 0.8279920816421509,
      "learning_rate": 9.089935760171306e-05,
      "loss": 0.0865,
      "step": 3058
    },
    {
      "epoch": 1.637751004016064,
      "grad_norm": 0.9591081738471985,
      "learning_rate": 9.08636688079943e-05,
      "loss": 0.1441,
      "step": 3059
    },
    {
      "epoch": 1.6382864792503347,
      "grad_norm": 0.9104008674621582,
      "learning_rate": 9.082798001427553e-05,
      "loss": 0.1299,
      "step": 3060
    },
    {
      "epoch": 1.6388219544846052,
      "grad_norm": 0.600635826587677,
      "learning_rate": 9.079229122055675e-05,
      "loss": 0.1056,
      "step": 3061
    },
    {
      "epoch": 1.6393574297188755,
      "grad_norm": 0.5761138200759888,
      "learning_rate": 9.075660242683798e-05,
      "loss": 0.0923,
      "step": 3062
    },
    {
      "epoch": 1.6398929049531459,
      "grad_norm": 0.4753345251083374,
      "learning_rate": 9.07209136331192e-05,
      "loss": 0.0572,
      "step": 3063
    },
    {
      "epoch": 1.6404283801874162,
      "grad_norm": 0.7583678364753723,
      "learning_rate": 9.068522483940042e-05,
      "loss": 0.1336,
      "step": 3064
    },
    {
      "epoch": 1.6409638554216868,
      "grad_norm": 0.6611965298652649,
      "learning_rate": 9.064953604568166e-05,
      "loss": 0.1353,
      "step": 3065
    },
    {
      "epoch": 1.6414993306559573,
      "grad_norm": 0.49598434567451477,
      "learning_rate": 9.061384725196289e-05,
      "loss": 0.0637,
      "step": 3066
    },
    {
      "epoch": 1.6420348058902277,
      "grad_norm": 1.3075878620147705,
      "learning_rate": 9.057815845824411e-05,
      "loss": 0.1429,
      "step": 3067
    },
    {
      "epoch": 1.642570281124498,
      "grad_norm": 2.8622894287109375,
      "learning_rate": 9.054246966452534e-05,
      "loss": 0.0811,
      "step": 3068
    },
    {
      "epoch": 1.6431057563587683,
      "grad_norm": 0.837485134601593,
      "learning_rate": 9.050678087080658e-05,
      "loss": 0.119,
      "step": 3069
    },
    {
      "epoch": 1.6436412315930389,
      "grad_norm": 0.7530585527420044,
      "learning_rate": 9.04710920770878e-05,
      "loss": 0.1359,
      "step": 3070
    },
    {
      "epoch": 1.6441767068273092,
      "grad_norm": 0.554938554763794,
      "learning_rate": 9.043540328336903e-05,
      "loss": 0.0527,
      "step": 3071
    },
    {
      "epoch": 1.6447121820615798,
      "grad_norm": 0.4572088420391083,
      "learning_rate": 9.039971448965025e-05,
      "loss": 0.0836,
      "step": 3072
    },
    {
      "epoch": 1.64524765729585,
      "grad_norm": 0.7928427457809448,
      "learning_rate": 9.036402569593148e-05,
      "loss": 0.1285,
      "step": 3073
    },
    {
      "epoch": 1.6457831325301204,
      "grad_norm": 0.2937142550945282,
      "learning_rate": 9.03283369022127e-05,
      "loss": 0.0315,
      "step": 3074
    },
    {
      "epoch": 1.6463186077643908,
      "grad_norm": 0.6526965498924255,
      "learning_rate": 9.029264810849394e-05,
      "loss": 0.1018,
      "step": 3075
    },
    {
      "epoch": 1.6468540829986613,
      "grad_norm": 0.7035512328147888,
      "learning_rate": 9.025695931477517e-05,
      "loss": 0.0518,
      "step": 3076
    },
    {
      "epoch": 1.6473895582329319,
      "grad_norm": 0.600853443145752,
      "learning_rate": 9.022127052105639e-05,
      "loss": 0.0883,
      "step": 3077
    },
    {
      "epoch": 1.6479250334672022,
      "grad_norm": 0.458636999130249,
      "learning_rate": 9.018558172733762e-05,
      "loss": 0.0639,
      "step": 3078
    },
    {
      "epoch": 1.6484605087014725,
      "grad_norm": 0.5364812612533569,
      "learning_rate": 9.014989293361886e-05,
      "loss": 0.0825,
      "step": 3079
    },
    {
      "epoch": 1.6489959839357429,
      "grad_norm": 0.5890099406242371,
      "learning_rate": 9.011420413990008e-05,
      "loss": 0.0824,
      "step": 3080
    },
    {
      "epoch": 1.6495314591700134,
      "grad_norm": 0.7456043362617493,
      "learning_rate": 9.00785153461813e-05,
      "loss": 0.0684,
      "step": 3081
    },
    {
      "epoch": 1.6500669344042838,
      "grad_norm": 1.2054407596588135,
      "learning_rate": 9.004282655246253e-05,
      "loss": 0.1727,
      "step": 3082
    },
    {
      "epoch": 1.6506024096385543,
      "grad_norm": 0.5491015911102295,
      "learning_rate": 9.000713775874375e-05,
      "loss": 0.0932,
      "step": 3083
    },
    {
      "epoch": 1.6511378848728246,
      "grad_norm": 4.448696613311768,
      "learning_rate": 8.997144896502498e-05,
      "loss": 0.0472,
      "step": 3084
    },
    {
      "epoch": 1.651673360107095,
      "grad_norm": 0.46270614862442017,
      "learning_rate": 8.993576017130622e-05,
      "loss": 0.0598,
      "step": 3085
    },
    {
      "epoch": 1.6522088353413653,
      "grad_norm": 0.33223092555999756,
      "learning_rate": 8.990007137758745e-05,
      "loss": 0.023,
      "step": 3086
    },
    {
      "epoch": 1.6527443105756359,
      "grad_norm": 0.7650120854377747,
      "learning_rate": 8.986438258386867e-05,
      "loss": 0.1341,
      "step": 3087
    },
    {
      "epoch": 1.6532797858099064,
      "grad_norm": 0.7718417644500732,
      "learning_rate": 8.98286937901499e-05,
      "loss": 0.0764,
      "step": 3088
    },
    {
      "epoch": 1.6538152610441768,
      "grad_norm": 0.9377773404121399,
      "learning_rate": 8.979300499643112e-05,
      "loss": 0.109,
      "step": 3089
    },
    {
      "epoch": 1.654350736278447,
      "grad_norm": 0.8032447695732117,
      "learning_rate": 8.975731620271234e-05,
      "loss": 0.1211,
      "step": 3090
    },
    {
      "epoch": 1.6548862115127174,
      "grad_norm": 0.5435712337493896,
      "learning_rate": 8.972162740899358e-05,
      "loss": 0.0564,
      "step": 3091
    },
    {
      "epoch": 1.655421686746988,
      "grad_norm": 0.7260092496871948,
      "learning_rate": 8.968593861527481e-05,
      "loss": 0.0813,
      "step": 3092
    },
    {
      "epoch": 1.6559571619812585,
      "grad_norm": 0.3456580340862274,
      "learning_rate": 8.965024982155603e-05,
      "loss": 0.0613,
      "step": 3093
    },
    {
      "epoch": 1.6564926372155289,
      "grad_norm": 0.5927234888076782,
      "learning_rate": 8.961456102783726e-05,
      "loss": 0.0943,
      "step": 3094
    },
    {
      "epoch": 1.6570281124497992,
      "grad_norm": 0.4884868562221527,
      "learning_rate": 8.95788722341185e-05,
      "loss": 0.0343,
      "step": 3095
    },
    {
      "epoch": 1.6575635876840695,
      "grad_norm": 0.7773914933204651,
      "learning_rate": 8.954318344039972e-05,
      "loss": 0.0881,
      "step": 3096
    },
    {
      "epoch": 1.6580990629183399,
      "grad_norm": 0.6454458236694336,
      "learning_rate": 8.950749464668095e-05,
      "loss": 0.0751,
      "step": 3097
    },
    {
      "epoch": 1.6586345381526104,
      "grad_norm": 3.233389139175415,
      "learning_rate": 8.947180585296217e-05,
      "loss": 0.1475,
      "step": 3098
    },
    {
      "epoch": 1.659170013386881,
      "grad_norm": 0.9279952645301819,
      "learning_rate": 8.94361170592434e-05,
      "loss": 0.081,
      "step": 3099
    },
    {
      "epoch": 1.6597054886211513,
      "grad_norm": 0.431065171957016,
      "learning_rate": 8.940042826552462e-05,
      "loss": 0.0405,
      "step": 3100
    },
    {
      "epoch": 1.6602409638554216,
      "grad_norm": 5.9717254638671875,
      "learning_rate": 8.936473947180586e-05,
      "loss": 0.421,
      "step": 3101
    },
    {
      "epoch": 1.660776439089692,
      "grad_norm": 1.1730188131332397,
      "learning_rate": 8.932905067808709e-05,
      "loss": 0.0416,
      "step": 3102
    },
    {
      "epoch": 1.6613119143239625,
      "grad_norm": 0.7101839184761047,
      "learning_rate": 8.929336188436831e-05,
      "loss": 0.0474,
      "step": 3103
    },
    {
      "epoch": 1.661847389558233,
      "grad_norm": 1.3927552700042725,
      "learning_rate": 8.925767309064954e-05,
      "loss": 0.1754,
      "step": 3104
    },
    {
      "epoch": 1.6623828647925034,
      "grad_norm": 1.0249823331832886,
      "learning_rate": 8.922198429693078e-05,
      "loss": 0.1062,
      "step": 3105
    },
    {
      "epoch": 1.6629183400267737,
      "grad_norm": 0.9264635443687439,
      "learning_rate": 8.9186295503212e-05,
      "loss": 0.0657,
      "step": 3106
    },
    {
      "epoch": 1.663453815261044,
      "grad_norm": 0.997342050075531,
      "learning_rate": 8.915060670949322e-05,
      "loss": 0.0673,
      "step": 3107
    },
    {
      "epoch": 1.6639892904953146,
      "grad_norm": 0.8654842972755432,
      "learning_rate": 8.911491791577445e-05,
      "loss": 0.0921,
      "step": 3108
    },
    {
      "epoch": 1.664524765729585,
      "grad_norm": 0.5933557748794556,
      "learning_rate": 8.907922912205567e-05,
      "loss": 0.0625,
      "step": 3109
    },
    {
      "epoch": 1.6650602409638555,
      "grad_norm": 1.1223702430725098,
      "learning_rate": 8.90435403283369e-05,
      "loss": 0.0721,
      "step": 3110
    },
    {
      "epoch": 1.6655957161981259,
      "grad_norm": 0.7867531776428223,
      "learning_rate": 8.900785153461814e-05,
      "loss": 0.0888,
      "step": 3111
    },
    {
      "epoch": 1.6661311914323962,
      "grad_norm": 0.7534082531929016,
      "learning_rate": 8.897216274089936e-05,
      "loss": 0.0643,
      "step": 3112
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 1.195847988128662,
      "learning_rate": 8.893647394718059e-05,
      "loss": 0.0699,
      "step": 3113
    },
    {
      "epoch": 1.667202141900937,
      "grad_norm": 1.0984441041946411,
      "learning_rate": 8.890078515346182e-05,
      "loss": 0.1058,
      "step": 3114
    },
    {
      "epoch": 1.6677376171352076,
      "grad_norm": 0.46153780817985535,
      "learning_rate": 8.886509635974304e-05,
      "loss": 0.0389,
      "step": 3115
    },
    {
      "epoch": 1.668273092369478,
      "grad_norm": 0.7132722735404968,
      "learning_rate": 8.882940756602426e-05,
      "loss": 0.1267,
      "step": 3116
    },
    {
      "epoch": 1.6688085676037483,
      "grad_norm": 1.3281564712524414,
      "learning_rate": 8.87937187723055e-05,
      "loss": 0.1555,
      "step": 3117
    },
    {
      "epoch": 1.6693440428380186,
      "grad_norm": 0.7569901943206787,
      "learning_rate": 8.875802997858673e-05,
      "loss": 0.13,
      "step": 3118
    },
    {
      "epoch": 1.6698795180722892,
      "grad_norm": 0.25573116540908813,
      "learning_rate": 8.872234118486795e-05,
      "loss": 0.0111,
      "step": 3119
    },
    {
      "epoch": 1.6704149933065597,
      "grad_norm": 2.547943353652954,
      "learning_rate": 8.868665239114918e-05,
      "loss": 0.1766,
      "step": 3120
    },
    {
      "epoch": 1.67095046854083,
      "grad_norm": 0.49354299902915955,
      "learning_rate": 8.865096359743042e-05,
      "loss": 0.0465,
      "step": 3121
    },
    {
      "epoch": 1.6714859437751004,
      "grad_norm": 1.3743730783462524,
      "learning_rate": 8.861527480371164e-05,
      "loss": 0.1297,
      "step": 3122
    },
    {
      "epoch": 1.6720214190093707,
      "grad_norm": 1.4239556789398193,
      "learning_rate": 8.857958600999287e-05,
      "loss": 0.1321,
      "step": 3123
    },
    {
      "epoch": 1.672556894243641,
      "grad_norm": 0.969683051109314,
      "learning_rate": 8.854389721627409e-05,
      "loss": 0.1126,
      "step": 3124
    },
    {
      "epoch": 1.6730923694779116,
      "grad_norm": 4.03809118270874,
      "learning_rate": 8.850820842255531e-05,
      "loss": 0.0833,
      "step": 3125
    },
    {
      "epoch": 1.6736278447121822,
      "grad_norm": 0.8337602019309998,
      "learning_rate": 8.847251962883654e-05,
      "loss": 0.1627,
      "step": 3126
    },
    {
      "epoch": 1.6741633199464525,
      "grad_norm": 1.1220030784606934,
      "learning_rate": 8.843683083511778e-05,
      "loss": 0.1466,
      "step": 3127
    },
    {
      "epoch": 1.6746987951807228,
      "grad_norm": 0.8829960823059082,
      "learning_rate": 8.840114204139901e-05,
      "loss": 0.0502,
      "step": 3128
    },
    {
      "epoch": 1.6752342704149932,
      "grad_norm": 1.132493495941162,
      "learning_rate": 8.836545324768023e-05,
      "loss": 0.1203,
      "step": 3129
    },
    {
      "epoch": 1.6757697456492637,
      "grad_norm": 0.6891895532608032,
      "learning_rate": 8.832976445396146e-05,
      "loss": 0.0829,
      "step": 3130
    },
    {
      "epoch": 1.6763052208835343,
      "grad_norm": 0.86005038022995,
      "learning_rate": 8.82940756602427e-05,
      "loss": 0.0686,
      "step": 3131
    },
    {
      "epoch": 1.6768406961178046,
      "grad_norm": 5.506687641143799,
      "learning_rate": 8.825838686652392e-05,
      "loss": 0.0767,
      "step": 3132
    },
    {
      "epoch": 1.677376171352075,
      "grad_norm": 0.5862424969673157,
      "learning_rate": 8.822269807280514e-05,
      "loss": 0.1044,
      "step": 3133
    },
    {
      "epoch": 1.6779116465863453,
      "grad_norm": 0.9228909611701965,
      "learning_rate": 8.818700927908637e-05,
      "loss": 0.1379,
      "step": 3134
    },
    {
      "epoch": 1.6784471218206158,
      "grad_norm": 0.7826868891716003,
      "learning_rate": 8.815132048536759e-05,
      "loss": 0.061,
      "step": 3135
    },
    {
      "epoch": 1.6789825970548862,
      "grad_norm": 1.4465973377227783,
      "learning_rate": 8.811563169164882e-05,
      "loss": 0.0787,
      "step": 3136
    },
    {
      "epoch": 1.6795180722891567,
      "grad_norm": 0.5907291769981384,
      "learning_rate": 8.807994289793006e-05,
      "loss": 0.0811,
      "step": 3137
    },
    {
      "epoch": 1.680053547523427,
      "grad_norm": 1.4649677276611328,
      "learning_rate": 8.804425410421128e-05,
      "loss": 0.1182,
      "step": 3138
    },
    {
      "epoch": 1.6805890227576974,
      "grad_norm": 4.963868618011475,
      "learning_rate": 8.800856531049251e-05,
      "loss": 0.2008,
      "step": 3139
    },
    {
      "epoch": 1.6811244979919677,
      "grad_norm": 1.1783998012542725,
      "learning_rate": 8.797287651677374e-05,
      "loss": 0.0757,
      "step": 3140
    },
    {
      "epoch": 1.6816599732262383,
      "grad_norm": 1.8379451036453247,
      "learning_rate": 8.793718772305496e-05,
      "loss": 0.1597,
      "step": 3141
    },
    {
      "epoch": 1.6821954484605088,
      "grad_norm": 0.6211150288581848,
      "learning_rate": 8.790149892933618e-05,
      "loss": 0.0588,
      "step": 3142
    },
    {
      "epoch": 1.6827309236947792,
      "grad_norm": 1.2952162027359009,
      "learning_rate": 8.786581013561742e-05,
      "loss": 0.1158,
      "step": 3143
    },
    {
      "epoch": 1.6832663989290495,
      "grad_norm": 1.1525697708129883,
      "learning_rate": 8.783012134189865e-05,
      "loss": 0.0791,
      "step": 3144
    },
    {
      "epoch": 1.6838018741633198,
      "grad_norm": 0.9291431903839111,
      "learning_rate": 8.779443254817987e-05,
      "loss": 0.1015,
      "step": 3145
    },
    {
      "epoch": 1.6843373493975904,
      "grad_norm": 3.489542007446289,
      "learning_rate": 8.77587437544611e-05,
      "loss": 0.0594,
      "step": 3146
    },
    {
      "epoch": 1.6848728246318607,
      "grad_norm": 0.988029420375824,
      "learning_rate": 8.772305496074234e-05,
      "loss": 0.1072,
      "step": 3147
    },
    {
      "epoch": 1.6854082998661313,
      "grad_norm": 0.9618769884109497,
      "learning_rate": 8.768736616702356e-05,
      "loss": 0.1514,
      "step": 3148
    },
    {
      "epoch": 1.6859437751004016,
      "grad_norm": 0.8752390742301941,
      "learning_rate": 8.765167737330479e-05,
      "loss": 0.1343,
      "step": 3149
    },
    {
      "epoch": 1.686479250334672,
      "grad_norm": 0.9857107996940613,
      "learning_rate": 8.761598857958601e-05,
      "loss": 0.0547,
      "step": 3150
    },
    {
      "epoch": 1.6870147255689423,
      "grad_norm": 1.287893533706665,
      "learning_rate": 8.758029978586723e-05,
      "loss": 0.1802,
      "step": 3151
    },
    {
      "epoch": 1.6875502008032128,
      "grad_norm": 1.2050979137420654,
      "learning_rate": 8.754461099214846e-05,
      "loss": 0.1493,
      "step": 3152
    },
    {
      "epoch": 1.6880856760374834,
      "grad_norm": 5.63716459274292,
      "learning_rate": 8.75089221984297e-05,
      "loss": 0.1255,
      "step": 3153
    },
    {
      "epoch": 1.6886211512717537,
      "grad_norm": 0.6982221007347107,
      "learning_rate": 8.747323340471093e-05,
      "loss": 0.0555,
      "step": 3154
    },
    {
      "epoch": 1.689156626506024,
      "grad_norm": 1.2287209033966064,
      "learning_rate": 8.743754461099215e-05,
      "loss": 0.1057,
      "step": 3155
    },
    {
      "epoch": 1.6896921017402944,
      "grad_norm": 0.749467134475708,
      "learning_rate": 8.740185581727338e-05,
      "loss": 0.1102,
      "step": 3156
    },
    {
      "epoch": 1.690227576974565,
      "grad_norm": 0.5268260836601257,
      "learning_rate": 8.736616702355462e-05,
      "loss": 0.0449,
      "step": 3157
    },
    {
      "epoch": 1.6907630522088355,
      "grad_norm": 0.8937445878982544,
      "learning_rate": 8.733047822983584e-05,
      "loss": 0.1667,
      "step": 3158
    },
    {
      "epoch": 1.6912985274431058,
      "grad_norm": 0.9738617539405823,
      "learning_rate": 8.729478943611706e-05,
      "loss": 0.0761,
      "step": 3159
    },
    {
      "epoch": 1.6918340026773762,
      "grad_norm": 1.4319963455200195,
      "learning_rate": 8.725910064239829e-05,
      "loss": 0.0809,
      "step": 3160
    },
    {
      "epoch": 1.6923694779116465,
      "grad_norm": 0.5164647698402405,
      "learning_rate": 8.722341184867951e-05,
      "loss": 0.0726,
      "step": 3161
    },
    {
      "epoch": 1.692904953145917,
      "grad_norm": 1.062221884727478,
      "learning_rate": 8.718772305496074e-05,
      "loss": 0.138,
      "step": 3162
    },
    {
      "epoch": 1.6934404283801874,
      "grad_norm": 0.6494451761245728,
      "learning_rate": 8.715203426124198e-05,
      "loss": 0.0791,
      "step": 3163
    },
    {
      "epoch": 1.693975903614458,
      "grad_norm": 0.9101865887641907,
      "learning_rate": 8.71163454675232e-05,
      "loss": 0.08,
      "step": 3164
    },
    {
      "epoch": 1.6945113788487283,
      "grad_norm": 1.0583208799362183,
      "learning_rate": 8.708065667380443e-05,
      "loss": 0.0567,
      "step": 3165
    },
    {
      "epoch": 1.6950468540829986,
      "grad_norm": 0.5038831233978271,
      "learning_rate": 8.704496788008566e-05,
      "loss": 0.0276,
      "step": 3166
    },
    {
      "epoch": 1.695582329317269,
      "grad_norm": 1.0706026554107666,
      "learning_rate": 8.700927908636688e-05,
      "loss": 0.1128,
      "step": 3167
    },
    {
      "epoch": 1.6961178045515395,
      "grad_norm": 0.8774222731590271,
      "learning_rate": 8.69735902926481e-05,
      "loss": 0.1375,
      "step": 3168
    },
    {
      "epoch": 1.69665327978581,
      "grad_norm": 0.9357107281684875,
      "learning_rate": 8.693790149892934e-05,
      "loss": 0.1895,
      "step": 3169
    },
    {
      "epoch": 1.6971887550200804,
      "grad_norm": 0.5987645983695984,
      "learning_rate": 8.690221270521057e-05,
      "loss": 0.0838,
      "step": 3170
    },
    {
      "epoch": 1.6977242302543507,
      "grad_norm": 0.5110834240913391,
      "learning_rate": 8.686652391149179e-05,
      "loss": 0.0809,
      "step": 3171
    },
    {
      "epoch": 1.698259705488621,
      "grad_norm": 1.0188411474227905,
      "learning_rate": 8.683083511777302e-05,
      "loss": 0.1415,
      "step": 3172
    },
    {
      "epoch": 1.6987951807228916,
      "grad_norm": 0.7801288962364197,
      "learning_rate": 8.679514632405426e-05,
      "loss": 0.0861,
      "step": 3173
    },
    {
      "epoch": 1.699330655957162,
      "grad_norm": 1.5938061475753784,
      "learning_rate": 8.675945753033548e-05,
      "loss": 0.1382,
      "step": 3174
    },
    {
      "epoch": 1.6998661311914325,
      "grad_norm": 0.7587681412696838,
      "learning_rate": 8.672376873661671e-05,
      "loss": 0.084,
      "step": 3175
    },
    {
      "epoch": 1.7004016064257028,
      "grad_norm": 1.0753813982009888,
      "learning_rate": 8.668807994289793e-05,
      "loss": 0.1171,
      "step": 3176
    },
    {
      "epoch": 1.7009370816599731,
      "grad_norm": 0.6209518909454346,
      "learning_rate": 8.665239114917915e-05,
      "loss": 0.0262,
      "step": 3177
    },
    {
      "epoch": 1.7014725568942435,
      "grad_norm": 1.1018474102020264,
      "learning_rate": 8.661670235546038e-05,
      "loss": 0.105,
      "step": 3178
    },
    {
      "epoch": 1.702008032128514,
      "grad_norm": 1.5857428312301636,
      "learning_rate": 8.658101356174162e-05,
      "loss": 0.1165,
      "step": 3179
    },
    {
      "epoch": 1.7025435073627846,
      "grad_norm": 0.5801267623901367,
      "learning_rate": 8.654532476802284e-05,
      "loss": 0.0544,
      "step": 3180
    },
    {
      "epoch": 1.703078982597055,
      "grad_norm": 0.7323088049888611,
      "learning_rate": 8.650963597430407e-05,
      "loss": 0.0557,
      "step": 3181
    },
    {
      "epoch": 1.7036144578313253,
      "grad_norm": 0.8331407904624939,
      "learning_rate": 8.64739471805853e-05,
      "loss": 0.1058,
      "step": 3182
    },
    {
      "epoch": 1.7041499330655956,
      "grad_norm": 0.9189674854278564,
      "learning_rate": 8.643825838686654e-05,
      "loss": 0.1235,
      "step": 3183
    },
    {
      "epoch": 1.7046854082998661,
      "grad_norm": 3.893282651901245,
      "learning_rate": 8.640256959314776e-05,
      "loss": 0.1046,
      "step": 3184
    },
    {
      "epoch": 1.7052208835341367,
      "grad_norm": 0.8306700587272644,
      "learning_rate": 8.636688079942898e-05,
      "loss": 0.1013,
      "step": 3185
    },
    {
      "epoch": 1.705756358768407,
      "grad_norm": 0.5614030957221985,
      "learning_rate": 8.633119200571021e-05,
      "loss": 0.0684,
      "step": 3186
    },
    {
      "epoch": 1.7062918340026774,
      "grad_norm": 1.1957412958145142,
      "learning_rate": 8.629550321199143e-05,
      "loss": 0.065,
      "step": 3187
    },
    {
      "epoch": 1.7068273092369477,
      "grad_norm": 1.2707650661468506,
      "learning_rate": 8.625981441827266e-05,
      "loss": 0.125,
      "step": 3188
    },
    {
      "epoch": 1.7073627844712183,
      "grad_norm": 0.5874441266059875,
      "learning_rate": 8.62241256245539e-05,
      "loss": 0.0855,
      "step": 3189
    },
    {
      "epoch": 1.7078982597054886,
      "grad_norm": 0.7169203758239746,
      "learning_rate": 8.618843683083512e-05,
      "loss": 0.1205,
      "step": 3190
    },
    {
      "epoch": 1.7084337349397591,
      "grad_norm": 0.6886670589447021,
      "learning_rate": 8.615274803711635e-05,
      "loss": 0.0386,
      "step": 3191
    },
    {
      "epoch": 1.7089692101740295,
      "grad_norm": 0.531261146068573,
      "learning_rate": 8.611705924339758e-05,
      "loss": 0.0606,
      "step": 3192
    },
    {
      "epoch": 1.7095046854082998,
      "grad_norm": 0.7446401715278625,
      "learning_rate": 8.60813704496788e-05,
      "loss": 0.0666,
      "step": 3193
    },
    {
      "epoch": 1.7100401606425701,
      "grad_norm": 0.5568779110908508,
      "learning_rate": 8.604568165596004e-05,
      "loss": 0.0366,
      "step": 3194
    },
    {
      "epoch": 1.7105756358768407,
      "grad_norm": 0.8515545129776001,
      "learning_rate": 8.600999286224126e-05,
      "loss": 0.0641,
      "step": 3195
    },
    {
      "epoch": 1.7111111111111112,
      "grad_norm": 0.5692749619483948,
      "learning_rate": 8.597430406852249e-05,
      "loss": 0.1003,
      "step": 3196
    },
    {
      "epoch": 1.7116465863453816,
      "grad_norm": 1.1370265483856201,
      "learning_rate": 8.593861527480371e-05,
      "loss": 0.2028,
      "step": 3197
    },
    {
      "epoch": 1.712182061579652,
      "grad_norm": 0.2653034031391144,
      "learning_rate": 8.590292648108494e-05,
      "loss": 0.0167,
      "step": 3198
    },
    {
      "epoch": 1.7127175368139222,
      "grad_norm": 0.8310642838478088,
      "learning_rate": 8.586723768736618e-05,
      "loss": 0.1631,
      "step": 3199
    },
    {
      "epoch": 1.7132530120481928,
      "grad_norm": 0.6722150444984436,
      "learning_rate": 8.58315488936474e-05,
      "loss": 0.0767,
      "step": 3200
    },
    {
      "epoch": 1.7137884872824631,
      "grad_norm": 0.8806473016738892,
      "learning_rate": 8.579586009992863e-05,
      "loss": 0.0739,
      "step": 3201
    },
    {
      "epoch": 1.7143239625167337,
      "grad_norm": 0.8399044275283813,
      "learning_rate": 8.576017130620986e-05,
      "loss": 0.0878,
      "step": 3202
    },
    {
      "epoch": 1.714859437751004,
      "grad_norm": 1.2685699462890625,
      "learning_rate": 8.572448251249108e-05,
      "loss": 0.1664,
      "step": 3203
    },
    {
      "epoch": 1.7153949129852744,
      "grad_norm": 0.7209412455558777,
      "learning_rate": 8.56887937187723e-05,
      "loss": 0.0902,
      "step": 3204
    },
    {
      "epoch": 1.7159303882195447,
      "grad_norm": 0.45564597845077515,
      "learning_rate": 8.565310492505354e-05,
      "loss": 0.0433,
      "step": 3205
    },
    {
      "epoch": 1.7164658634538152,
      "grad_norm": 1.4271246194839478,
      "learning_rate": 8.561741613133476e-05,
      "loss": 0.0293,
      "step": 3206
    },
    {
      "epoch": 1.7170013386880858,
      "grad_norm": 0.6416117548942566,
      "learning_rate": 8.558172733761599e-05,
      "loss": 0.0324,
      "step": 3207
    },
    {
      "epoch": 1.7175368139223561,
      "grad_norm": 1.0209711790084839,
      "learning_rate": 8.554603854389722e-05,
      "loss": 0.0922,
      "step": 3208
    },
    {
      "epoch": 1.7180722891566265,
      "grad_norm": 0.8807413578033447,
      "learning_rate": 8.551034975017846e-05,
      "loss": 0.0682,
      "step": 3209
    },
    {
      "epoch": 1.7186077643908968,
      "grad_norm": 1.2161552906036377,
      "learning_rate": 8.547466095645968e-05,
      "loss": 0.0959,
      "step": 3210
    },
    {
      "epoch": 1.7191432396251674,
      "grad_norm": 1.470617413520813,
      "learning_rate": 8.543897216274091e-05,
      "loss": 0.149,
      "step": 3211
    },
    {
      "epoch": 1.719678714859438,
      "grad_norm": 0.9356474876403809,
      "learning_rate": 8.540328336902213e-05,
      "loss": 0.1033,
      "step": 3212
    },
    {
      "epoch": 1.7202141900937082,
      "grad_norm": 1.6326677799224854,
      "learning_rate": 8.536759457530335e-05,
      "loss": 0.0853,
      "step": 3213
    },
    {
      "epoch": 1.7207496653279786,
      "grad_norm": 0.9030808806419373,
      "learning_rate": 8.533190578158458e-05,
      "loss": 0.0888,
      "step": 3214
    },
    {
      "epoch": 1.721285140562249,
      "grad_norm": 0.9311928153038025,
      "learning_rate": 8.529621698786582e-05,
      "loss": 0.0495,
      "step": 3215
    },
    {
      "epoch": 1.7218206157965195,
      "grad_norm": 0.7673560976982117,
      "learning_rate": 8.526052819414704e-05,
      "loss": 0.0718,
      "step": 3216
    },
    {
      "epoch": 1.7223560910307898,
      "grad_norm": 0.6528847217559814,
      "learning_rate": 8.522483940042827e-05,
      "loss": 0.0638,
      "step": 3217
    },
    {
      "epoch": 1.7228915662650603,
      "grad_norm": 0.6798214316368103,
      "learning_rate": 8.51891506067095e-05,
      "loss": 0.0954,
      "step": 3218
    },
    {
      "epoch": 1.7234270414993307,
      "grad_norm": 0.9748454689979553,
      "learning_rate": 8.515346181299072e-05,
      "loss": 0.1005,
      "step": 3219
    },
    {
      "epoch": 1.723962516733601,
      "grad_norm": 6.550887107849121,
      "learning_rate": 8.511777301927196e-05,
      "loss": 0.2707,
      "step": 3220
    },
    {
      "epoch": 1.7244979919678713,
      "grad_norm": 0.7552980780601501,
      "learning_rate": 8.508208422555318e-05,
      "loss": 0.0599,
      "step": 3221
    },
    {
      "epoch": 1.725033467202142,
      "grad_norm": 1.704178810119629,
      "learning_rate": 8.504639543183441e-05,
      "loss": 0.122,
      "step": 3222
    },
    {
      "epoch": 1.7255689424364125,
      "grad_norm": 0.6180002093315125,
      "learning_rate": 8.501070663811563e-05,
      "loss": 0.0318,
      "step": 3223
    },
    {
      "epoch": 1.7261044176706828,
      "grad_norm": 1.5817127227783203,
      "learning_rate": 8.497501784439686e-05,
      "loss": 0.1299,
      "step": 3224
    },
    {
      "epoch": 1.7266398929049531,
      "grad_norm": 1.250988245010376,
      "learning_rate": 8.49393290506781e-05,
      "loss": 0.0927,
      "step": 3225
    },
    {
      "epoch": 1.7271753681392235,
      "grad_norm": 0.7656401991844177,
      "learning_rate": 8.490364025695932e-05,
      "loss": 0.0904,
      "step": 3226
    },
    {
      "epoch": 1.727710843373494,
      "grad_norm": 0.6838396787643433,
      "learning_rate": 8.486795146324055e-05,
      "loss": 0.0698,
      "step": 3227
    },
    {
      "epoch": 1.7282463186077643,
      "grad_norm": 0.9403435587882996,
      "learning_rate": 8.483226266952178e-05,
      "loss": 0.0799,
      "step": 3228
    },
    {
      "epoch": 1.728781793842035,
      "grad_norm": 1.058027744293213,
      "learning_rate": 8.4796573875803e-05,
      "loss": 0.0947,
      "step": 3229
    },
    {
      "epoch": 1.7293172690763052,
      "grad_norm": 3.2289512157440186,
      "learning_rate": 8.476088508208422e-05,
      "loss": 0.0846,
      "step": 3230
    },
    {
      "epoch": 1.7298527443105756,
      "grad_norm": 1.151039719581604,
      "learning_rate": 8.472519628836546e-05,
      "loss": 0.1046,
      "step": 3231
    },
    {
      "epoch": 1.730388219544846,
      "grad_norm": 1.410118818283081,
      "learning_rate": 8.468950749464668e-05,
      "loss": 0.1372,
      "step": 3232
    },
    {
      "epoch": 1.7309236947791165,
      "grad_norm": 0.8510369062423706,
      "learning_rate": 8.465381870092791e-05,
      "loss": 0.1263,
      "step": 3233
    },
    {
      "epoch": 1.731459170013387,
      "grad_norm": 0.5994448661804199,
      "learning_rate": 8.461812990720914e-05,
      "loss": 0.041,
      "step": 3234
    },
    {
      "epoch": 1.7319946452476573,
      "grad_norm": 1.731589913368225,
      "learning_rate": 8.458244111349036e-05,
      "loss": 0.1622,
      "step": 3235
    },
    {
      "epoch": 1.7325301204819277,
      "grad_norm": 1.073216438293457,
      "learning_rate": 8.45467523197716e-05,
      "loss": 0.1461,
      "step": 3236
    },
    {
      "epoch": 1.733065595716198,
      "grad_norm": 0.5668593645095825,
      "learning_rate": 8.451106352605283e-05,
      "loss": 0.0648,
      "step": 3237
    },
    {
      "epoch": 1.7336010709504686,
      "grad_norm": 0.8908601999282837,
      "learning_rate": 8.447537473233405e-05,
      "loss": 0.1196,
      "step": 3238
    },
    {
      "epoch": 1.7341365461847391,
      "grad_norm": 0.4653463065624237,
      "learning_rate": 8.443968593861527e-05,
      "loss": 0.0399,
      "step": 3239
    },
    {
      "epoch": 1.7346720214190094,
      "grad_norm": 0.8511834740638733,
      "learning_rate": 8.44039971448965e-05,
      "loss": 0.065,
      "step": 3240
    },
    {
      "epoch": 1.7352074966532798,
      "grad_norm": 1.1611807346343994,
      "learning_rate": 8.436830835117774e-05,
      "loss": 0.0521,
      "step": 3241
    },
    {
      "epoch": 1.7357429718875501,
      "grad_norm": 0.402950644493103,
      "learning_rate": 8.433261955745896e-05,
      "loss": 0.0665,
      "step": 3242
    },
    {
      "epoch": 1.7362784471218207,
      "grad_norm": 5.581594467163086,
      "learning_rate": 8.429693076374019e-05,
      "loss": 0.1448,
      "step": 3243
    },
    {
      "epoch": 1.736813922356091,
      "grad_norm": 0.5708227753639221,
      "learning_rate": 8.426124197002142e-05,
      "loss": 0.0453,
      "step": 3244
    },
    {
      "epoch": 1.7373493975903616,
      "grad_norm": 6.615211009979248,
      "learning_rate": 8.422555317630264e-05,
      "loss": 0.1192,
      "step": 3245
    },
    {
      "epoch": 1.7378848728246319,
      "grad_norm": 0.7260258793830872,
      "learning_rate": 8.418986438258388e-05,
      "loss": 0.0307,
      "step": 3246
    },
    {
      "epoch": 1.7384203480589022,
      "grad_norm": 1.4146026372909546,
      "learning_rate": 8.41541755888651e-05,
      "loss": 0.145,
      "step": 3247
    },
    {
      "epoch": 1.7389558232931726,
      "grad_norm": 2.3182690143585205,
      "learning_rate": 8.411848679514632e-05,
      "loss": 0.161,
      "step": 3248
    },
    {
      "epoch": 1.739491298527443,
      "grad_norm": 0.7303075194358826,
      "learning_rate": 8.408279800142755e-05,
      "loss": 0.0549,
      "step": 3249
    },
    {
      "epoch": 1.7400267737617137,
      "grad_norm": 0.7905926704406738,
      "learning_rate": 8.404710920770878e-05,
      "loss": 0.0755,
      "step": 3250
    },
    {
      "epoch": 1.740562248995984,
      "grad_norm": 1.2093397378921509,
      "learning_rate": 8.401142041399002e-05,
      "loss": 0.0917,
      "step": 3251
    },
    {
      "epoch": 1.7410977242302543,
      "grad_norm": 0.5043678879737854,
      "learning_rate": 8.397573162027124e-05,
      "loss": 0.0255,
      "step": 3252
    },
    {
      "epoch": 1.7416331994645247,
      "grad_norm": 0.6318050622940063,
      "learning_rate": 8.394004282655247e-05,
      "loss": 0.0555,
      "step": 3253
    },
    {
      "epoch": 1.7421686746987952,
      "grad_norm": 0.4046231210231781,
      "learning_rate": 8.39043540328337e-05,
      "loss": 0.0353,
      "step": 3254
    },
    {
      "epoch": 1.7427041499330655,
      "grad_norm": 0.892589271068573,
      "learning_rate": 8.386866523911492e-05,
      "loss": 0.054,
      "step": 3255
    },
    {
      "epoch": 1.743239625167336,
      "grad_norm": 2.0851805210113525,
      "learning_rate": 8.383297644539614e-05,
      "loss": 0.1545,
      "step": 3256
    },
    {
      "epoch": 1.7437751004016064,
      "grad_norm": 1.1537652015686035,
      "learning_rate": 8.379728765167738e-05,
      "loss": 0.1181,
      "step": 3257
    },
    {
      "epoch": 1.7443105756358768,
      "grad_norm": 0.4244082570075989,
      "learning_rate": 8.37615988579586e-05,
      "loss": 0.052,
      "step": 3258
    },
    {
      "epoch": 1.744846050870147,
      "grad_norm": 0.9325183629989624,
      "learning_rate": 8.372591006423983e-05,
      "loss": 0.1144,
      "step": 3259
    },
    {
      "epoch": 1.7453815261044177,
      "grad_norm": 0.8467893600463867,
      "learning_rate": 8.369022127052106e-05,
      "loss": 0.0985,
      "step": 3260
    },
    {
      "epoch": 1.7459170013386882,
      "grad_norm": 1.1086653470993042,
      "learning_rate": 8.365453247680228e-05,
      "loss": 0.1851,
      "step": 3261
    },
    {
      "epoch": 1.7464524765729585,
      "grad_norm": 0.6477844715118408,
      "learning_rate": 8.361884368308352e-05,
      "loss": 0.1218,
      "step": 3262
    },
    {
      "epoch": 1.7469879518072289,
      "grad_norm": 1.2848525047302246,
      "learning_rate": 8.358315488936475e-05,
      "loss": 0.143,
      "step": 3263
    },
    {
      "epoch": 1.7475234270414992,
      "grad_norm": 3.927722930908203,
      "learning_rate": 8.354746609564597e-05,
      "loss": 0.1438,
      "step": 3264
    },
    {
      "epoch": 1.7480589022757698,
      "grad_norm": 0.3674076795578003,
      "learning_rate": 8.351177730192719e-05,
      "loss": 0.0423,
      "step": 3265
    },
    {
      "epoch": 1.7485943775100403,
      "grad_norm": 6.955999851226807,
      "learning_rate": 8.347608850820842e-05,
      "loss": 0.2174,
      "step": 3266
    },
    {
      "epoch": 1.7491298527443107,
      "grad_norm": 1.113107681274414,
      "learning_rate": 8.344039971448966e-05,
      "loss": 0.142,
      "step": 3267
    },
    {
      "epoch": 1.749665327978581,
      "grad_norm": 0.7710415720939636,
      "learning_rate": 8.340471092077088e-05,
      "loss": 0.1061,
      "step": 3268
    },
    {
      "epoch": 1.7502008032128513,
      "grad_norm": 1.5408108234405518,
      "learning_rate": 8.336902212705211e-05,
      "loss": 0.1373,
      "step": 3269
    },
    {
      "epoch": 1.7507362784471219,
      "grad_norm": 0.6682798266410828,
      "learning_rate": 8.333333333333334e-05,
      "loss": 0.0998,
      "step": 3270
    },
    {
      "epoch": 1.7512717536813922,
      "grad_norm": 1.0612173080444336,
      "learning_rate": 8.329764453961456e-05,
      "loss": 0.1644,
      "step": 3271
    },
    {
      "epoch": 1.7518072289156628,
      "grad_norm": 0.5469962954521179,
      "learning_rate": 8.32619557458958e-05,
      "loss": 0.0683,
      "step": 3272
    },
    {
      "epoch": 1.752342704149933,
      "grad_norm": 1.1580986976623535,
      "learning_rate": 8.322626695217702e-05,
      "loss": 0.173,
      "step": 3273
    },
    {
      "epoch": 1.7528781793842034,
      "grad_norm": 0.6582410931587219,
      "learning_rate": 8.319057815845824e-05,
      "loss": 0.1164,
      "step": 3274
    },
    {
      "epoch": 1.7534136546184738,
      "grad_norm": 0.8060892224311829,
      "learning_rate": 8.315488936473947e-05,
      "loss": 0.1544,
      "step": 3275
    },
    {
      "epoch": 1.7539491298527443,
      "grad_norm": 0.9094763994216919,
      "learning_rate": 8.31192005710207e-05,
      "loss": 0.0985,
      "step": 3276
    },
    {
      "epoch": 1.7544846050870149,
      "grad_norm": 1.1965278387069702,
      "learning_rate": 8.308351177730194e-05,
      "loss": 0.0799,
      "step": 3277
    },
    {
      "epoch": 1.7550200803212852,
      "grad_norm": 0.5810368657112122,
      "learning_rate": 8.304782298358316e-05,
      "loss": 0.0842,
      "step": 3278
    },
    {
      "epoch": 1.7555555555555555,
      "grad_norm": 1.0523455142974854,
      "learning_rate": 8.301213418986439e-05,
      "loss": 0.1003,
      "step": 3279
    },
    {
      "epoch": 1.7560910307898259,
      "grad_norm": 0.6174715757369995,
      "learning_rate": 8.297644539614562e-05,
      "loss": 0.0649,
      "step": 3280
    },
    {
      "epoch": 1.7566265060240964,
      "grad_norm": 0.589817225933075,
      "learning_rate": 8.294075660242684e-05,
      "loss": 0.0647,
      "step": 3281
    },
    {
      "epoch": 1.7571619812583668,
      "grad_norm": 0.5729250311851501,
      "learning_rate": 8.290506780870806e-05,
      "loss": 0.1137,
      "step": 3282
    },
    {
      "epoch": 1.7576974564926373,
      "grad_norm": 0.5076900720596313,
      "learning_rate": 8.28693790149893e-05,
      "loss": 0.06,
      "step": 3283
    },
    {
      "epoch": 1.7582329317269076,
      "grad_norm": 0.6888930201530457,
      "learning_rate": 8.283369022127052e-05,
      "loss": 0.1199,
      "step": 3284
    },
    {
      "epoch": 1.758768406961178,
      "grad_norm": 0.463756799697876,
      "learning_rate": 8.279800142755175e-05,
      "loss": 0.0535,
      "step": 3285
    },
    {
      "epoch": 1.7593038821954483,
      "grad_norm": 0.7619320154190063,
      "learning_rate": 8.276231263383298e-05,
      "loss": 0.0962,
      "step": 3286
    },
    {
      "epoch": 1.7598393574297189,
      "grad_norm": 1.0884917974472046,
      "learning_rate": 8.27266238401142e-05,
      "loss": 0.1922,
      "step": 3287
    },
    {
      "epoch": 1.7603748326639894,
      "grad_norm": 0.5974902510643005,
      "learning_rate": 8.269093504639544e-05,
      "loss": 0.0703,
      "step": 3288
    },
    {
      "epoch": 1.7609103078982598,
      "grad_norm": 0.5464768409729004,
      "learning_rate": 8.265524625267667e-05,
      "loss": 0.0544,
      "step": 3289
    },
    {
      "epoch": 1.76144578313253,
      "grad_norm": 0.9771963953971863,
      "learning_rate": 8.261955745895789e-05,
      "loss": 0.1558,
      "step": 3290
    },
    {
      "epoch": 1.7619812583668004,
      "grad_norm": 0.6128710508346558,
      "learning_rate": 8.258386866523911e-05,
      "loss": 0.0908,
      "step": 3291
    },
    {
      "epoch": 1.762516733601071,
      "grad_norm": 0.5324615240097046,
      "learning_rate": 8.254817987152034e-05,
      "loss": 0.0919,
      "step": 3292
    },
    {
      "epoch": 1.7630522088353415,
      "grad_norm": 0.8218613266944885,
      "learning_rate": 8.251249107780158e-05,
      "loss": 0.072,
      "step": 3293
    },
    {
      "epoch": 1.7635876840696119,
      "grad_norm": 0.44933152198791504,
      "learning_rate": 8.24768022840828e-05,
      "loss": 0.0462,
      "step": 3294
    },
    {
      "epoch": 1.7641231593038822,
      "grad_norm": 0.5531262755393982,
      "learning_rate": 8.244111349036403e-05,
      "loss": 0.0534,
      "step": 3295
    },
    {
      "epoch": 1.7646586345381525,
      "grad_norm": 0.476841002702713,
      "learning_rate": 8.240542469664526e-05,
      "loss": 0.0633,
      "step": 3296
    },
    {
      "epoch": 1.7651941097724229,
      "grad_norm": 0.3288882374763489,
      "learning_rate": 8.236973590292648e-05,
      "loss": 0.0221,
      "step": 3297
    },
    {
      "epoch": 1.7657295850066934,
      "grad_norm": 0.8028223514556885,
      "learning_rate": 8.233404710920772e-05,
      "loss": 0.072,
      "step": 3298
    },
    {
      "epoch": 1.766265060240964,
      "grad_norm": 0.6073128581047058,
      "learning_rate": 8.229835831548894e-05,
      "loss": 0.0813,
      "step": 3299
    },
    {
      "epoch": 1.7668005354752343,
      "grad_norm": 1.1791051626205444,
      "learning_rate": 8.226266952177016e-05,
      "loss": 0.2099,
      "step": 3300
    },
    {
      "epoch": 1.7673360107095046,
      "grad_norm": 0.6190423369407654,
      "learning_rate": 8.222698072805139e-05,
      "loss": 0.0651,
      "step": 3301
    },
    {
      "epoch": 1.767871485943775,
      "grad_norm": 0.6738430261611938,
      "learning_rate": 8.219129193433262e-05,
      "loss": 0.0663,
      "step": 3302
    },
    {
      "epoch": 1.7684069611780455,
      "grad_norm": 1.0419625043869019,
      "learning_rate": 8.215560314061384e-05,
      "loss": 0.1455,
      "step": 3303
    },
    {
      "epoch": 1.768942436412316,
      "grad_norm": 0.641354501247406,
      "learning_rate": 8.211991434689508e-05,
      "loss": 0.0345,
      "step": 3304
    },
    {
      "epoch": 1.7694779116465864,
      "grad_norm": 1.3754459619522095,
      "learning_rate": 8.208422555317631e-05,
      "loss": 0.1563,
      "step": 3305
    },
    {
      "epoch": 1.7700133868808567,
      "grad_norm": 0.7316375970840454,
      "learning_rate": 8.204853675945754e-05,
      "loss": 0.0846,
      "step": 3306
    },
    {
      "epoch": 1.770548862115127,
      "grad_norm": 0.6490682363510132,
      "learning_rate": 8.201284796573876e-05,
      "loss": 0.0672,
      "step": 3307
    },
    {
      "epoch": 1.7710843373493976,
      "grad_norm": 0.7517656087875366,
      "learning_rate": 8.197715917201998e-05,
      "loss": 0.1017,
      "step": 3308
    },
    {
      "epoch": 1.771619812583668,
      "grad_norm": 1.1330350637435913,
      "learning_rate": 8.194147037830122e-05,
      "loss": 0.1075,
      "step": 3309
    },
    {
      "epoch": 1.7721552878179385,
      "grad_norm": 0.5831356048583984,
      "learning_rate": 8.190578158458244e-05,
      "loss": 0.0558,
      "step": 3310
    },
    {
      "epoch": 1.7726907630522089,
      "grad_norm": 0.6307084560394287,
      "learning_rate": 8.187009279086367e-05,
      "loss": 0.0608,
      "step": 3311
    },
    {
      "epoch": 1.7732262382864792,
      "grad_norm": 1.2295713424682617,
      "learning_rate": 8.18344039971449e-05,
      "loss": 0.1823,
      "step": 3312
    },
    {
      "epoch": 1.7737617135207495,
      "grad_norm": 0.8376558423042297,
      "learning_rate": 8.179871520342612e-05,
      "loss": 0.099,
      "step": 3313
    },
    {
      "epoch": 1.77429718875502,
      "grad_norm": 0.5820186138153076,
      "learning_rate": 8.176302640970736e-05,
      "loss": 0.073,
      "step": 3314
    },
    {
      "epoch": 1.7748326639892906,
      "grad_norm": 0.9199619293212891,
      "learning_rate": 8.172733761598859e-05,
      "loss": 0.0867,
      "step": 3315
    },
    {
      "epoch": 1.775368139223561,
      "grad_norm": 1.1508941650390625,
      "learning_rate": 8.169164882226981e-05,
      "loss": 0.0993,
      "step": 3316
    },
    {
      "epoch": 1.7759036144578313,
      "grad_norm": 0.6952767372131348,
      "learning_rate": 8.165596002855104e-05,
      "loss": 0.0723,
      "step": 3317
    },
    {
      "epoch": 1.7764390896921016,
      "grad_norm": 2.511836290359497,
      "learning_rate": 8.162027123483226e-05,
      "loss": 0.1814,
      "step": 3318
    },
    {
      "epoch": 1.7769745649263722,
      "grad_norm": 0.9817960858345032,
      "learning_rate": 8.15845824411135e-05,
      "loss": 0.0843,
      "step": 3319
    },
    {
      "epoch": 1.7775100401606427,
      "grad_norm": 0.6437094211578369,
      "learning_rate": 8.154889364739472e-05,
      "loss": 0.0546,
      "step": 3320
    },
    {
      "epoch": 1.778045515394913,
      "grad_norm": 0.4138590693473816,
      "learning_rate": 8.151320485367595e-05,
      "loss": 0.0519,
      "step": 3321
    },
    {
      "epoch": 1.7785809906291834,
      "grad_norm": 2.3989434242248535,
      "learning_rate": 8.147751605995718e-05,
      "loss": 0.2138,
      "step": 3322
    },
    {
      "epoch": 1.7791164658634537,
      "grad_norm": 0.5771358609199524,
      "learning_rate": 8.14418272662384e-05,
      "loss": 0.0678,
      "step": 3323
    },
    {
      "epoch": 1.779651941097724,
      "grad_norm": 0.9381840825080872,
      "learning_rate": 8.140613847251964e-05,
      "loss": 0.0841,
      "step": 3324
    },
    {
      "epoch": 1.7801874163319946,
      "grad_norm": 0.376068651676178,
      "learning_rate": 8.137044967880087e-05,
      "loss": 0.0196,
      "step": 3325
    },
    {
      "epoch": 1.7807228915662652,
      "grad_norm": 1.0330209732055664,
      "learning_rate": 8.133476088508209e-05,
      "loss": 0.1057,
      "step": 3326
    },
    {
      "epoch": 1.7812583668005355,
      "grad_norm": 0.6877104043960571,
      "learning_rate": 8.129907209136331e-05,
      "loss": 0.0567,
      "step": 3327
    },
    {
      "epoch": 1.7817938420348058,
      "grad_norm": 0.3310103416442871,
      "learning_rate": 8.126338329764454e-05,
      "loss": 0.0116,
      "step": 3328
    },
    {
      "epoch": 1.7823293172690762,
      "grad_norm": 0.7071405649185181,
      "learning_rate": 8.122769450392576e-05,
      "loss": 0.0734,
      "step": 3329
    },
    {
      "epoch": 1.7828647925033467,
      "grad_norm": 0.5569969415664673,
      "learning_rate": 8.1192005710207e-05,
      "loss": 0.0413,
      "step": 3330
    },
    {
      "epoch": 1.7834002677376173,
      "grad_norm": 0.5510094165802002,
      "learning_rate": 8.115631691648823e-05,
      "loss": 0.085,
      "step": 3331
    },
    {
      "epoch": 1.7839357429718876,
      "grad_norm": 0.4306848347187042,
      "learning_rate": 8.112062812276946e-05,
      "loss": 0.0587,
      "step": 3332
    },
    {
      "epoch": 1.784471218206158,
      "grad_norm": 0.778730571269989,
      "learning_rate": 8.108493932905068e-05,
      "loss": 0.0834,
      "step": 3333
    },
    {
      "epoch": 1.7850066934404283,
      "grad_norm": 0.805901050567627,
      "learning_rate": 8.104925053533192e-05,
      "loss": 0.0552,
      "step": 3334
    },
    {
      "epoch": 1.7855421686746988,
      "grad_norm": 0.5382232666015625,
      "learning_rate": 8.101356174161314e-05,
      "loss": 0.0749,
      "step": 3335
    },
    {
      "epoch": 1.7860776439089692,
      "grad_norm": 1.5701160430908203,
      "learning_rate": 8.097787294789436e-05,
      "loss": 0.1882,
      "step": 3336
    },
    {
      "epoch": 1.7866131191432397,
      "grad_norm": 1.3951438665390015,
      "learning_rate": 8.094218415417559e-05,
      "loss": 0.1284,
      "step": 3337
    },
    {
      "epoch": 1.78714859437751,
      "grad_norm": 1.0581454038619995,
      "learning_rate": 8.090649536045682e-05,
      "loss": 0.1236,
      "step": 3338
    },
    {
      "epoch": 1.7876840696117804,
      "grad_norm": 0.8133560419082642,
      "learning_rate": 8.087080656673804e-05,
      "loss": 0.0762,
      "step": 3339
    },
    {
      "epoch": 1.7882195448460507,
      "grad_norm": 0.6675983667373657,
      "learning_rate": 8.083511777301928e-05,
      "loss": 0.1002,
      "step": 3340
    },
    {
      "epoch": 1.7887550200803213,
      "grad_norm": 1.0372416973114014,
      "learning_rate": 8.079942897930051e-05,
      "loss": 0.0972,
      "step": 3341
    },
    {
      "epoch": 1.7892904953145918,
      "grad_norm": 0.6012776494026184,
      "learning_rate": 8.076374018558173e-05,
      "loss": 0.0492,
      "step": 3342
    },
    {
      "epoch": 1.7898259705488622,
      "grad_norm": 0.9427855014801025,
      "learning_rate": 8.072805139186296e-05,
      "loss": 0.0859,
      "step": 3343
    },
    {
      "epoch": 1.7903614457831325,
      "grad_norm": 0.9313750267028809,
      "learning_rate": 8.069236259814418e-05,
      "loss": 0.14,
      "step": 3344
    },
    {
      "epoch": 1.7908969210174028,
      "grad_norm": 0.4949374794960022,
      "learning_rate": 8.06566738044254e-05,
      "loss": 0.0389,
      "step": 3345
    },
    {
      "epoch": 1.7914323962516734,
      "grad_norm": 0.5227290391921997,
      "learning_rate": 8.062098501070664e-05,
      "loss": 0.0489,
      "step": 3346
    },
    {
      "epoch": 1.7919678714859437,
      "grad_norm": 1.1087182760238647,
      "learning_rate": 8.058529621698787e-05,
      "loss": 0.1196,
      "step": 3347
    },
    {
      "epoch": 1.7925033467202143,
      "grad_norm": 1.0406368970870972,
      "learning_rate": 8.05496074232691e-05,
      "loss": 0.194,
      "step": 3348
    },
    {
      "epoch": 1.7930388219544846,
      "grad_norm": 0.6401388049125671,
      "learning_rate": 8.051391862955032e-05,
      "loss": 0.0444,
      "step": 3349
    },
    {
      "epoch": 1.793574297188755,
      "grad_norm": 0.3750312626361847,
      "learning_rate": 8.047822983583156e-05,
      "loss": 0.065,
      "step": 3350
    },
    {
      "epoch": 1.7941097724230253,
      "grad_norm": 1.16299569606781,
      "learning_rate": 8.044254104211279e-05,
      "loss": 0.1602,
      "step": 3351
    },
    {
      "epoch": 1.7946452476572958,
      "grad_norm": 0.6471302509307861,
      "learning_rate": 8.040685224839401e-05,
      "loss": 0.0979,
      "step": 3352
    },
    {
      "epoch": 1.7951807228915664,
      "grad_norm": 0.43181630969047546,
      "learning_rate": 8.037116345467523e-05,
      "loss": 0.0552,
      "step": 3353
    },
    {
      "epoch": 1.7957161981258367,
      "grad_norm": 0.7648612260818481,
      "learning_rate": 8.033547466095646e-05,
      "loss": 0.1179,
      "step": 3354
    },
    {
      "epoch": 1.796251673360107,
      "grad_norm": 1.1690617799758911,
      "learning_rate": 8.029978586723768e-05,
      "loss": 0.1,
      "step": 3355
    },
    {
      "epoch": 1.7967871485943774,
      "grad_norm": 0.7783830165863037,
      "learning_rate": 8.026409707351892e-05,
      "loss": 0.0683,
      "step": 3356
    },
    {
      "epoch": 1.797322623828648,
      "grad_norm": 0.44873055815696716,
      "learning_rate": 8.022840827980015e-05,
      "loss": 0.0406,
      "step": 3357
    },
    {
      "epoch": 1.7978580990629185,
      "grad_norm": 0.7780547738075256,
      "learning_rate": 8.019271948608137e-05,
      "loss": 0.1046,
      "step": 3358
    },
    {
      "epoch": 1.7983935742971888,
      "grad_norm": 0.5494268536567688,
      "learning_rate": 8.01570306923626e-05,
      "loss": 0.0806,
      "step": 3359
    },
    {
      "epoch": 1.7989290495314592,
      "grad_norm": 0.7915239930152893,
      "learning_rate": 8.012134189864384e-05,
      "loss": 0.1788,
      "step": 3360
    },
    {
      "epoch": 1.7994645247657295,
      "grad_norm": 0.9113973379135132,
      "learning_rate": 8.008565310492506e-05,
      "loss": 0.095,
      "step": 3361
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.7134687304496765,
      "learning_rate": 8.004996431120628e-05,
      "loss": 0.1058,
      "step": 3362
    },
    {
      "epoch": 1.8005354752342704,
      "grad_norm": 0.6784136295318604,
      "learning_rate": 8.001427551748751e-05,
      "loss": 0.0657,
      "step": 3363
    },
    {
      "epoch": 1.801070950468541,
      "grad_norm": 0.5310019850730896,
      "learning_rate": 7.997858672376874e-05,
      "loss": 0.0358,
      "step": 3364
    },
    {
      "epoch": 1.8016064257028113,
      "grad_norm": 0.8385034203529358,
      "learning_rate": 7.994289793004996e-05,
      "loss": 0.0419,
      "step": 3365
    },
    {
      "epoch": 1.8021419009370816,
      "grad_norm": 0.7133999466896057,
      "learning_rate": 7.99072091363312e-05,
      "loss": 0.0545,
      "step": 3366
    },
    {
      "epoch": 1.802677376171352,
      "grad_norm": 0.566646933555603,
      "learning_rate": 7.987152034261243e-05,
      "loss": 0.05,
      "step": 3367
    },
    {
      "epoch": 1.8032128514056225,
      "grad_norm": 0.6810698509216309,
      "learning_rate": 7.983583154889365e-05,
      "loss": 0.1135,
      "step": 3368
    },
    {
      "epoch": 1.803748326639893,
      "grad_norm": 0.8285796642303467,
      "learning_rate": 7.980014275517488e-05,
      "loss": 0.1132,
      "step": 3369
    },
    {
      "epoch": 1.8042838018741634,
      "grad_norm": 0.6606481075286865,
      "learning_rate": 7.97644539614561e-05,
      "loss": 0.0856,
      "step": 3370
    },
    {
      "epoch": 1.8048192771084337,
      "grad_norm": 0.49193036556243896,
      "learning_rate": 7.972876516773732e-05,
      "loss": 0.0494,
      "step": 3371
    },
    {
      "epoch": 1.805354752342704,
      "grad_norm": 1.9013311862945557,
      "learning_rate": 7.969307637401856e-05,
      "loss": 0.0608,
      "step": 3372
    },
    {
      "epoch": 1.8058902275769746,
      "grad_norm": 1.1571062803268433,
      "learning_rate": 7.965738758029979e-05,
      "loss": 0.1252,
      "step": 3373
    },
    {
      "epoch": 1.806425702811245,
      "grad_norm": 1.3300447463989258,
      "learning_rate": 7.962169878658102e-05,
      "loss": 0.2515,
      "step": 3374
    },
    {
      "epoch": 1.8069611780455155,
      "grad_norm": 0.14931632578372955,
      "learning_rate": 7.958600999286224e-05,
      "loss": 0.0082,
      "step": 3375
    },
    {
      "epoch": 1.8074966532797858,
      "grad_norm": 1.0972185134887695,
      "learning_rate": 7.955032119914348e-05,
      "loss": 0.1407,
      "step": 3376
    },
    {
      "epoch": 1.8080321285140561,
      "grad_norm": 0.533101499080658,
      "learning_rate": 7.951463240542471e-05,
      "loss": 0.0578,
      "step": 3377
    },
    {
      "epoch": 1.8085676037483265,
      "grad_norm": 0.43898287415504456,
      "learning_rate": 7.947894361170593e-05,
      "loss": 0.039,
      "step": 3378
    },
    {
      "epoch": 1.809103078982597,
      "grad_norm": 0.7141373157501221,
      "learning_rate": 7.944325481798715e-05,
      "loss": 0.126,
      "step": 3379
    },
    {
      "epoch": 1.8096385542168676,
      "grad_norm": 0.6099156141281128,
      "learning_rate": 7.940756602426838e-05,
      "loss": 0.0467,
      "step": 3380
    },
    {
      "epoch": 1.810174029451138,
      "grad_norm": 0.44398555159568787,
      "learning_rate": 7.93718772305496e-05,
      "loss": 0.0632,
      "step": 3381
    },
    {
      "epoch": 1.8107095046854083,
      "grad_norm": 2.700556516647339,
      "learning_rate": 7.933618843683084e-05,
      "loss": 0.0991,
      "step": 3382
    },
    {
      "epoch": 1.8112449799196786,
      "grad_norm": 0.8275203108787537,
      "learning_rate": 7.930049964311207e-05,
      "loss": 0.0686,
      "step": 3383
    },
    {
      "epoch": 1.8117804551539491,
      "grad_norm": 0.5759111046791077,
      "learning_rate": 7.926481084939329e-05,
      "loss": 0.0725,
      "step": 3384
    },
    {
      "epoch": 1.8123159303882197,
      "grad_norm": 0.9263603091239929,
      "learning_rate": 7.922912205567452e-05,
      "loss": 0.1715,
      "step": 3385
    },
    {
      "epoch": 1.81285140562249,
      "grad_norm": 4.676527976989746,
      "learning_rate": 7.919343326195576e-05,
      "loss": 0.0675,
      "step": 3386
    },
    {
      "epoch": 1.8133868808567604,
      "grad_norm": 0.40387821197509766,
      "learning_rate": 7.915774446823698e-05,
      "loss": 0.0318,
      "step": 3387
    },
    {
      "epoch": 1.8139223560910307,
      "grad_norm": 0.6053984761238098,
      "learning_rate": 7.91220556745182e-05,
      "loss": 0.1072,
      "step": 3388
    },
    {
      "epoch": 1.8144578313253013,
      "grad_norm": 0.634774386882782,
      "learning_rate": 7.908636688079943e-05,
      "loss": 0.0928,
      "step": 3389
    },
    {
      "epoch": 1.8149933065595716,
      "grad_norm": 0.562771201133728,
      "learning_rate": 7.905067808708066e-05,
      "loss": 0.0473,
      "step": 3390
    },
    {
      "epoch": 1.8155287817938421,
      "grad_norm": 0.7051553130149841,
      "learning_rate": 7.901498929336188e-05,
      "loss": 0.1346,
      "step": 3391
    },
    {
      "epoch": 1.8160642570281125,
      "grad_norm": 0.5236268639564514,
      "learning_rate": 7.897930049964312e-05,
      "loss": 0.0446,
      "step": 3392
    },
    {
      "epoch": 1.8165997322623828,
      "grad_norm": 0.7870213389396667,
      "learning_rate": 7.894361170592435e-05,
      "loss": 0.1032,
      "step": 3393
    },
    {
      "epoch": 1.8171352074966531,
      "grad_norm": 0.5120847225189209,
      "learning_rate": 7.890792291220557e-05,
      "loss": 0.0948,
      "step": 3394
    },
    {
      "epoch": 1.8176706827309237,
      "grad_norm": 0.4285604655742645,
      "learning_rate": 7.88722341184868e-05,
      "loss": 0.0302,
      "step": 3395
    },
    {
      "epoch": 1.8182061579651942,
      "grad_norm": 0.9120644330978394,
      "learning_rate": 7.883654532476802e-05,
      "loss": 0.072,
      "step": 3396
    },
    {
      "epoch": 1.8187416331994646,
      "grad_norm": 0.7984820008277893,
      "learning_rate": 7.880085653104924e-05,
      "loss": 0.152,
      "step": 3397
    },
    {
      "epoch": 1.819277108433735,
      "grad_norm": 0.7948271036148071,
      "learning_rate": 7.876516773733048e-05,
      "loss": 0.0602,
      "step": 3398
    },
    {
      "epoch": 1.8198125836680052,
      "grad_norm": 0.9586973786354065,
      "learning_rate": 7.872947894361171e-05,
      "loss": 0.0872,
      "step": 3399
    },
    {
      "epoch": 1.8203480589022758,
      "grad_norm": 0.5983842015266418,
      "learning_rate": 7.869379014989293e-05,
      "loss": 0.0459,
      "step": 3400
    },
    {
      "epoch": 1.8208835341365461,
      "grad_norm": 0.3847483694553375,
      "learning_rate": 7.865810135617416e-05,
      "loss": 0.0473,
      "step": 3401
    },
    {
      "epoch": 1.8214190093708167,
      "grad_norm": 0.779330849647522,
      "learning_rate": 7.86224125624554e-05,
      "loss": 0.0866,
      "step": 3402
    },
    {
      "epoch": 1.821954484605087,
      "grad_norm": 1.0259329080581665,
      "learning_rate": 7.858672376873663e-05,
      "loss": 0.1045,
      "step": 3403
    },
    {
      "epoch": 1.8224899598393574,
      "grad_norm": 0.5313771367073059,
      "learning_rate": 7.855103497501785e-05,
      "loss": 0.0766,
      "step": 3404
    },
    {
      "epoch": 1.8230254350736277,
      "grad_norm": 0.6719155311584473,
      "learning_rate": 7.851534618129907e-05,
      "loss": 0.1023,
      "step": 3405
    },
    {
      "epoch": 1.8235609103078982,
      "grad_norm": 0.7249709367752075,
      "learning_rate": 7.84796573875803e-05,
      "loss": 0.0545,
      "step": 3406
    },
    {
      "epoch": 1.8240963855421688,
      "grad_norm": 0.45997631549835205,
      "learning_rate": 7.844396859386152e-05,
      "loss": 0.0511,
      "step": 3407
    },
    {
      "epoch": 1.8246318607764391,
      "grad_norm": 0.7769597172737122,
      "learning_rate": 7.840827980014276e-05,
      "loss": 0.1329,
      "step": 3408
    },
    {
      "epoch": 1.8251673360107095,
      "grad_norm": 0.21714246273040771,
      "learning_rate": 7.837259100642399e-05,
      "loss": 0.0182,
      "step": 3409
    },
    {
      "epoch": 1.8257028112449798,
      "grad_norm": 1.0005611181259155,
      "learning_rate": 7.833690221270521e-05,
      "loss": 0.138,
      "step": 3410
    },
    {
      "epoch": 1.8262382864792504,
      "grad_norm": 0.8518394827842712,
      "learning_rate": 7.830121341898644e-05,
      "loss": 0.1267,
      "step": 3411
    },
    {
      "epoch": 1.826773761713521,
      "grad_norm": 0.946078360080719,
      "learning_rate": 7.826552462526768e-05,
      "loss": 0.0928,
      "step": 3412
    },
    {
      "epoch": 1.8273092369477912,
      "grad_norm": 0.5100644826889038,
      "learning_rate": 7.82298358315489e-05,
      "loss": 0.0667,
      "step": 3413
    },
    {
      "epoch": 1.8278447121820616,
      "grad_norm": 0.1771666407585144,
      "learning_rate": 7.819414703783012e-05,
      "loss": 0.0121,
      "step": 3414
    },
    {
      "epoch": 1.828380187416332,
      "grad_norm": 0.5719009637832642,
      "learning_rate": 7.815845824411135e-05,
      "loss": 0.0283,
      "step": 3415
    },
    {
      "epoch": 1.8289156626506025,
      "grad_norm": 0.9033204317092896,
      "learning_rate": 7.812276945039258e-05,
      "loss": 0.0863,
      "step": 3416
    },
    {
      "epoch": 1.8294511378848728,
      "grad_norm": 0.6020325422286987,
      "learning_rate": 7.80870806566738e-05,
      "loss": 0.0671,
      "step": 3417
    },
    {
      "epoch": 1.8299866131191433,
      "grad_norm": 0.754671037197113,
      "learning_rate": 7.805139186295504e-05,
      "loss": 0.0903,
      "step": 3418
    },
    {
      "epoch": 1.8305220883534137,
      "grad_norm": 0.9931113719940186,
      "learning_rate": 7.801570306923627e-05,
      "loss": 0.1816,
      "step": 3419
    },
    {
      "epoch": 1.831057563587684,
      "grad_norm": 0.9805815815925598,
      "learning_rate": 7.798001427551749e-05,
      "loss": 0.1173,
      "step": 3420
    },
    {
      "epoch": 1.8315930388219543,
      "grad_norm": 0.49797379970550537,
      "learning_rate": 7.794432548179872e-05,
      "loss": 0.0408,
      "step": 3421
    },
    {
      "epoch": 1.832128514056225,
      "grad_norm": 0.9199890494346619,
      "learning_rate": 7.790863668807994e-05,
      "loss": 0.0552,
      "step": 3422
    },
    {
      "epoch": 1.8326639892904955,
      "grad_norm": 0.40944138169288635,
      "learning_rate": 7.787294789436116e-05,
      "loss": 0.0194,
      "step": 3423
    },
    {
      "epoch": 1.8331994645247658,
      "grad_norm": 0.5736424922943115,
      "learning_rate": 7.78372591006424e-05,
      "loss": 0.1131,
      "step": 3424
    },
    {
      "epoch": 1.8337349397590361,
      "grad_norm": 0.7401378750801086,
      "learning_rate": 7.780157030692363e-05,
      "loss": 0.0889,
      "step": 3425
    },
    {
      "epoch": 1.8342704149933065,
      "grad_norm": 0.4494687616825104,
      "learning_rate": 7.776588151320485e-05,
      "loss": 0.031,
      "step": 3426
    },
    {
      "epoch": 1.834805890227577,
      "grad_norm": 0.6571255326271057,
      "learning_rate": 7.773019271948608e-05,
      "loss": 0.0564,
      "step": 3427
    },
    {
      "epoch": 1.8353413654618473,
      "grad_norm": 0.7816391587257385,
      "learning_rate": 7.769450392576732e-05,
      "loss": 0.0879,
      "step": 3428
    },
    {
      "epoch": 1.835876840696118,
      "grad_norm": 1.4184916019439697,
      "learning_rate": 7.765881513204855e-05,
      "loss": 0.1049,
      "step": 3429
    },
    {
      "epoch": 1.8364123159303882,
      "grad_norm": 1.218531608581543,
      "learning_rate": 7.762312633832977e-05,
      "loss": 0.1611,
      "step": 3430
    },
    {
      "epoch": 1.8369477911646586,
      "grad_norm": 1.0685359239578247,
      "learning_rate": 7.758743754461099e-05,
      "loss": 0.0771,
      "step": 3431
    },
    {
      "epoch": 1.837483266398929,
      "grad_norm": 1.2598601579666138,
      "learning_rate": 7.755174875089222e-05,
      "loss": 0.0812,
      "step": 3432
    },
    {
      "epoch": 1.8380187416331994,
      "grad_norm": 0.3755016028881073,
      "learning_rate": 7.751605995717344e-05,
      "loss": 0.0356,
      "step": 3433
    },
    {
      "epoch": 1.83855421686747,
      "grad_norm": 0.4243573844432831,
      "learning_rate": 7.748037116345468e-05,
      "loss": 0.0234,
      "step": 3434
    },
    {
      "epoch": 1.8390896921017403,
      "grad_norm": 1.1944736242294312,
      "learning_rate": 7.744468236973591e-05,
      "loss": 0.0812,
      "step": 3435
    },
    {
      "epoch": 1.8396251673360107,
      "grad_norm": 0.766342282295227,
      "learning_rate": 7.740899357601713e-05,
      "loss": 0.0518,
      "step": 3436
    },
    {
      "epoch": 1.840160642570281,
      "grad_norm": 0.8756003975868225,
      "learning_rate": 7.737330478229836e-05,
      "loss": 0.1386,
      "step": 3437
    },
    {
      "epoch": 1.8406961178045516,
      "grad_norm": 0.662283718585968,
      "learning_rate": 7.73376159885796e-05,
      "loss": 0.0824,
      "step": 3438
    },
    {
      "epoch": 1.8412315930388221,
      "grad_norm": 0.9562327265739441,
      "learning_rate": 7.730192719486082e-05,
      "loss": 0.1197,
      "step": 3439
    },
    {
      "epoch": 1.8417670682730924,
      "grad_norm": 1.172594428062439,
      "learning_rate": 7.726623840114205e-05,
      "loss": 0.11,
      "step": 3440
    },
    {
      "epoch": 1.8423025435073628,
      "grad_norm": 0.5508102774620056,
      "learning_rate": 7.723054960742327e-05,
      "loss": 0.0553,
      "step": 3441
    },
    {
      "epoch": 1.842838018741633,
      "grad_norm": 1.1448934078216553,
      "learning_rate": 7.71948608137045e-05,
      "loss": 0.1173,
      "step": 3442
    },
    {
      "epoch": 1.8433734939759037,
      "grad_norm": 1.1431578397750854,
      "learning_rate": 7.715917201998572e-05,
      "loss": 0.1774,
      "step": 3443
    },
    {
      "epoch": 1.843908969210174,
      "grad_norm": 0.8298962116241455,
      "learning_rate": 7.712348322626696e-05,
      "loss": 0.0732,
      "step": 3444
    },
    {
      "epoch": 1.8444444444444446,
      "grad_norm": 1.5017201900482178,
      "learning_rate": 7.708779443254819e-05,
      "loss": 0.1017,
      "step": 3445
    },
    {
      "epoch": 1.8449799196787149,
      "grad_norm": 0.4677864909172058,
      "learning_rate": 7.705210563882941e-05,
      "loss": 0.0335,
      "step": 3446
    },
    {
      "epoch": 1.8455153949129852,
      "grad_norm": 4.3028082847595215,
      "learning_rate": 7.701641684511064e-05,
      "loss": 0.1557,
      "step": 3447
    },
    {
      "epoch": 1.8460508701472556,
      "grad_norm": 0.4761475622653961,
      "learning_rate": 7.698072805139188e-05,
      "loss": 0.0438,
      "step": 3448
    },
    {
      "epoch": 1.846586345381526,
      "grad_norm": 0.5857180953025818,
      "learning_rate": 7.69450392576731e-05,
      "loss": 0.0807,
      "step": 3449
    },
    {
      "epoch": 1.8471218206157967,
      "grad_norm": 1.179357886314392,
      "learning_rate": 7.690935046395432e-05,
      "loss": 0.1611,
      "step": 3450
    },
    {
      "epoch": 1.847657295850067,
      "grad_norm": 0.5388861894607544,
      "learning_rate": 7.687366167023555e-05,
      "loss": 0.0195,
      "step": 3451
    },
    {
      "epoch": 1.8481927710843373,
      "grad_norm": 0.8265576362609863,
      "learning_rate": 7.683797287651677e-05,
      "loss": 0.1169,
      "step": 3452
    },
    {
      "epoch": 1.8487282463186077,
      "grad_norm": 0.6689706444740295,
      "learning_rate": 7.6802284082798e-05,
      "loss": 0.0395,
      "step": 3453
    },
    {
      "epoch": 1.8492637215528782,
      "grad_norm": 0.8397175073623657,
      "learning_rate": 7.676659528907924e-05,
      "loss": 0.0785,
      "step": 3454
    },
    {
      "epoch": 1.8497991967871485,
      "grad_norm": 1.177464246749878,
      "learning_rate": 7.673090649536046e-05,
      "loss": 0.0515,
      "step": 3455
    },
    {
      "epoch": 1.850334672021419,
      "grad_norm": 1.2296711206436157,
      "learning_rate": 7.669521770164169e-05,
      "loss": 0.14,
      "step": 3456
    },
    {
      "epoch": 1.8508701472556894,
      "grad_norm": 1.2055232524871826,
      "learning_rate": 7.665952890792292e-05,
      "loss": 0.1698,
      "step": 3457
    },
    {
      "epoch": 1.8514056224899598,
      "grad_norm": 1.53429114818573,
      "learning_rate": 7.662384011420414e-05,
      "loss": 0.0501,
      "step": 3458
    },
    {
      "epoch": 1.85194109772423,
      "grad_norm": 0.6209845542907715,
      "learning_rate": 7.658815132048536e-05,
      "loss": 0.0745,
      "step": 3459
    },
    {
      "epoch": 1.8524765729585007,
      "grad_norm": 0.3305601477622986,
      "learning_rate": 7.65524625267666e-05,
      "loss": 0.0278,
      "step": 3460
    },
    {
      "epoch": 1.8530120481927712,
      "grad_norm": 0.5796141028404236,
      "learning_rate": 7.651677373304783e-05,
      "loss": 0.0618,
      "step": 3461
    },
    {
      "epoch": 1.8535475234270415,
      "grad_norm": 0.7623153924942017,
      "learning_rate": 7.648108493932905e-05,
      "loss": 0.071,
      "step": 3462
    },
    {
      "epoch": 1.8540829986613119,
      "grad_norm": 0.7972906231880188,
      "learning_rate": 7.644539614561028e-05,
      "loss": 0.0349,
      "step": 3463
    },
    {
      "epoch": 1.8546184738955822,
      "grad_norm": 0.5886691808700562,
      "learning_rate": 7.640970735189152e-05,
      "loss": 0.0728,
      "step": 3464
    },
    {
      "epoch": 1.8551539491298528,
      "grad_norm": 1.0248708724975586,
      "learning_rate": 7.637401855817274e-05,
      "loss": 0.0968,
      "step": 3465
    },
    {
      "epoch": 1.8556894243641233,
      "grad_norm": 1.0945121049880981,
      "learning_rate": 7.633832976445397e-05,
      "loss": 0.1186,
      "step": 3466
    },
    {
      "epoch": 1.8562248995983937,
      "grad_norm": 0.8561594486236572,
      "learning_rate": 7.630264097073519e-05,
      "loss": 0.0552,
      "step": 3467
    },
    {
      "epoch": 1.856760374832664,
      "grad_norm": 1.6516060829162598,
      "learning_rate": 7.626695217701641e-05,
      "loss": 0.109,
      "step": 3468
    },
    {
      "epoch": 1.8572958500669343,
      "grad_norm": 1.0749362707138062,
      "learning_rate": 7.623126338329764e-05,
      "loss": 0.0575,
      "step": 3469
    },
    {
      "epoch": 1.8578313253012049,
      "grad_norm": 1.049907922744751,
      "learning_rate": 7.619557458957888e-05,
      "loss": 0.0545,
      "step": 3470
    },
    {
      "epoch": 1.8583668005354752,
      "grad_norm": 1.4056949615478516,
      "learning_rate": 7.615988579586011e-05,
      "loss": 0.0979,
      "step": 3471
    },
    {
      "epoch": 1.8589022757697458,
      "grad_norm": 3.174241065979004,
      "learning_rate": 7.612419700214133e-05,
      "loss": 0.057,
      "step": 3472
    },
    {
      "epoch": 1.859437751004016,
      "grad_norm": 1.125644326210022,
      "learning_rate": 7.608850820842256e-05,
      "loss": 0.0591,
      "step": 3473
    },
    {
      "epoch": 1.8599732262382864,
      "grad_norm": 1.0006030797958374,
      "learning_rate": 7.60528194147038e-05,
      "loss": 0.1002,
      "step": 3474
    },
    {
      "epoch": 1.8605087014725568,
      "grad_norm": 1.0133355855941772,
      "learning_rate": 7.601713062098502e-05,
      "loss": 0.1394,
      "step": 3475
    },
    {
      "epoch": 1.8610441767068273,
      "grad_norm": 0.6096384525299072,
      "learning_rate": 7.598144182726624e-05,
      "loss": 0.0667,
      "step": 3476
    },
    {
      "epoch": 1.8615796519410979,
      "grad_norm": 1.0145800113677979,
      "learning_rate": 7.594575303354747e-05,
      "loss": 0.1087,
      "step": 3477
    },
    {
      "epoch": 1.8621151271753682,
      "grad_norm": 1.0722404718399048,
      "learning_rate": 7.591006423982869e-05,
      "loss": 0.0747,
      "step": 3478
    },
    {
      "epoch": 1.8626506024096385,
      "grad_norm": 1.0506105422973633,
      "learning_rate": 7.587437544610992e-05,
      "loss": 0.0965,
      "step": 3479
    },
    {
      "epoch": 1.8631860776439089,
      "grad_norm": 0.7220376133918762,
      "learning_rate": 7.583868665239116e-05,
      "loss": 0.0487,
      "step": 3480
    },
    {
      "epoch": 1.8637215528781794,
      "grad_norm": 0.7449482083320618,
      "learning_rate": 7.580299785867238e-05,
      "loss": 0.0762,
      "step": 3481
    },
    {
      "epoch": 1.8642570281124498,
      "grad_norm": 1.930682897567749,
      "learning_rate": 7.576730906495361e-05,
      "loss": 0.2239,
      "step": 3482
    },
    {
      "epoch": 1.8647925033467203,
      "grad_norm": 1.6317218542099,
      "learning_rate": 7.573162027123484e-05,
      "loss": 0.0958,
      "step": 3483
    },
    {
      "epoch": 1.8653279785809906,
      "grad_norm": 1.3616821765899658,
      "learning_rate": 7.569593147751606e-05,
      "loss": 0.1495,
      "step": 3484
    },
    {
      "epoch": 1.865863453815261,
      "grad_norm": 0.25474560260772705,
      "learning_rate": 7.566024268379728e-05,
      "loss": 0.0179,
      "step": 3485
    },
    {
      "epoch": 1.8663989290495313,
      "grad_norm": 1.1650158166885376,
      "learning_rate": 7.562455389007852e-05,
      "loss": 0.0729,
      "step": 3486
    },
    {
      "epoch": 1.8669344042838019,
      "grad_norm": 1.0733258724212646,
      "learning_rate": 7.558886509635975e-05,
      "loss": 0.1146,
      "step": 3487
    },
    {
      "epoch": 1.8674698795180724,
      "grad_norm": 1.0984690189361572,
      "learning_rate": 7.555317630264097e-05,
      "loss": 0.078,
      "step": 3488
    },
    {
      "epoch": 1.8680053547523428,
      "grad_norm": 0.5306885838508606,
      "learning_rate": 7.55174875089222e-05,
      "loss": 0.062,
      "step": 3489
    },
    {
      "epoch": 1.868540829986613,
      "grad_norm": 0.9935061931610107,
      "learning_rate": 7.548179871520344e-05,
      "loss": 0.0432,
      "step": 3490
    },
    {
      "epoch": 1.8690763052208834,
      "grad_norm": 6.520660877227783,
      "learning_rate": 7.544610992148466e-05,
      "loss": 0.1227,
      "step": 3491
    },
    {
      "epoch": 1.869611780455154,
      "grad_norm": 1.2127431631088257,
      "learning_rate": 7.541042112776589e-05,
      "loss": 0.0741,
      "step": 3492
    },
    {
      "epoch": 1.8701472556894245,
      "grad_norm": 0.8152380585670471,
      "learning_rate": 7.537473233404711e-05,
      "loss": 0.136,
      "step": 3493
    },
    {
      "epoch": 1.8706827309236949,
      "grad_norm": 0.8468725085258484,
      "learning_rate": 7.533904354032833e-05,
      "loss": 0.1605,
      "step": 3494
    },
    {
      "epoch": 1.8712182061579652,
      "grad_norm": 1.2058768272399902,
      "learning_rate": 7.530335474660956e-05,
      "loss": 0.1328,
      "step": 3495
    },
    {
      "epoch": 1.8717536813922355,
      "grad_norm": 1.240370750427246,
      "learning_rate": 7.52676659528908e-05,
      "loss": 0.1129,
      "step": 3496
    },
    {
      "epoch": 1.872289156626506,
      "grad_norm": 0.6375730633735657,
      "learning_rate": 7.523197715917203e-05,
      "loss": 0.0398,
      "step": 3497
    },
    {
      "epoch": 1.8728246318607764,
      "grad_norm": 0.8828179240226746,
      "learning_rate": 7.519628836545325e-05,
      "loss": 0.1157,
      "step": 3498
    },
    {
      "epoch": 1.873360107095047,
      "grad_norm": 1.0353434085845947,
      "learning_rate": 7.516059957173448e-05,
      "loss": 0.0765,
      "step": 3499
    },
    {
      "epoch": 1.8738955823293173,
      "grad_norm": 0.5478166937828064,
      "learning_rate": 7.512491077801572e-05,
      "loss": 0.0341,
      "step": 3500
    },
    {
      "epoch": 1.8744310575635876,
      "grad_norm": 0.7864735722541809,
      "learning_rate": 7.508922198429694e-05,
      "loss": 0.0978,
      "step": 3501
    },
    {
      "epoch": 1.874966532797858,
      "grad_norm": 0.9356247782707214,
      "learning_rate": 7.505353319057816e-05,
      "loss": 0.0914,
      "step": 3502
    },
    {
      "epoch": 1.8755020080321285,
      "grad_norm": 0.8043166995048523,
      "learning_rate": 7.501784439685939e-05,
      "loss": 0.0675,
      "step": 3503
    },
    {
      "epoch": 1.876037483266399,
      "grad_norm": 0.5580267310142517,
      "learning_rate": 7.498215560314061e-05,
      "loss": 0.0791,
      "step": 3504
    },
    {
      "epoch": 1.8765729585006694,
      "grad_norm": 0.8042739629745483,
      "learning_rate": 7.494646680942184e-05,
      "loss": 0.0543,
      "step": 3505
    },
    {
      "epoch": 1.8771084337349397,
      "grad_norm": 0.6224591135978699,
      "learning_rate": 7.491077801570308e-05,
      "loss": 0.097,
      "step": 3506
    },
    {
      "epoch": 1.87764390896921,
      "grad_norm": 0.4744998514652252,
      "learning_rate": 7.48750892219843e-05,
      "loss": 0.0321,
      "step": 3507
    },
    {
      "epoch": 1.8781793842034806,
      "grad_norm": 0.8357565402984619,
      "learning_rate": 7.483940042826553e-05,
      "loss": 0.1098,
      "step": 3508
    },
    {
      "epoch": 1.878714859437751,
      "grad_norm": 0.7045326828956604,
      "learning_rate": 7.480371163454676e-05,
      "loss": 0.0817,
      "step": 3509
    },
    {
      "epoch": 1.8792503346720215,
      "grad_norm": 0.719015896320343,
      "learning_rate": 7.476802284082798e-05,
      "loss": 0.0831,
      "step": 3510
    },
    {
      "epoch": 1.8797858099062918,
      "grad_norm": 0.5912412405014038,
      "learning_rate": 7.47323340471092e-05,
      "loss": 0.102,
      "step": 3511
    },
    {
      "epoch": 1.8803212851405622,
      "grad_norm": 0.7036956548690796,
      "learning_rate": 7.469664525339044e-05,
      "loss": 0.0824,
      "step": 3512
    },
    {
      "epoch": 1.8808567603748325,
      "grad_norm": 0.3284096121788025,
      "learning_rate": 7.466095645967167e-05,
      "loss": 0.0187,
      "step": 3513
    },
    {
      "epoch": 1.881392235609103,
      "grad_norm": 0.4339878261089325,
      "learning_rate": 7.462526766595289e-05,
      "loss": 0.0568,
      "step": 3514
    },
    {
      "epoch": 1.8819277108433736,
      "grad_norm": 0.15805143117904663,
      "learning_rate": 7.458957887223412e-05,
      "loss": 0.0076,
      "step": 3515
    },
    {
      "epoch": 1.882463186077644,
      "grad_norm": 0.07913484424352646,
      "learning_rate": 7.455389007851536e-05,
      "loss": 0.0024,
      "step": 3516
    },
    {
      "epoch": 1.8829986613119143,
      "grad_norm": 1.1976252794265747,
      "learning_rate": 7.451820128479658e-05,
      "loss": 0.1809,
      "step": 3517
    },
    {
      "epoch": 1.8835341365461846,
      "grad_norm": 0.2972867488861084,
      "learning_rate": 7.448251249107781e-05,
      "loss": 0.0241,
      "step": 3518
    },
    {
      "epoch": 1.8840696117804552,
      "grad_norm": 0.5543740391731262,
      "learning_rate": 7.444682369735903e-05,
      "loss": 0.0668,
      "step": 3519
    },
    {
      "epoch": 1.8846050870147257,
      "grad_norm": 0.42517024278640747,
      "learning_rate": 7.441113490364025e-05,
      "loss": 0.0504,
      "step": 3520
    },
    {
      "epoch": 1.885140562248996,
      "grad_norm": 0.5870122313499451,
      "learning_rate": 7.437544610992148e-05,
      "loss": 0.0657,
      "step": 3521
    },
    {
      "epoch": 1.8856760374832664,
      "grad_norm": 0.6274899840354919,
      "learning_rate": 7.433975731620272e-05,
      "loss": 0.101,
      "step": 3522
    },
    {
      "epoch": 1.8862115127175367,
      "grad_norm": 0.9181696772575378,
      "learning_rate": 7.430406852248394e-05,
      "loss": 0.092,
      "step": 3523
    },
    {
      "epoch": 1.886746987951807,
      "grad_norm": 0.9082942008972168,
      "learning_rate": 7.426837972876517e-05,
      "loss": 0.0669,
      "step": 3524
    },
    {
      "epoch": 1.8872824631860776,
      "grad_norm": 0.7685887217521667,
      "learning_rate": 7.42326909350464e-05,
      "loss": 0.07,
      "step": 3525
    },
    {
      "epoch": 1.8878179384203482,
      "grad_norm": 0.7639548182487488,
      "learning_rate": 7.419700214132764e-05,
      "loss": 0.1263,
      "step": 3526
    },
    {
      "epoch": 1.8883534136546185,
      "grad_norm": 0.5865436792373657,
      "learning_rate": 7.416131334760886e-05,
      "loss": 0.0961,
      "step": 3527
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 0.6326198577880859,
      "learning_rate": 7.412562455389008e-05,
      "loss": 0.062,
      "step": 3528
    },
    {
      "epoch": 1.8894243641231592,
      "grad_norm": 0.5175685286521912,
      "learning_rate": 7.408993576017131e-05,
      "loss": 0.0421,
      "step": 3529
    },
    {
      "epoch": 1.8899598393574297,
      "grad_norm": 0.8465824127197266,
      "learning_rate": 7.405424696645253e-05,
      "loss": 0.0937,
      "step": 3530
    },
    {
      "epoch": 1.8904953145917003,
      "grad_norm": 0.36205384135246277,
      "learning_rate": 7.401855817273376e-05,
      "loss": 0.0525,
      "step": 3531
    },
    {
      "epoch": 1.8910307898259706,
      "grad_norm": 0.7364829778671265,
      "learning_rate": 7.3982869379015e-05,
      "loss": 0.1461,
      "step": 3532
    },
    {
      "epoch": 1.891566265060241,
      "grad_norm": 1.1441444158554077,
      "learning_rate": 7.394718058529622e-05,
      "loss": 0.1744,
      "step": 3533
    },
    {
      "epoch": 1.8921017402945113,
      "grad_norm": 0.8953832983970642,
      "learning_rate": 7.391149179157745e-05,
      "loss": 0.089,
      "step": 3534
    },
    {
      "epoch": 1.8926372155287818,
      "grad_norm": 1.3154425621032715,
      "learning_rate": 7.387580299785868e-05,
      "loss": 0.156,
      "step": 3535
    },
    {
      "epoch": 1.8931726907630522,
      "grad_norm": 2.2977352142333984,
      "learning_rate": 7.38401142041399e-05,
      "loss": 0.1284,
      "step": 3536
    },
    {
      "epoch": 1.8937081659973227,
      "grad_norm": 2.2669999599456787,
      "learning_rate": 7.380442541042112e-05,
      "loss": 0.0798,
      "step": 3537
    },
    {
      "epoch": 1.894243641231593,
      "grad_norm": 0.31671130657196045,
      "learning_rate": 7.376873661670236e-05,
      "loss": 0.0502,
      "step": 3538
    },
    {
      "epoch": 1.8947791164658634,
      "grad_norm": 1.0733009576797485,
      "learning_rate": 7.373304782298359e-05,
      "loss": 0.0861,
      "step": 3539
    },
    {
      "epoch": 1.8953145917001337,
      "grad_norm": 0.445187509059906,
      "learning_rate": 7.369735902926481e-05,
      "loss": 0.0584,
      "step": 3540
    },
    {
      "epoch": 1.8958500669344043,
      "grad_norm": 1.3279322385787964,
      "learning_rate": 7.366167023554604e-05,
      "loss": 0.0967,
      "step": 3541
    },
    {
      "epoch": 1.8963855421686748,
      "grad_norm": 1.0163437128067017,
      "learning_rate": 7.362598144182728e-05,
      "loss": 0.1804,
      "step": 3542
    },
    {
      "epoch": 1.8969210174029452,
      "grad_norm": 0.9698280692100525,
      "learning_rate": 7.35902926481085e-05,
      "loss": 0.1322,
      "step": 3543
    },
    {
      "epoch": 1.8974564926372155,
      "grad_norm": 0.7711978554725647,
      "learning_rate": 7.355460385438973e-05,
      "loss": 0.1167,
      "step": 3544
    },
    {
      "epoch": 1.8979919678714858,
      "grad_norm": 1.0760585069656372,
      "learning_rate": 7.351891506067095e-05,
      "loss": 0.0981,
      "step": 3545
    },
    {
      "epoch": 1.8985274431057564,
      "grad_norm": 0.19454702734947205,
      "learning_rate": 7.348322626695217e-05,
      "loss": 0.0128,
      "step": 3546
    },
    {
      "epoch": 1.899062918340027,
      "grad_norm": 0.9262656569480896,
      "learning_rate": 7.34475374732334e-05,
      "loss": 0.0735,
      "step": 3547
    },
    {
      "epoch": 1.8995983935742973,
      "grad_norm": 0.8766844868659973,
      "learning_rate": 7.341184867951464e-05,
      "loss": 0.1327,
      "step": 3548
    },
    {
      "epoch": 1.9001338688085676,
      "grad_norm": 0.6431966423988342,
      "learning_rate": 7.337615988579586e-05,
      "loss": 0.1005,
      "step": 3549
    },
    {
      "epoch": 1.900669344042838,
      "grad_norm": 0.48382070660591125,
      "learning_rate": 7.334047109207709e-05,
      "loss": 0.0329,
      "step": 3550
    },
    {
      "epoch": 1.9012048192771083,
      "grad_norm": 1.976232886314392,
      "learning_rate": 7.330478229835832e-05,
      "loss": 0.2408,
      "step": 3551
    },
    {
      "epoch": 1.9017402945113788,
      "grad_norm": 0.8352543115615845,
      "learning_rate": 7.326909350463956e-05,
      "loss": 0.1229,
      "step": 3552
    },
    {
      "epoch": 1.9022757697456494,
      "grad_norm": 0.49966123700141907,
      "learning_rate": 7.323340471092078e-05,
      "loss": 0.0267,
      "step": 3553
    },
    {
      "epoch": 1.9028112449799197,
      "grad_norm": 1.88463294506073,
      "learning_rate": 7.3197715917202e-05,
      "loss": 0.1786,
      "step": 3554
    },
    {
      "epoch": 1.90334672021419,
      "grad_norm": 1.133846640586853,
      "learning_rate": 7.316202712348323e-05,
      "loss": 0.1262,
      "step": 3555
    },
    {
      "epoch": 1.9038821954484604,
      "grad_norm": 0.28860199451446533,
      "learning_rate": 7.312633832976445e-05,
      "loss": 0.0473,
      "step": 3556
    },
    {
      "epoch": 1.904417670682731,
      "grad_norm": 0.7352806925773621,
      "learning_rate": 7.309064953604568e-05,
      "loss": 0.0618,
      "step": 3557
    },
    {
      "epoch": 1.9049531459170015,
      "grad_norm": 0.8449399471282959,
      "learning_rate": 7.305496074232692e-05,
      "loss": 0.073,
      "step": 3558
    },
    {
      "epoch": 1.9054886211512718,
      "grad_norm": 0.5158072113990784,
      "learning_rate": 7.301927194860814e-05,
      "loss": 0.0579,
      "step": 3559
    },
    {
      "epoch": 1.9060240963855422,
      "grad_norm": 0.9213280081748962,
      "learning_rate": 7.298358315488937e-05,
      "loss": 0.1582,
      "step": 3560
    },
    {
      "epoch": 1.9065595716198125,
      "grad_norm": 0.8393397927284241,
      "learning_rate": 7.29478943611706e-05,
      "loss": 0.09,
      "step": 3561
    },
    {
      "epoch": 1.907095046854083,
      "grad_norm": 0.8331878185272217,
      "learning_rate": 7.291220556745182e-05,
      "loss": 0.128,
      "step": 3562
    },
    {
      "epoch": 1.9076305220883534,
      "grad_norm": 0.7349236011505127,
      "learning_rate": 7.287651677373304e-05,
      "loss": 0.0717,
      "step": 3563
    },
    {
      "epoch": 1.908165997322624,
      "grad_norm": 0.5742255449295044,
      "learning_rate": 7.284082798001428e-05,
      "loss": 0.0945,
      "step": 3564
    },
    {
      "epoch": 1.9087014725568943,
      "grad_norm": 0.5423439145088196,
      "learning_rate": 7.28051391862955e-05,
      "loss": 0.0702,
      "step": 3565
    },
    {
      "epoch": 1.9092369477911646,
      "grad_norm": 0.6074624061584473,
      "learning_rate": 7.276945039257673e-05,
      "loss": 0.0712,
      "step": 3566
    },
    {
      "epoch": 1.909772423025435,
      "grad_norm": 0.5140739679336548,
      "learning_rate": 7.273376159885796e-05,
      "loss": 0.054,
      "step": 3567
    },
    {
      "epoch": 1.9103078982597055,
      "grad_norm": 0.48354771733283997,
      "learning_rate": 7.26980728051392e-05,
      "loss": 0.0583,
      "step": 3568
    },
    {
      "epoch": 1.910843373493976,
      "grad_norm": 0.16897810995578766,
      "learning_rate": 7.266238401142042e-05,
      "loss": 0.0095,
      "step": 3569
    },
    {
      "epoch": 1.9113788487282464,
      "grad_norm": 0.8304959535598755,
      "learning_rate": 7.262669521770165e-05,
      "loss": 0.0852,
      "step": 3570
    },
    {
      "epoch": 1.9119143239625167,
      "grad_norm": 0.8697085976600647,
      "learning_rate": 7.259100642398287e-05,
      "loss": 0.1359,
      "step": 3571
    },
    {
      "epoch": 1.912449799196787,
      "grad_norm": 0.8853593468666077,
      "learning_rate": 7.25553176302641e-05,
      "loss": 0.1217,
      "step": 3572
    },
    {
      "epoch": 1.9129852744310576,
      "grad_norm": 0.6463549733161926,
      "learning_rate": 7.251962883654532e-05,
      "loss": 0.0616,
      "step": 3573
    },
    {
      "epoch": 1.913520749665328,
      "grad_norm": 0.9651440978050232,
      "learning_rate": 7.248394004282656e-05,
      "loss": 0.1381,
      "step": 3574
    },
    {
      "epoch": 1.9140562248995985,
      "grad_norm": 0.722254753112793,
      "learning_rate": 7.244825124910778e-05,
      "loss": 0.1098,
      "step": 3575
    },
    {
      "epoch": 1.9145917001338688,
      "grad_norm": 0.4566696882247925,
      "learning_rate": 7.241256245538901e-05,
      "loss": 0.0613,
      "step": 3576
    },
    {
      "epoch": 1.9151271753681391,
      "grad_norm": 0.7017666697502136,
      "learning_rate": 7.237687366167024e-05,
      "loss": 0.08,
      "step": 3577
    },
    {
      "epoch": 1.9156626506024095,
      "grad_norm": 0.7881905436515808,
      "learning_rate": 7.234118486795146e-05,
      "loss": 0.0546,
      "step": 3578
    },
    {
      "epoch": 1.91619812583668,
      "grad_norm": 1.027376413345337,
      "learning_rate": 7.23054960742327e-05,
      "loss": 0.1291,
      "step": 3579
    },
    {
      "epoch": 1.9167336010709506,
      "grad_norm": 0.6750479340553284,
      "learning_rate": 7.226980728051393e-05,
      "loss": 0.0555,
      "step": 3580
    },
    {
      "epoch": 1.917269076305221,
      "grad_norm": 0.9712252020835876,
      "learning_rate": 7.223411848679515e-05,
      "loss": 0.1285,
      "step": 3581
    },
    {
      "epoch": 1.9178045515394913,
      "grad_norm": 0.7114648222923279,
      "learning_rate": 7.219842969307637e-05,
      "loss": 0.0762,
      "step": 3582
    },
    {
      "epoch": 1.9183400267737616,
      "grad_norm": 1.2556861639022827,
      "learning_rate": 7.21627408993576e-05,
      "loss": 0.0791,
      "step": 3583
    },
    {
      "epoch": 1.9188755020080321,
      "grad_norm": 1.248457670211792,
      "learning_rate": 7.212705210563884e-05,
      "loss": 0.1172,
      "step": 3584
    },
    {
      "epoch": 1.9194109772423027,
      "grad_norm": 0.8349844217300415,
      "learning_rate": 7.209136331192006e-05,
      "loss": 0.0932,
      "step": 3585
    },
    {
      "epoch": 1.919946452476573,
      "grad_norm": 0.5491934418678284,
      "learning_rate": 7.205567451820129e-05,
      "loss": 0.081,
      "step": 3586
    },
    {
      "epoch": 1.9204819277108434,
      "grad_norm": 1.386627435684204,
      "learning_rate": 7.201998572448252e-05,
      "loss": 0.1028,
      "step": 3587
    },
    {
      "epoch": 1.9210174029451137,
      "grad_norm": 0.8277608156204224,
      "learning_rate": 7.198429693076374e-05,
      "loss": 0.0932,
      "step": 3588
    },
    {
      "epoch": 1.9215528781793842,
      "grad_norm": 0.6505936980247498,
      "learning_rate": 7.194860813704498e-05,
      "loss": 0.0639,
      "step": 3589
    },
    {
      "epoch": 1.9220883534136546,
      "grad_norm": 0.9155027866363525,
      "learning_rate": 7.19129193433262e-05,
      "loss": 0.1126,
      "step": 3590
    },
    {
      "epoch": 1.9226238286479251,
      "grad_norm": 0.4911181330680847,
      "learning_rate": 7.187723054960742e-05,
      "loss": 0.0662,
      "step": 3591
    },
    {
      "epoch": 1.9231593038821955,
      "grad_norm": 1.1797763109207153,
      "learning_rate": 7.184154175588865e-05,
      "loss": 0.1227,
      "step": 3592
    },
    {
      "epoch": 1.9236947791164658,
      "grad_norm": 1.1343674659729004,
      "learning_rate": 7.180585296216988e-05,
      "loss": 0.1315,
      "step": 3593
    },
    {
      "epoch": 1.9242302543507361,
      "grad_norm": 0.927508533000946,
      "learning_rate": 7.177016416845112e-05,
      "loss": 0.0872,
      "step": 3594
    },
    {
      "epoch": 1.9247657295850067,
      "grad_norm": 0.5287267565727234,
      "learning_rate": 7.173447537473234e-05,
      "loss": 0.0474,
      "step": 3595
    },
    {
      "epoch": 1.9253012048192772,
      "grad_norm": 0.9333932399749756,
      "learning_rate": 7.169878658101357e-05,
      "loss": 0.0981,
      "step": 3596
    },
    {
      "epoch": 1.9258366800535476,
      "grad_norm": 0.8296591639518738,
      "learning_rate": 7.16630977872948e-05,
      "loss": 0.0901,
      "step": 3597
    },
    {
      "epoch": 1.926372155287818,
      "grad_norm": 0.6215788722038269,
      "learning_rate": 7.162740899357602e-05,
      "loss": 0.0352,
      "step": 3598
    },
    {
      "epoch": 1.9269076305220882,
      "grad_norm": 0.9478033781051636,
      "learning_rate": 7.159172019985724e-05,
      "loss": 0.0981,
      "step": 3599
    },
    {
      "epoch": 1.9274431057563588,
      "grad_norm": 0.7350337505340576,
      "learning_rate": 7.155603140613848e-05,
      "loss": 0.0684,
      "step": 3600
    },
    {
      "epoch": 1.9279785809906291,
      "grad_norm": 1.5462431907653809,
      "learning_rate": 7.15203426124197e-05,
      "loss": 0.0515,
      "step": 3601
    },
    {
      "epoch": 1.9285140562248997,
      "grad_norm": 1.7996543645858765,
      "learning_rate": 7.148465381870093e-05,
      "loss": 0.0757,
      "step": 3602
    },
    {
      "epoch": 1.92904953145917,
      "grad_norm": 0.3929179608821869,
      "learning_rate": 7.144896502498216e-05,
      "loss": 0.0295,
      "step": 3603
    },
    {
      "epoch": 1.9295850066934404,
      "grad_norm": 1.7656668424606323,
      "learning_rate": 7.141327623126338e-05,
      "loss": 0.1716,
      "step": 3604
    },
    {
      "epoch": 1.9301204819277107,
      "grad_norm": 1.047224998474121,
      "learning_rate": 7.137758743754462e-05,
      "loss": 0.1043,
      "step": 3605
    },
    {
      "epoch": 1.9306559571619812,
      "grad_norm": 1.0290255546569824,
      "learning_rate": 7.134189864382585e-05,
      "loss": 0.1529,
      "step": 3606
    },
    {
      "epoch": 1.9311914323962518,
      "grad_norm": 8.696611404418945,
      "learning_rate": 7.130620985010707e-05,
      "loss": 0.1525,
      "step": 3607
    },
    {
      "epoch": 1.9317269076305221,
      "grad_norm": 0.551236093044281,
      "learning_rate": 7.127052105638829e-05,
      "loss": 0.0533,
      "step": 3608
    },
    {
      "epoch": 1.9322623828647925,
      "grad_norm": 1.1752821207046509,
      "learning_rate": 7.123483226266952e-05,
      "loss": 0.1022,
      "step": 3609
    },
    {
      "epoch": 1.9327978580990628,
      "grad_norm": 0.8181456327438354,
      "learning_rate": 7.119914346895076e-05,
      "loss": 0.0184,
      "step": 3610
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.5718756914138794,
      "learning_rate": 7.116345467523198e-05,
      "loss": 0.0405,
      "step": 3611
    },
    {
      "epoch": 1.933868808567604,
      "grad_norm": 5.902153968811035,
      "learning_rate": 7.112776588151321e-05,
      "loss": 0.0738,
      "step": 3612
    },
    {
      "epoch": 1.9344042838018742,
      "grad_norm": 0.9065760374069214,
      "learning_rate": 7.109207708779444e-05,
      "loss": 0.048,
      "step": 3613
    },
    {
      "epoch": 1.9349397590361446,
      "grad_norm": 1.3087542057037354,
      "learning_rate": 7.105638829407566e-05,
      "loss": 0.1056,
      "step": 3614
    },
    {
      "epoch": 1.935475234270415,
      "grad_norm": 0.541831374168396,
      "learning_rate": 7.10206995003569e-05,
      "loss": 0.0403,
      "step": 3615
    },
    {
      "epoch": 1.9360107095046855,
      "grad_norm": 0.6908935308456421,
      "learning_rate": 7.098501070663812e-05,
      "loss": 0.0816,
      "step": 3616
    },
    {
      "epoch": 1.9365461847389558,
      "grad_norm": 1.2579185962677002,
      "learning_rate": 7.094932191291934e-05,
      "loss": 0.1116,
      "step": 3617
    },
    {
      "epoch": 1.9370816599732263,
      "grad_norm": 2.3299782276153564,
      "learning_rate": 7.091363311920057e-05,
      "loss": 0.1509,
      "step": 3618
    },
    {
      "epoch": 1.9376171352074967,
      "grad_norm": 0.701202392578125,
      "learning_rate": 7.08779443254818e-05,
      "loss": 0.0579,
      "step": 3619
    },
    {
      "epoch": 1.938152610441767,
      "grad_norm": 1.5319150686264038,
      "learning_rate": 7.084225553176302e-05,
      "loss": 0.0835,
      "step": 3620
    },
    {
      "epoch": 1.9386880856760373,
      "grad_norm": 1.030829668045044,
      "learning_rate": 7.080656673804426e-05,
      "loss": 0.1581,
      "step": 3621
    },
    {
      "epoch": 1.939223560910308,
      "grad_norm": 0.7775757908821106,
      "learning_rate": 7.077087794432549e-05,
      "loss": 0.0535,
      "step": 3622
    },
    {
      "epoch": 1.9397590361445785,
      "grad_norm": 0.8068910241127014,
      "learning_rate": 7.073518915060672e-05,
      "loss": 0.0461,
      "step": 3623
    },
    {
      "epoch": 1.9402945113788488,
      "grad_norm": 0.25300949811935425,
      "learning_rate": 7.069950035688794e-05,
      "loss": 0.0127,
      "step": 3624
    },
    {
      "epoch": 1.9408299866131191,
      "grad_norm": 0.40879538655281067,
      "learning_rate": 7.066381156316916e-05,
      "loss": 0.025,
      "step": 3625
    },
    {
      "epoch": 1.9413654618473895,
      "grad_norm": 0.8319774270057678,
      "learning_rate": 7.06281227694504e-05,
      "loss": 0.0919,
      "step": 3626
    },
    {
      "epoch": 1.94190093708166,
      "grad_norm": 0.7303526401519775,
      "learning_rate": 7.059243397573162e-05,
      "loss": 0.0898,
      "step": 3627
    },
    {
      "epoch": 1.9424364123159303,
      "grad_norm": 0.4255087673664093,
      "learning_rate": 7.055674518201285e-05,
      "loss": 0.0613,
      "step": 3628
    },
    {
      "epoch": 1.942971887550201,
      "grad_norm": 1.1015836000442505,
      "learning_rate": 7.052105638829408e-05,
      "loss": 0.1502,
      "step": 3629
    },
    {
      "epoch": 1.9435073627844712,
      "grad_norm": 0.506083071231842,
      "learning_rate": 7.04853675945753e-05,
      "loss": 0.0289,
      "step": 3630
    },
    {
      "epoch": 1.9440428380187416,
      "grad_norm": 0.7612352967262268,
      "learning_rate": 7.044967880085654e-05,
      "loss": 0.0516,
      "step": 3631
    },
    {
      "epoch": 1.944578313253012,
      "grad_norm": 0.6549001336097717,
      "learning_rate": 7.041399000713777e-05,
      "loss": 0.0733,
      "step": 3632
    },
    {
      "epoch": 1.9451137884872824,
      "grad_norm": 0.20482167601585388,
      "learning_rate": 7.037830121341899e-05,
      "loss": 0.01,
      "step": 3633
    },
    {
      "epoch": 1.945649263721553,
      "grad_norm": 0.725508451461792,
      "learning_rate": 7.034261241970021e-05,
      "loss": 0.0531,
      "step": 3634
    },
    {
      "epoch": 1.9461847389558233,
      "grad_norm": 0.6900098919868469,
      "learning_rate": 7.030692362598144e-05,
      "loss": 0.1085,
      "step": 3635
    },
    {
      "epoch": 1.9467202141900937,
      "grad_norm": 1.1890645027160645,
      "learning_rate": 7.027123483226268e-05,
      "loss": 0.0901,
      "step": 3636
    },
    {
      "epoch": 1.947255689424364,
      "grad_norm": 1.1071513891220093,
      "learning_rate": 7.02355460385439e-05,
      "loss": 0.0538,
      "step": 3637
    },
    {
      "epoch": 1.9477911646586346,
      "grad_norm": 0.8290926218032837,
      "learning_rate": 7.019985724482513e-05,
      "loss": 0.0428,
      "step": 3638
    },
    {
      "epoch": 1.948326639892905,
      "grad_norm": 0.763468861579895,
      "learning_rate": 7.016416845110636e-05,
      "loss": 0.0801,
      "step": 3639
    },
    {
      "epoch": 1.9488621151271754,
      "grad_norm": 0.7921121120452881,
      "learning_rate": 7.012847965738758e-05,
      "loss": 0.0845,
      "step": 3640
    },
    {
      "epoch": 1.9493975903614458,
      "grad_norm": 3.2329459190368652,
      "learning_rate": 7.009279086366882e-05,
      "loss": 0.12,
      "step": 3641
    },
    {
      "epoch": 1.949933065595716,
      "grad_norm": 0.8039023280143738,
      "learning_rate": 7.005710206995004e-05,
      "loss": 0.0886,
      "step": 3642
    },
    {
      "epoch": 1.9504685408299867,
      "grad_norm": 0.5995399355888367,
      "learning_rate": 7.002141327623126e-05,
      "loss": 0.0385,
      "step": 3643
    },
    {
      "epoch": 1.951004016064257,
      "grad_norm": 0.8715289831161499,
      "learning_rate": 6.998572448251249e-05,
      "loss": 0.0937,
      "step": 3644
    },
    {
      "epoch": 1.9515394912985276,
      "grad_norm": 1.1260313987731934,
      "learning_rate": 6.995003568879372e-05,
      "loss": 0.1571,
      "step": 3645
    },
    {
      "epoch": 1.9520749665327979,
      "grad_norm": 0.12476852536201477,
      "learning_rate": 6.991434689507494e-05,
      "loss": 0.0038,
      "step": 3646
    },
    {
      "epoch": 1.9526104417670682,
      "grad_norm": 0.6736873388290405,
      "learning_rate": 6.987865810135618e-05,
      "loss": 0.087,
      "step": 3647
    },
    {
      "epoch": 1.9531459170013385,
      "grad_norm": 1.1798748970031738,
      "learning_rate": 6.984296930763741e-05,
      "loss": 0.0559,
      "step": 3648
    },
    {
      "epoch": 1.953681392235609,
      "grad_norm": 1.3867907524108887,
      "learning_rate": 6.980728051391864e-05,
      "loss": 0.0492,
      "step": 3649
    },
    {
      "epoch": 1.9542168674698797,
      "grad_norm": 0.9966058731079102,
      "learning_rate": 6.977159172019986e-05,
      "loss": 0.0681,
      "step": 3650
    },
    {
      "epoch": 1.95475234270415,
      "grad_norm": 1.2503764629364014,
      "learning_rate": 6.973590292648108e-05,
      "loss": 0.1492,
      "step": 3651
    },
    {
      "epoch": 1.9552878179384203,
      "grad_norm": 5.071109771728516,
      "learning_rate": 6.970021413276232e-05,
      "loss": 0.076,
      "step": 3652
    },
    {
      "epoch": 1.9558232931726907,
      "grad_norm": 1.2329411506652832,
      "learning_rate": 6.966452533904354e-05,
      "loss": 0.1443,
      "step": 3653
    },
    {
      "epoch": 1.9563587684069612,
      "grad_norm": 0.6945968866348267,
      "learning_rate": 6.962883654532477e-05,
      "loss": 0.0678,
      "step": 3654
    },
    {
      "epoch": 1.9568942436412315,
      "grad_norm": 0.48039665818214417,
      "learning_rate": 6.9593147751606e-05,
      "loss": 0.04,
      "step": 3655
    },
    {
      "epoch": 1.957429718875502,
      "grad_norm": 0.6863126158714294,
      "learning_rate": 6.955745895788722e-05,
      "loss": 0.0963,
      "step": 3656
    },
    {
      "epoch": 1.9579651941097724,
      "grad_norm": 0.9164527654647827,
      "learning_rate": 6.952177016416846e-05,
      "loss": 0.1094,
      "step": 3657
    },
    {
      "epoch": 1.9585006693440428,
      "grad_norm": 1.5101006031036377,
      "learning_rate": 6.948608137044969e-05,
      "loss": 0.1466,
      "step": 3658
    },
    {
      "epoch": 1.959036144578313,
      "grad_norm": 1.1735798120498657,
      "learning_rate": 6.945039257673091e-05,
      "loss": 0.1623,
      "step": 3659
    },
    {
      "epoch": 1.9595716198125837,
      "grad_norm": 1.2255101203918457,
      "learning_rate": 6.941470378301213e-05,
      "loss": 0.131,
      "step": 3660
    },
    {
      "epoch": 1.9601070950468542,
      "grad_norm": 0.8651978969573975,
      "learning_rate": 6.937901498929336e-05,
      "loss": 0.1165,
      "step": 3661
    },
    {
      "epoch": 1.9606425702811245,
      "grad_norm": 1.0672543048858643,
      "learning_rate": 6.93433261955746e-05,
      "loss": 0.18,
      "step": 3662
    },
    {
      "epoch": 1.9611780455153949,
      "grad_norm": 1.2483453750610352,
      "learning_rate": 6.930763740185582e-05,
      "loss": 0.1002,
      "step": 3663
    },
    {
      "epoch": 1.9617135207496652,
      "grad_norm": 0.5837464332580566,
      "learning_rate": 6.927194860813705e-05,
      "loss": 0.046,
      "step": 3664
    },
    {
      "epoch": 1.9622489959839358,
      "grad_norm": 0.6815006136894226,
      "learning_rate": 6.923625981441828e-05,
      "loss": 0.0544,
      "step": 3665
    },
    {
      "epoch": 1.9627844712182063,
      "grad_norm": 0.3205311894416809,
      "learning_rate": 6.92005710206995e-05,
      "loss": 0.0441,
      "step": 3666
    },
    {
      "epoch": 1.9633199464524766,
      "grad_norm": 0.9525973200798035,
      "learning_rate": 6.916488222698074e-05,
      "loss": 0.1068,
      "step": 3667
    },
    {
      "epoch": 1.963855421686747,
      "grad_norm": 0.6245584487915039,
      "learning_rate": 6.912919343326196e-05,
      "loss": 0.0843,
      "step": 3668
    },
    {
      "epoch": 1.9643908969210173,
      "grad_norm": 0.6823186874389648,
      "learning_rate": 6.909350463954318e-05,
      "loss": 0.0866,
      "step": 3669
    },
    {
      "epoch": 1.9649263721552879,
      "grad_norm": 0.6511375308036804,
      "learning_rate": 6.905781584582441e-05,
      "loss": 0.0802,
      "step": 3670
    },
    {
      "epoch": 1.9654618473895582,
      "grad_norm": 0.8678598403930664,
      "learning_rate": 6.902212705210564e-05,
      "loss": 0.1137,
      "step": 3671
    },
    {
      "epoch": 1.9659973226238288,
      "grad_norm": 0.9096899628639221,
      "learning_rate": 6.898643825838686e-05,
      "loss": 0.0864,
      "step": 3672
    },
    {
      "epoch": 1.966532797858099,
      "grad_norm": 0.8153203129768372,
      "learning_rate": 6.89507494646681e-05,
      "loss": 0.0973,
      "step": 3673
    },
    {
      "epoch": 1.9670682730923694,
      "grad_norm": 0.6901244521141052,
      "learning_rate": 6.891506067094933e-05,
      "loss": 0.1068,
      "step": 3674
    },
    {
      "epoch": 1.9676037483266398,
      "grad_norm": 0.5883651375770569,
      "learning_rate": 6.887937187723055e-05,
      "loss": 0.0491,
      "step": 3675
    },
    {
      "epoch": 1.9681392235609103,
      "grad_norm": 1.2693078517913818,
      "learning_rate": 6.884368308351178e-05,
      "loss": 0.1476,
      "step": 3676
    },
    {
      "epoch": 1.9686746987951809,
      "grad_norm": 1.1456633806228638,
      "learning_rate": 6.8807994289793e-05,
      "loss": 0.0959,
      "step": 3677
    },
    {
      "epoch": 1.9692101740294512,
      "grad_norm": 0.3303292691707611,
      "learning_rate": 6.877230549607424e-05,
      "loss": 0.0169,
      "step": 3678
    },
    {
      "epoch": 1.9697456492637215,
      "grad_norm": 0.5055719614028931,
      "learning_rate": 6.873661670235546e-05,
      "loss": 0.0797,
      "step": 3679
    },
    {
      "epoch": 1.9702811244979919,
      "grad_norm": 3.65175724029541,
      "learning_rate": 6.870092790863669e-05,
      "loss": 0.0534,
      "step": 3680
    },
    {
      "epoch": 1.9708165997322624,
      "grad_norm": 0.8838924169540405,
      "learning_rate": 6.866523911491792e-05,
      "loss": 0.0787,
      "step": 3681
    },
    {
      "epoch": 1.9713520749665328,
      "grad_norm": 0.5438567399978638,
      "learning_rate": 6.862955032119914e-05,
      "loss": 0.0355,
      "step": 3682
    },
    {
      "epoch": 1.9718875502008033,
      "grad_norm": 1.0807147026062012,
      "learning_rate": 6.859386152748038e-05,
      "loss": 0.129,
      "step": 3683
    },
    {
      "epoch": 1.9724230254350736,
      "grad_norm": 1.2152570486068726,
      "learning_rate": 6.855817273376161e-05,
      "loss": 0.1248,
      "step": 3684
    },
    {
      "epoch": 1.972958500669344,
      "grad_norm": 1.0476011037826538,
      "learning_rate": 6.852248394004283e-05,
      "loss": 0.1032,
      "step": 3685
    },
    {
      "epoch": 1.9734939759036143,
      "grad_norm": 0.6500994563102722,
      "learning_rate": 6.848679514632405e-05,
      "loss": 0.0692,
      "step": 3686
    },
    {
      "epoch": 1.9740294511378849,
      "grad_norm": 0.7204658389091492,
      "learning_rate": 6.845110635260528e-05,
      "loss": 0.0801,
      "step": 3687
    },
    {
      "epoch": 1.9745649263721554,
      "grad_norm": 0.7420520782470703,
      "learning_rate": 6.84154175588865e-05,
      "loss": 0.1142,
      "step": 3688
    },
    {
      "epoch": 1.9751004016064257,
      "grad_norm": 1.0435820817947388,
      "learning_rate": 6.837972876516774e-05,
      "loss": 0.1226,
      "step": 3689
    },
    {
      "epoch": 1.975635876840696,
      "grad_norm": 1.0574254989624023,
      "learning_rate": 6.834403997144897e-05,
      "loss": 0.0775,
      "step": 3690
    },
    {
      "epoch": 1.9761713520749664,
      "grad_norm": 0.7843729257583618,
      "learning_rate": 6.83083511777302e-05,
      "loss": 0.0733,
      "step": 3691
    },
    {
      "epoch": 1.976706827309237,
      "grad_norm": 2.365297555923462,
      "learning_rate": 6.827266238401142e-05,
      "loss": 0.1431,
      "step": 3692
    },
    {
      "epoch": 1.9772423025435075,
      "grad_norm": 0.6389465928077698,
      "learning_rate": 6.823697359029266e-05,
      "loss": 0.0628,
      "step": 3693
    },
    {
      "epoch": 1.9777777777777779,
      "grad_norm": 1.4250613451004028,
      "learning_rate": 6.820128479657388e-05,
      "loss": 0.0937,
      "step": 3694
    },
    {
      "epoch": 1.9783132530120482,
      "grad_norm": 1.1882082223892212,
      "learning_rate": 6.816559600285511e-05,
      "loss": 0.0895,
      "step": 3695
    },
    {
      "epoch": 1.9788487282463185,
      "grad_norm": 0.5561718940734863,
      "learning_rate": 6.812990720913633e-05,
      "loss": 0.0746,
      "step": 3696
    },
    {
      "epoch": 1.979384203480589,
      "grad_norm": 0.31745144724845886,
      "learning_rate": 6.809421841541756e-05,
      "loss": 0.0308,
      "step": 3697
    },
    {
      "epoch": 1.9799196787148594,
      "grad_norm": 0.7006827592849731,
      "learning_rate": 6.805852962169878e-05,
      "loss": 0.0977,
      "step": 3698
    },
    {
      "epoch": 1.98045515394913,
      "grad_norm": 0.9654658436775208,
      "learning_rate": 6.802284082798002e-05,
      "loss": 0.1047,
      "step": 3699
    },
    {
      "epoch": 1.9809906291834003,
      "grad_norm": 1.6679366827011108,
      "learning_rate": 6.798715203426125e-05,
      "loss": 0.2002,
      "step": 3700
    },
    {
      "epoch": 1.9815261044176706,
      "grad_norm": 0.705702543258667,
      "learning_rate": 6.795146324054247e-05,
      "loss": 0.063,
      "step": 3701
    },
    {
      "epoch": 1.982061579651941,
      "grad_norm": 1.0773345232009888,
      "learning_rate": 6.79157744468237e-05,
      "loss": 0.1142,
      "step": 3702
    },
    {
      "epoch": 1.9825970548862115,
      "grad_norm": 0.8822324275970459,
      "learning_rate": 6.788008565310494e-05,
      "loss": 0.0582,
      "step": 3703
    },
    {
      "epoch": 1.983132530120482,
      "grad_norm": 0.9176386594772339,
      "learning_rate": 6.784439685938616e-05,
      "loss": 0.0762,
      "step": 3704
    },
    {
      "epoch": 1.9836680053547524,
      "grad_norm": 3.8845956325531006,
      "learning_rate": 6.780870806566738e-05,
      "loss": 0.1858,
      "step": 3705
    },
    {
      "epoch": 1.9842034805890227,
      "grad_norm": 0.9916679263114929,
      "learning_rate": 6.777301927194861e-05,
      "loss": 0.1716,
      "step": 3706
    },
    {
      "epoch": 1.984738955823293,
      "grad_norm": 3.1339008808135986,
      "learning_rate": 6.773733047822984e-05,
      "loss": 0.0824,
      "step": 3707
    },
    {
      "epoch": 1.9852744310575636,
      "grad_norm": 0.4429818391799927,
      "learning_rate": 6.770164168451106e-05,
      "loss": 0.075,
      "step": 3708
    },
    {
      "epoch": 1.985809906291834,
      "grad_norm": 0.549315333366394,
      "learning_rate": 6.76659528907923e-05,
      "loss": 0.0317,
      "step": 3709
    },
    {
      "epoch": 1.9863453815261045,
      "grad_norm": 0.8449345231056213,
      "learning_rate": 6.763026409707353e-05,
      "loss": 0.0884,
      "step": 3710
    },
    {
      "epoch": 1.9868808567603748,
      "grad_norm": 1.017861008644104,
      "learning_rate": 6.759457530335475e-05,
      "loss": 0.0452,
      "step": 3711
    },
    {
      "epoch": 1.9874163319946452,
      "grad_norm": 0.6762430667877197,
      "learning_rate": 6.755888650963598e-05,
      "loss": 0.1195,
      "step": 3712
    },
    {
      "epoch": 1.9879518072289155,
      "grad_norm": 0.7951171398162842,
      "learning_rate": 6.75231977159172e-05,
      "loss": 0.0568,
      "step": 3713
    },
    {
      "epoch": 1.988487282463186,
      "grad_norm": 1.1750973463058472,
      "learning_rate": 6.748750892219842e-05,
      "loss": 0.1853,
      "step": 3714
    },
    {
      "epoch": 1.9890227576974566,
      "grad_norm": 1.042161464691162,
      "learning_rate": 6.745182012847966e-05,
      "loss": 0.0313,
      "step": 3715
    },
    {
      "epoch": 1.989558232931727,
      "grad_norm": 0.37632665038108826,
      "learning_rate": 6.741613133476089e-05,
      "loss": 0.0318,
      "step": 3716
    },
    {
      "epoch": 1.9900937081659973,
      "grad_norm": 0.9795770645141602,
      "learning_rate": 6.738044254104212e-05,
      "loss": 0.1011,
      "step": 3717
    },
    {
      "epoch": 1.9906291834002676,
      "grad_norm": 0.7368313074111938,
      "learning_rate": 6.734475374732334e-05,
      "loss": 0.0557,
      "step": 3718
    },
    {
      "epoch": 1.9911646586345382,
      "grad_norm": 0.8626444935798645,
      "learning_rate": 6.730906495360458e-05,
      "loss": 0.0528,
      "step": 3719
    },
    {
      "epoch": 1.9917001338688087,
      "grad_norm": 0.7958058714866638,
      "learning_rate": 6.727337615988581e-05,
      "loss": 0.1074,
      "step": 3720
    },
    {
      "epoch": 1.992235609103079,
      "grad_norm": 0.9317436814308167,
      "learning_rate": 6.723768736616703e-05,
      "loss": 0.11,
      "step": 3721
    },
    {
      "epoch": 1.9927710843373494,
      "grad_norm": 0.8534601926803589,
      "learning_rate": 6.720199857244825e-05,
      "loss": 0.1209,
      "step": 3722
    },
    {
      "epoch": 1.9933065595716197,
      "grad_norm": 0.48108530044555664,
      "learning_rate": 6.716630977872948e-05,
      "loss": 0.047,
      "step": 3723
    },
    {
      "epoch": 1.99384203480589,
      "grad_norm": 1.2494032382965088,
      "learning_rate": 6.71306209850107e-05,
      "loss": 0.1727,
      "step": 3724
    },
    {
      "epoch": 1.9943775100401606,
      "grad_norm": 1.0967555046081543,
      "learning_rate": 6.709493219129194e-05,
      "loss": 0.129,
      "step": 3725
    },
    {
      "epoch": 1.9949129852744312,
      "grad_norm": 1.2806682586669922,
      "learning_rate": 6.705924339757317e-05,
      "loss": 0.087,
      "step": 3726
    },
    {
      "epoch": 1.9954484605087015,
      "grad_norm": 0.6205191612243652,
      "learning_rate": 6.702355460385439e-05,
      "loss": 0.0851,
      "step": 3727
    },
    {
      "epoch": 1.9959839357429718,
      "grad_norm": 0.5488608479499817,
      "learning_rate": 6.698786581013562e-05,
      "loss": 0.0265,
      "step": 3728
    },
    {
      "epoch": 1.9965194109772422,
      "grad_norm": 1.229213833808899,
      "learning_rate": 6.695217701641686e-05,
      "loss": 0.0598,
      "step": 3729
    },
    {
      "epoch": 1.9970548862115127,
      "grad_norm": 0.7248932123184204,
      "learning_rate": 6.691648822269808e-05,
      "loss": 0.0983,
      "step": 3730
    },
    {
      "epoch": 1.9975903614457833,
      "grad_norm": 0.5150104761123657,
      "learning_rate": 6.68807994289793e-05,
      "loss": 0.0531,
      "step": 3731
    },
    {
      "epoch": 1.9981258366800536,
      "grad_norm": 1.4349132776260376,
      "learning_rate": 6.684511063526053e-05,
      "loss": 0.0291,
      "step": 3732
    },
    {
      "epoch": 1.998661311914324,
      "grad_norm": 0.842867910861969,
      "learning_rate": 6.680942184154176e-05,
      "loss": 0.1237,
      "step": 3733
    },
    {
      "epoch": 1.9991967871485943,
      "grad_norm": 0.5186551213264465,
      "learning_rate": 6.677373304782298e-05,
      "loss": 0.0342,
      "step": 3734
    },
    {
      "epoch": 1.9997322623828648,
      "grad_norm": 0.6693862676620483,
      "learning_rate": 6.673804425410422e-05,
      "loss": 0.0684,
      "step": 3735
    },
    {
      "epoch": 2.0,
      "grad_norm": 2.1054141521453857,
      "learning_rate": 6.670235546038545e-05,
      "loss": 0.1041,
      "step": 3736
    },
    {
      "epoch": 2.0005354752342703,
      "grad_norm": 0.47284582257270813,
      "learning_rate": 6.666666666666667e-05,
      "loss": 0.024,
      "step": 3737
    },
    {
      "epoch": 2.0010709504685407,
      "grad_norm": 0.7310038208961487,
      "learning_rate": 6.66309778729479e-05,
      "loss": 0.0699,
      "step": 3738
    },
    {
      "epoch": 2.0016064257028114,
      "grad_norm": 1.2607042789459229,
      "learning_rate": 6.659528907922912e-05,
      "loss": 0.1913,
      "step": 3739
    },
    {
      "epoch": 2.0021419009370818,
      "grad_norm": 0.5109767317771912,
      "learning_rate": 6.655960028551034e-05,
      "loss": 0.0061,
      "step": 3740
    },
    {
      "epoch": 2.002677376171352,
      "grad_norm": 1.0013747215270996,
      "learning_rate": 6.652391149179158e-05,
      "loss": 0.0491,
      "step": 3741
    },
    {
      "epoch": 2.0032128514056224,
      "grad_norm": 0.8155255317687988,
      "learning_rate": 6.648822269807281e-05,
      "loss": 0.0927,
      "step": 3742
    },
    {
      "epoch": 2.0037483266398928,
      "grad_norm": 0.8719286918640137,
      "learning_rate": 6.645253390435403e-05,
      "loss": 0.0873,
      "step": 3743
    },
    {
      "epoch": 2.004283801874163,
      "grad_norm": 2.4499881267547607,
      "learning_rate": 6.641684511063526e-05,
      "loss": 0.1977,
      "step": 3744
    },
    {
      "epoch": 2.004819277108434,
      "grad_norm": 3.169283390045166,
      "learning_rate": 6.63811563169165e-05,
      "loss": 0.1428,
      "step": 3745
    },
    {
      "epoch": 2.005354752342704,
      "grad_norm": 1.5433874130249023,
      "learning_rate": 6.634546752319773e-05,
      "loss": 0.1558,
      "step": 3746
    },
    {
      "epoch": 2.0058902275769745,
      "grad_norm": 0.9720354080200195,
      "learning_rate": 6.630977872947895e-05,
      "loss": 0.1109,
      "step": 3747
    },
    {
      "epoch": 2.006425702811245,
      "grad_norm": 0.4621737003326416,
      "learning_rate": 6.627408993576017e-05,
      "loss": 0.0814,
      "step": 3748
    },
    {
      "epoch": 2.006961178045515,
      "grad_norm": 0.7660937309265137,
      "learning_rate": 6.62384011420414e-05,
      "loss": 0.0748,
      "step": 3749
    },
    {
      "epoch": 2.007496653279786,
      "grad_norm": 0.4678346812725067,
      "learning_rate": 6.620271234832262e-05,
      "loss": 0.0302,
      "step": 3750
    },
    {
      "epoch": 2.0080321285140563,
      "grad_norm": 1.048969030380249,
      "learning_rate": 6.616702355460386e-05,
      "loss": 0.0966,
      "step": 3751
    },
    {
      "epoch": 2.0085676037483267,
      "grad_norm": 0.5084102153778076,
      "learning_rate": 6.613133476088509e-05,
      "loss": 0.066,
      "step": 3752
    },
    {
      "epoch": 2.009103078982597,
      "grad_norm": 1.1092571020126343,
      "learning_rate": 6.609564596716631e-05,
      "loss": 0.0758,
      "step": 3753
    },
    {
      "epoch": 2.0096385542168673,
      "grad_norm": 0.6316718459129333,
      "learning_rate": 6.605995717344754e-05,
      "loss": 0.0568,
      "step": 3754
    },
    {
      "epoch": 2.010174029451138,
      "grad_norm": 0.46028757095336914,
      "learning_rate": 6.602426837972878e-05,
      "loss": 0.0314,
      "step": 3755
    },
    {
      "epoch": 2.0107095046854084,
      "grad_norm": 0.7099691033363342,
      "learning_rate": 6.598857958601e-05,
      "loss": 0.0817,
      "step": 3756
    },
    {
      "epoch": 2.0112449799196788,
      "grad_norm": 0.49862995743751526,
      "learning_rate": 6.595289079229122e-05,
      "loss": 0.037,
      "step": 3757
    },
    {
      "epoch": 2.011780455153949,
      "grad_norm": 1.2327864170074463,
      "learning_rate": 6.591720199857245e-05,
      "loss": 0.1107,
      "step": 3758
    },
    {
      "epoch": 2.0123159303882194,
      "grad_norm": 0.9530348181724548,
      "learning_rate": 6.588151320485368e-05,
      "loss": 0.0908,
      "step": 3759
    },
    {
      "epoch": 2.0128514056224898,
      "grad_norm": 0.6334007978439331,
      "learning_rate": 6.58458244111349e-05,
      "loss": 0.0496,
      "step": 3760
    },
    {
      "epoch": 2.0133868808567605,
      "grad_norm": 1.2577040195465088,
      "learning_rate": 6.581013561741614e-05,
      "loss": 0.1336,
      "step": 3761
    },
    {
      "epoch": 2.013922356091031,
      "grad_norm": 0.6898051500320435,
      "learning_rate": 6.577444682369737e-05,
      "loss": 0.0509,
      "step": 3762
    },
    {
      "epoch": 2.014457831325301,
      "grad_norm": 0.6453495621681213,
      "learning_rate": 6.573875802997859e-05,
      "loss": 0.0538,
      "step": 3763
    },
    {
      "epoch": 2.0149933065595715,
      "grad_norm": 0.8029627799987793,
      "learning_rate": 6.570306923625982e-05,
      "loss": 0.0521,
      "step": 3764
    },
    {
      "epoch": 2.015528781793842,
      "grad_norm": 1.2955173254013062,
      "learning_rate": 6.566738044254104e-05,
      "loss": 0.0668,
      "step": 3765
    },
    {
      "epoch": 2.0160642570281126,
      "grad_norm": 0.7003456354141235,
      "learning_rate": 6.563169164882226e-05,
      "loss": 0.0483,
      "step": 3766
    },
    {
      "epoch": 2.016599732262383,
      "grad_norm": 0.6332595348358154,
      "learning_rate": 6.55960028551035e-05,
      "loss": 0.0585,
      "step": 3767
    },
    {
      "epoch": 2.0171352074966533,
      "grad_norm": 0.45709937810897827,
      "learning_rate": 6.556031406138473e-05,
      "loss": 0.0356,
      "step": 3768
    },
    {
      "epoch": 2.0176706827309236,
      "grad_norm": 0.9503955841064453,
      "learning_rate": 6.552462526766595e-05,
      "loss": 0.0702,
      "step": 3769
    },
    {
      "epoch": 2.018206157965194,
      "grad_norm": 0.48938944935798645,
      "learning_rate": 6.548893647394718e-05,
      "loss": 0.0337,
      "step": 3770
    },
    {
      "epoch": 2.0187416331994643,
      "grad_norm": 0.8304653763771057,
      "learning_rate": 6.545324768022842e-05,
      "loss": 0.0883,
      "step": 3771
    },
    {
      "epoch": 2.019277108433735,
      "grad_norm": 0.7636493444442749,
      "learning_rate": 6.541755888650965e-05,
      "loss": 0.0599,
      "step": 3772
    },
    {
      "epoch": 2.0198125836680054,
      "grad_norm": 0.6983897686004639,
      "learning_rate": 6.538187009279087e-05,
      "loss": 0.0293,
      "step": 3773
    },
    {
      "epoch": 2.0203480589022758,
      "grad_norm": 0.6417555212974548,
      "learning_rate": 6.534618129907209e-05,
      "loss": 0.0517,
      "step": 3774
    },
    {
      "epoch": 2.020883534136546,
      "grad_norm": 1.9518742561340332,
      "learning_rate": 6.531049250535332e-05,
      "loss": 0.0969,
      "step": 3775
    },
    {
      "epoch": 2.0214190093708164,
      "grad_norm": 0.3197237551212311,
      "learning_rate": 6.527480371163454e-05,
      "loss": 0.0118,
      "step": 3776
    },
    {
      "epoch": 2.021954484605087,
      "grad_norm": 1.3476314544677734,
      "learning_rate": 6.523911491791578e-05,
      "loss": 0.0829,
      "step": 3777
    },
    {
      "epoch": 2.0224899598393575,
      "grad_norm": 1.17388916015625,
      "learning_rate": 6.520342612419701e-05,
      "loss": 0.0698,
      "step": 3778
    },
    {
      "epoch": 2.023025435073628,
      "grad_norm": 2.789794921875,
      "learning_rate": 6.516773733047823e-05,
      "loss": 0.1211,
      "step": 3779
    },
    {
      "epoch": 2.023560910307898,
      "grad_norm": 2.509653091430664,
      "learning_rate": 6.513204853675946e-05,
      "loss": 0.0636,
      "step": 3780
    },
    {
      "epoch": 2.0240963855421685,
      "grad_norm": 0.7312256097793579,
      "learning_rate": 6.50963597430407e-05,
      "loss": 0.0632,
      "step": 3781
    },
    {
      "epoch": 2.0246318607764393,
      "grad_norm": 1.154057502746582,
      "learning_rate": 6.506067094932192e-05,
      "loss": 0.0577,
      "step": 3782
    },
    {
      "epoch": 2.0251673360107096,
      "grad_norm": 2.0926671028137207,
      "learning_rate": 6.502498215560314e-05,
      "loss": 0.099,
      "step": 3783
    },
    {
      "epoch": 2.02570281124498,
      "grad_norm": 0.7105622291564941,
      "learning_rate": 6.498929336188437e-05,
      "loss": 0.0467,
      "step": 3784
    },
    {
      "epoch": 2.0262382864792503,
      "grad_norm": 0.6514278054237366,
      "learning_rate": 6.495360456816559e-05,
      "loss": 0.0215,
      "step": 3785
    },
    {
      "epoch": 2.0267737617135206,
      "grad_norm": 1.101095199584961,
      "learning_rate": 6.491791577444682e-05,
      "loss": 0.072,
      "step": 3786
    },
    {
      "epoch": 2.027309236947791,
      "grad_norm": 0.8134412169456482,
      "learning_rate": 6.488222698072806e-05,
      "loss": 0.0488,
      "step": 3787
    },
    {
      "epoch": 2.0278447121820617,
      "grad_norm": 0.5886479020118713,
      "learning_rate": 6.484653818700929e-05,
      "loss": 0.0389,
      "step": 3788
    },
    {
      "epoch": 2.028380187416332,
      "grad_norm": 2.269566535949707,
      "learning_rate": 6.481084939329051e-05,
      "loss": 0.0865,
      "step": 3789
    },
    {
      "epoch": 2.0289156626506024,
      "grad_norm": 1.4965336322784424,
      "learning_rate": 6.477516059957174e-05,
      "loss": 0.0571,
      "step": 3790
    },
    {
      "epoch": 2.0294511378848727,
      "grad_norm": 0.7140210270881653,
      "learning_rate": 6.473947180585296e-05,
      "loss": 0.0859,
      "step": 3791
    },
    {
      "epoch": 2.029986613119143,
      "grad_norm": 1.8444069623947144,
      "learning_rate": 6.470378301213418e-05,
      "loss": 0.0752,
      "step": 3792
    },
    {
      "epoch": 2.030522088353414,
      "grad_norm": 3.165898323059082,
      "learning_rate": 6.466809421841542e-05,
      "loss": 0.049,
      "step": 3793
    },
    {
      "epoch": 2.031057563587684,
      "grad_norm": 1.1336110830307007,
      "learning_rate": 6.463240542469665e-05,
      "loss": 0.0513,
      "step": 3794
    },
    {
      "epoch": 2.0315930388219545,
      "grad_norm": 1.0472060441970825,
      "learning_rate": 6.459671663097787e-05,
      "loss": 0.0688,
      "step": 3795
    },
    {
      "epoch": 2.032128514056225,
      "grad_norm": 1.7672470808029175,
      "learning_rate": 6.45610278372591e-05,
      "loss": 0.102,
      "step": 3796
    },
    {
      "epoch": 2.032663989290495,
      "grad_norm": 1.498916506767273,
      "learning_rate": 6.452533904354034e-05,
      "loss": 0.1153,
      "step": 3797
    },
    {
      "epoch": 2.0331994645247655,
      "grad_norm": 0.7691907286643982,
      "learning_rate": 6.448965024982156e-05,
      "loss": 0.0592,
      "step": 3798
    },
    {
      "epoch": 2.0337349397590363,
      "grad_norm": 0.022158736363053322,
      "learning_rate": 6.445396145610279e-05,
      "loss": 0.0007,
      "step": 3799
    },
    {
      "epoch": 2.0342704149933066,
      "grad_norm": 0.878555178642273,
      "learning_rate": 6.441827266238401e-05,
      "loss": 0.047,
      "step": 3800
    },
    {
      "epoch": 2.034805890227577,
      "grad_norm": 0.9739067554473877,
      "learning_rate": 6.438258386866524e-05,
      "loss": 0.0575,
      "step": 3801
    },
    {
      "epoch": 2.0353413654618473,
      "grad_norm": 0.6246405243873596,
      "learning_rate": 6.434689507494646e-05,
      "loss": 0.0294,
      "step": 3802
    },
    {
      "epoch": 2.0358768406961176,
      "grad_norm": 2.055537223815918,
      "learning_rate": 6.43112062812277e-05,
      "loss": 0.1581,
      "step": 3803
    },
    {
      "epoch": 2.0364123159303884,
      "grad_norm": 1.366159200668335,
      "learning_rate": 6.427551748750893e-05,
      "loss": 0.0706,
      "step": 3804
    },
    {
      "epoch": 2.0369477911646587,
      "grad_norm": 0.7665183544158936,
      "learning_rate": 6.423982869379015e-05,
      "loss": 0.0419,
      "step": 3805
    },
    {
      "epoch": 2.037483266398929,
      "grad_norm": 0.8913003206253052,
      "learning_rate": 6.420413990007138e-05,
      "loss": 0.0613,
      "step": 3806
    },
    {
      "epoch": 2.0380187416331994,
      "grad_norm": 0.7643176317214966,
      "learning_rate": 6.416845110635262e-05,
      "loss": 0.0649,
      "step": 3807
    },
    {
      "epoch": 2.0385542168674697,
      "grad_norm": 1.2233175039291382,
      "learning_rate": 6.413276231263384e-05,
      "loss": 0.094,
      "step": 3808
    },
    {
      "epoch": 2.03908969210174,
      "grad_norm": 1.0198495388031006,
      "learning_rate": 6.409707351891506e-05,
      "loss": 0.1037,
      "step": 3809
    },
    {
      "epoch": 2.039625167336011,
      "grad_norm": 0.7737930417060852,
      "learning_rate": 6.406138472519629e-05,
      "loss": 0.0381,
      "step": 3810
    },
    {
      "epoch": 2.040160642570281,
      "grad_norm": 0.5586519837379456,
      "learning_rate": 6.402569593147751e-05,
      "loss": 0.0356,
      "step": 3811
    },
    {
      "epoch": 2.0406961178045515,
      "grad_norm": 2.755072593688965,
      "learning_rate": 6.399000713775874e-05,
      "loss": 0.0875,
      "step": 3812
    },
    {
      "epoch": 2.041231593038822,
      "grad_norm": 0.4592203199863434,
      "learning_rate": 6.395431834403998e-05,
      "loss": 0.0217,
      "step": 3813
    },
    {
      "epoch": 2.041767068273092,
      "grad_norm": 0.7232345938682556,
      "learning_rate": 6.391862955032121e-05,
      "loss": 0.0653,
      "step": 3814
    },
    {
      "epoch": 2.042302543507363,
      "grad_norm": 1.4881768226623535,
      "learning_rate": 6.388294075660243e-05,
      "loss": 0.1008,
      "step": 3815
    },
    {
      "epoch": 2.0428380187416333,
      "grad_norm": 1.1577339172363281,
      "learning_rate": 6.384725196288366e-05,
      "loss": 0.0923,
      "step": 3816
    },
    {
      "epoch": 2.0433734939759036,
      "grad_norm": 1.015041470527649,
      "learning_rate": 6.381156316916488e-05,
      "loss": 0.065,
      "step": 3817
    },
    {
      "epoch": 2.043908969210174,
      "grad_norm": 1.0527304410934448,
      "learning_rate": 6.37758743754461e-05,
      "loss": 0.0575,
      "step": 3818
    },
    {
      "epoch": 2.0444444444444443,
      "grad_norm": 0.4561282694339752,
      "learning_rate": 6.374018558172734e-05,
      "loss": 0.0252,
      "step": 3819
    },
    {
      "epoch": 2.044979919678715,
      "grad_norm": 1.2624590396881104,
      "learning_rate": 6.370449678800857e-05,
      "loss": 0.0553,
      "step": 3820
    },
    {
      "epoch": 2.0455153949129854,
      "grad_norm": 1.2314850091934204,
      "learning_rate": 6.366880799428979e-05,
      "loss": 0.0961,
      "step": 3821
    },
    {
      "epoch": 2.0460508701472557,
      "grad_norm": 0.3819425106048584,
      "learning_rate": 6.363311920057102e-05,
      "loss": 0.02,
      "step": 3822
    },
    {
      "epoch": 2.046586345381526,
      "grad_norm": 0.7964953184127808,
      "learning_rate": 6.359743040685226e-05,
      "loss": 0.0796,
      "step": 3823
    },
    {
      "epoch": 2.0471218206157964,
      "grad_norm": 0.44227948784828186,
      "learning_rate": 6.356174161313348e-05,
      "loss": 0.0321,
      "step": 3824
    },
    {
      "epoch": 2.0476572958500667,
      "grad_norm": 1.2627488374710083,
      "learning_rate": 6.352605281941471e-05,
      "loss": 0.115,
      "step": 3825
    },
    {
      "epoch": 2.0481927710843375,
      "grad_norm": 1.088577151298523,
      "learning_rate": 6.349036402569594e-05,
      "loss": 0.0543,
      "step": 3826
    },
    {
      "epoch": 2.048728246318608,
      "grad_norm": 0.9789484143257141,
      "learning_rate": 6.345467523197716e-05,
      "loss": 0.0865,
      "step": 3827
    },
    {
      "epoch": 2.049263721552878,
      "grad_norm": 0.7713422179222107,
      "learning_rate": 6.341898643825838e-05,
      "loss": 0.0616,
      "step": 3828
    },
    {
      "epoch": 2.0497991967871485,
      "grad_norm": 1.301566243171692,
      "learning_rate": 6.338329764453962e-05,
      "loss": 0.0686,
      "step": 3829
    },
    {
      "epoch": 2.050334672021419,
      "grad_norm": 2.029367685317993,
      "learning_rate": 6.334760885082085e-05,
      "loss": 0.0673,
      "step": 3830
    },
    {
      "epoch": 2.0508701472556896,
      "grad_norm": 2.4797396659851074,
      "learning_rate": 6.331192005710207e-05,
      "loss": 0.0747,
      "step": 3831
    },
    {
      "epoch": 2.05140562248996,
      "grad_norm": 1.7792608737945557,
      "learning_rate": 6.32762312633833e-05,
      "loss": 0.1513,
      "step": 3832
    },
    {
      "epoch": 2.0519410977242303,
      "grad_norm": 1.5433496236801147,
      "learning_rate": 6.324054246966454e-05,
      "loss": 0.0972,
      "step": 3833
    },
    {
      "epoch": 2.0524765729585006,
      "grad_norm": 1.4132245779037476,
      "learning_rate": 6.320485367594576e-05,
      "loss": 0.0677,
      "step": 3834
    },
    {
      "epoch": 2.053012048192771,
      "grad_norm": 1.1893072128295898,
      "learning_rate": 6.316916488222699e-05,
      "loss": 0.0834,
      "step": 3835
    },
    {
      "epoch": 2.0535475234270413,
      "grad_norm": 1.8351976871490479,
      "learning_rate": 6.313347608850821e-05,
      "loss": 0.0942,
      "step": 3836
    },
    {
      "epoch": 2.054082998661312,
      "grad_norm": 3.519458770751953,
      "learning_rate": 6.309778729478943e-05,
      "loss": 0.1341,
      "step": 3837
    },
    {
      "epoch": 2.0546184738955824,
      "grad_norm": 0.3946874141693115,
      "learning_rate": 6.306209850107066e-05,
      "loss": 0.0224,
      "step": 3838
    },
    {
      "epoch": 2.0551539491298527,
      "grad_norm": 2.326979637145996,
      "learning_rate": 6.30264097073519e-05,
      "loss": 0.0542,
      "step": 3839
    },
    {
      "epoch": 2.055689424364123,
      "grad_norm": 1.3298050165176392,
      "learning_rate": 6.299072091363312e-05,
      "loss": 0.138,
      "step": 3840
    },
    {
      "epoch": 2.0562248995983934,
      "grad_norm": 1.081217646598816,
      "learning_rate": 6.295503211991435e-05,
      "loss": 0.0678,
      "step": 3841
    },
    {
      "epoch": 2.056760374832664,
      "grad_norm": 1.0547202825546265,
      "learning_rate": 6.291934332619558e-05,
      "loss": 0.0455,
      "step": 3842
    },
    {
      "epoch": 2.0572958500669345,
      "grad_norm": 0.6675323843955994,
      "learning_rate": 6.288365453247682e-05,
      "loss": 0.0407,
      "step": 3843
    },
    {
      "epoch": 2.057831325301205,
      "grad_norm": 0.8033778071403503,
      "learning_rate": 6.284796573875804e-05,
      "loss": 0.076,
      "step": 3844
    },
    {
      "epoch": 2.058366800535475,
      "grad_norm": 1.1448451280593872,
      "learning_rate": 6.281227694503926e-05,
      "loss": 0.0939,
      "step": 3845
    },
    {
      "epoch": 2.0589022757697455,
      "grad_norm": 1.354832410812378,
      "learning_rate": 6.277658815132049e-05,
      "loss": 0.0722,
      "step": 3846
    },
    {
      "epoch": 2.0594377510040163,
      "grad_norm": 0.6597937345504761,
      "learning_rate": 6.274089935760171e-05,
      "loss": 0.0412,
      "step": 3847
    },
    {
      "epoch": 2.0599732262382866,
      "grad_norm": 2.278688430786133,
      "learning_rate": 6.270521056388294e-05,
      "loss": 0.1591,
      "step": 3848
    },
    {
      "epoch": 2.060508701472557,
      "grad_norm": 1.2234796285629272,
      "learning_rate": 6.266952177016418e-05,
      "loss": 0.1018,
      "step": 3849
    },
    {
      "epoch": 2.0610441767068273,
      "grad_norm": 2.244283437728882,
      "learning_rate": 6.26338329764454e-05,
      "loss": 0.0732,
      "step": 3850
    },
    {
      "epoch": 2.0615796519410976,
      "grad_norm": 0.6734533905982971,
      "learning_rate": 6.259814418272663e-05,
      "loss": 0.0242,
      "step": 3851
    },
    {
      "epoch": 2.062115127175368,
      "grad_norm": 0.6369301676750183,
      "learning_rate": 6.256245538900786e-05,
      "loss": 0.0332,
      "step": 3852
    },
    {
      "epoch": 2.0626506024096387,
      "grad_norm": 0.6048199534416199,
      "learning_rate": 6.252676659528908e-05,
      "loss": 0.0331,
      "step": 3853
    },
    {
      "epoch": 2.063186077643909,
      "grad_norm": 0.5180715322494507,
      "learning_rate": 6.24910778015703e-05,
      "loss": 0.0459,
      "step": 3854
    },
    {
      "epoch": 2.0637215528781794,
      "grad_norm": 0.8897387981414795,
      "learning_rate": 6.245538900785154e-05,
      "loss": 0.0462,
      "step": 3855
    },
    {
      "epoch": 2.0642570281124497,
      "grad_norm": 5.082785129547119,
      "learning_rate": 6.241970021413277e-05,
      "loss": 0.0894,
      "step": 3856
    },
    {
      "epoch": 2.06479250334672,
      "grad_norm": 1.1333975791931152,
      "learning_rate": 6.238401142041399e-05,
      "loss": 0.1129,
      "step": 3857
    },
    {
      "epoch": 2.065327978580991,
      "grad_norm": 1.195548415184021,
      "learning_rate": 6.234832262669522e-05,
      "loss": 0.1065,
      "step": 3858
    },
    {
      "epoch": 2.065863453815261,
      "grad_norm": 1.2176042795181274,
      "learning_rate": 6.231263383297646e-05,
      "loss": 0.0706,
      "step": 3859
    },
    {
      "epoch": 2.0663989290495315,
      "grad_norm": 0.6794830560684204,
      "learning_rate": 6.227694503925768e-05,
      "loss": 0.0701,
      "step": 3860
    },
    {
      "epoch": 2.066934404283802,
      "grad_norm": 1.3171610832214355,
      "learning_rate": 6.224125624553891e-05,
      "loss": 0.0867,
      "step": 3861
    },
    {
      "epoch": 2.067469879518072,
      "grad_norm": 0.6914975643157959,
      "learning_rate": 6.220556745182013e-05,
      "loss": 0.0693,
      "step": 3862
    },
    {
      "epoch": 2.0680053547523425,
      "grad_norm": 0.741309404373169,
      "learning_rate": 6.216987865810135e-05,
      "loss": 0.0586,
      "step": 3863
    },
    {
      "epoch": 2.0685408299866133,
      "grad_norm": 0.8534976243972778,
      "learning_rate": 6.213418986438258e-05,
      "loss": 0.0669,
      "step": 3864
    },
    {
      "epoch": 2.0690763052208836,
      "grad_norm": 1.0922276973724365,
      "learning_rate": 6.209850107066382e-05,
      "loss": 0.1097,
      "step": 3865
    },
    {
      "epoch": 2.069611780455154,
      "grad_norm": 0.9142577648162842,
      "learning_rate": 6.206281227694504e-05,
      "loss": 0.1064,
      "step": 3866
    },
    {
      "epoch": 2.0701472556894243,
      "grad_norm": 0.7503395080566406,
      "learning_rate": 6.202712348322627e-05,
      "loss": 0.0506,
      "step": 3867
    },
    {
      "epoch": 2.0706827309236946,
      "grad_norm": 0.9070537090301514,
      "learning_rate": 6.19914346895075e-05,
      "loss": 0.0854,
      "step": 3868
    },
    {
      "epoch": 2.0712182061579654,
      "grad_norm": 1.722437858581543,
      "learning_rate": 6.195574589578874e-05,
      "loss": 0.165,
      "step": 3869
    },
    {
      "epoch": 2.0717536813922357,
      "grad_norm": 0.7746512293815613,
      "learning_rate": 6.192005710206996e-05,
      "loss": 0.0881,
      "step": 3870
    },
    {
      "epoch": 2.072289156626506,
      "grad_norm": 1.0302523374557495,
      "learning_rate": 6.188436830835118e-05,
      "loss": 0.0794,
      "step": 3871
    },
    {
      "epoch": 2.0728246318607764,
      "grad_norm": 1.0041463375091553,
      "learning_rate": 6.184867951463241e-05,
      "loss": 0.1242,
      "step": 3872
    },
    {
      "epoch": 2.0733601070950467,
      "grad_norm": 1.2228187322616577,
      "learning_rate": 6.181299072091363e-05,
      "loss": 0.0934,
      "step": 3873
    },
    {
      "epoch": 2.0738955823293175,
      "grad_norm": 0.7384450435638428,
      "learning_rate": 6.177730192719486e-05,
      "loss": 0.0531,
      "step": 3874
    },
    {
      "epoch": 2.074431057563588,
      "grad_norm": 0.7933139204978943,
      "learning_rate": 6.17416131334761e-05,
      "loss": 0.0302,
      "step": 3875
    },
    {
      "epoch": 2.074966532797858,
      "grad_norm": 1.423241138458252,
      "learning_rate": 6.170592433975732e-05,
      "loss": 0.1104,
      "step": 3876
    },
    {
      "epoch": 2.0755020080321285,
      "grad_norm": 1.6258944272994995,
      "learning_rate": 6.167023554603855e-05,
      "loss": 0.0967,
      "step": 3877
    },
    {
      "epoch": 2.076037483266399,
      "grad_norm": 1.57193922996521,
      "learning_rate": 6.163454675231978e-05,
      "loss": 0.1183,
      "step": 3878
    },
    {
      "epoch": 2.076572958500669,
      "grad_norm": 1.6472352743148804,
      "learning_rate": 6.1598857958601e-05,
      "loss": 0.2344,
      "step": 3879
    },
    {
      "epoch": 2.07710843373494,
      "grad_norm": 0.4669412672519684,
      "learning_rate": 6.156316916488222e-05,
      "loss": 0.0395,
      "step": 3880
    },
    {
      "epoch": 2.0776439089692103,
      "grad_norm": 0.6038892269134521,
      "learning_rate": 6.152748037116346e-05,
      "loss": 0.0234,
      "step": 3881
    },
    {
      "epoch": 2.0781793842034806,
      "grad_norm": 1.0727254152297974,
      "learning_rate": 6.149179157744469e-05,
      "loss": 0.0825,
      "step": 3882
    },
    {
      "epoch": 2.078714859437751,
      "grad_norm": 1.1101042032241821,
      "learning_rate": 6.145610278372591e-05,
      "loss": 0.1138,
      "step": 3883
    },
    {
      "epoch": 2.0792503346720212,
      "grad_norm": 0.5178952813148499,
      "learning_rate": 6.142041399000714e-05,
      "loss": 0.0393,
      "step": 3884
    },
    {
      "epoch": 2.079785809906292,
      "grad_norm": 0.5548200607299805,
      "learning_rate": 6.138472519628838e-05,
      "loss": 0.0396,
      "step": 3885
    },
    {
      "epoch": 2.0803212851405624,
      "grad_norm": 0.8777289390563965,
      "learning_rate": 6.13490364025696e-05,
      "loss": 0.0604,
      "step": 3886
    },
    {
      "epoch": 2.0808567603748327,
      "grad_norm": 1.0220468044281006,
      "learning_rate": 6.131334760885083e-05,
      "loss": 0.1011,
      "step": 3887
    },
    {
      "epoch": 2.081392235609103,
      "grad_norm": 0.7584763765335083,
      "learning_rate": 6.127765881513205e-05,
      "loss": 0.0527,
      "step": 3888
    },
    {
      "epoch": 2.0819277108433734,
      "grad_norm": 0.39846503734588623,
      "learning_rate": 6.124197002141327e-05,
      "loss": 0.0275,
      "step": 3889
    },
    {
      "epoch": 2.0824631860776437,
      "grad_norm": 0.8940514326095581,
      "learning_rate": 6.12062812276945e-05,
      "loss": 0.078,
      "step": 3890
    },
    {
      "epoch": 2.0829986613119145,
      "grad_norm": 0.8980836868286133,
      "learning_rate": 6.117059243397574e-05,
      "loss": 0.0852,
      "step": 3891
    },
    {
      "epoch": 2.083534136546185,
      "grad_norm": 1.1809687614440918,
      "learning_rate": 6.113490364025696e-05,
      "loss": 0.1312,
      "step": 3892
    },
    {
      "epoch": 2.084069611780455,
      "grad_norm": 0.6104502081871033,
      "learning_rate": 6.109921484653819e-05,
      "loss": 0.03,
      "step": 3893
    },
    {
      "epoch": 2.0846050870147255,
      "grad_norm": 1.186194896697998,
      "learning_rate": 6.106352605281942e-05,
      "loss": 0.0472,
      "step": 3894
    },
    {
      "epoch": 2.085140562248996,
      "grad_norm": 0.4519558548927307,
      "learning_rate": 6.102783725910065e-05,
      "loss": 0.0281,
      "step": 3895
    },
    {
      "epoch": 2.0856760374832666,
      "grad_norm": 1.5253902673721313,
      "learning_rate": 6.099214846538187e-05,
      "loss": 0.1434,
      "step": 3896
    },
    {
      "epoch": 2.086211512717537,
      "grad_norm": 1.1377079486846924,
      "learning_rate": 6.09564596716631e-05,
      "loss": 0.0764,
      "step": 3897
    },
    {
      "epoch": 2.0867469879518072,
      "grad_norm": 0.9118683934211731,
      "learning_rate": 6.0920770877944336e-05,
      "loss": 0.0972,
      "step": 3898
    },
    {
      "epoch": 2.0872824631860776,
      "grad_norm": 0.328735888004303,
      "learning_rate": 6.0885082084225556e-05,
      "loss": 0.0136,
      "step": 3899
    },
    {
      "epoch": 2.087817938420348,
      "grad_norm": 1.4577009677886963,
      "learning_rate": 6.084939329050678e-05,
      "loss": 0.1718,
      "step": 3900
    },
    {
      "epoch": 2.0883534136546187,
      "grad_norm": 1.0699973106384277,
      "learning_rate": 6.0813704496788016e-05,
      "loss": 0.0815,
      "step": 3901
    },
    {
      "epoch": 2.088888888888889,
      "grad_norm": 0.563037633895874,
      "learning_rate": 6.0778015703069236e-05,
      "loss": 0.0574,
      "step": 3902
    },
    {
      "epoch": 2.0894243641231594,
      "grad_norm": 2.3816163539886475,
      "learning_rate": 6.074232690935047e-05,
      "loss": 0.1131,
      "step": 3903
    },
    {
      "epoch": 2.0899598393574297,
      "grad_norm": 0.7293775677680969,
      "learning_rate": 6.0706638115631696e-05,
      "loss": 0.0546,
      "step": 3904
    },
    {
      "epoch": 2.0904953145917,
      "grad_norm": 0.8310662508010864,
      "learning_rate": 6.0670949321912916e-05,
      "loss": 0.072,
      "step": 3905
    },
    {
      "epoch": 2.0910307898259703,
      "grad_norm": 1.5392649173736572,
      "learning_rate": 6.063526052819415e-05,
      "loss": 0.1223,
      "step": 3906
    },
    {
      "epoch": 2.091566265060241,
      "grad_norm": 0.8343867063522339,
      "learning_rate": 6.059957173447538e-05,
      "loss": 0.0669,
      "step": 3907
    },
    {
      "epoch": 2.0921017402945115,
      "grad_norm": 0.788663387298584,
      "learning_rate": 6.05638829407566e-05,
      "loss": 0.0818,
      "step": 3908
    },
    {
      "epoch": 2.092637215528782,
      "grad_norm": 1.6621308326721191,
      "learning_rate": 6.052819414703783e-05,
      "loss": 0.1383,
      "step": 3909
    },
    {
      "epoch": 2.093172690763052,
      "grad_norm": 0.9114758372306824,
      "learning_rate": 6.049250535331906e-05,
      "loss": 0.1019,
      "step": 3910
    },
    {
      "epoch": 2.0937081659973225,
      "grad_norm": 0.9615715742111206,
      "learning_rate": 6.0456816559600296e-05,
      "loss": 0.085,
      "step": 3911
    },
    {
      "epoch": 2.0942436412315932,
      "grad_norm": 1.5680136680603027,
      "learning_rate": 6.0421127765881516e-05,
      "loss": 0.1443,
      "step": 3912
    },
    {
      "epoch": 2.0947791164658636,
      "grad_norm": 1.6475341320037842,
      "learning_rate": 6.038543897216274e-05,
      "loss": 0.1439,
      "step": 3913
    },
    {
      "epoch": 2.095314591700134,
      "grad_norm": 1.4137312173843384,
      "learning_rate": 6.0349750178443976e-05,
      "loss": 0.0984,
      "step": 3914
    },
    {
      "epoch": 2.0958500669344042,
      "grad_norm": 1.04265296459198,
      "learning_rate": 6.0314061384725196e-05,
      "loss": 0.0945,
      "step": 3915
    },
    {
      "epoch": 2.0963855421686746,
      "grad_norm": 0.8146258592605591,
      "learning_rate": 6.027837259100643e-05,
      "loss": 0.0369,
      "step": 3916
    },
    {
      "epoch": 2.096921017402945,
      "grad_norm": 1.111607551574707,
      "learning_rate": 6.0242683797287656e-05,
      "loss": 0.0707,
      "step": 3917
    },
    {
      "epoch": 2.0974564926372157,
      "grad_norm": 0.8301950693130493,
      "learning_rate": 6.0206995003568876e-05,
      "loss": 0.0566,
      "step": 3918
    },
    {
      "epoch": 2.097991967871486,
      "grad_norm": 1.126944661140442,
      "learning_rate": 6.017130620985011e-05,
      "loss": 0.0858,
      "step": 3919
    },
    {
      "epoch": 2.0985274431057563,
      "grad_norm": 1.0784366130828857,
      "learning_rate": 6.013561741613134e-05,
      "loss": 0.0762,
      "step": 3920
    },
    {
      "epoch": 2.0990629183400267,
      "grad_norm": 0.7688690423965454,
      "learning_rate": 6.009992862241256e-05,
      "loss": 0.0651,
      "step": 3921
    },
    {
      "epoch": 2.099598393574297,
      "grad_norm": 1.06995689868927,
      "learning_rate": 6.006423982869379e-05,
      "loss": 0.0778,
      "step": 3922
    },
    {
      "epoch": 2.100133868808568,
      "grad_norm": 1.443393349647522,
      "learning_rate": 6.002855103497502e-05,
      "loss": 0.1125,
      "step": 3923
    },
    {
      "epoch": 2.100669344042838,
      "grad_norm": 0.7957137227058411,
      "learning_rate": 5.9992862241256256e-05,
      "loss": 0.0539,
      "step": 3924
    },
    {
      "epoch": 2.1012048192771084,
      "grad_norm": 2.6459732055664062,
      "learning_rate": 5.9957173447537476e-05,
      "loss": 0.0689,
      "step": 3925
    },
    {
      "epoch": 2.101740294511379,
      "grad_norm": 1.014778971672058,
      "learning_rate": 5.99214846538187e-05,
      "loss": 0.0987,
      "step": 3926
    },
    {
      "epoch": 2.102275769745649,
      "grad_norm": 0.4698094129562378,
      "learning_rate": 5.9885795860099936e-05,
      "loss": 0.0404,
      "step": 3927
    },
    {
      "epoch": 2.10281124497992,
      "grad_norm": 0.850663959980011,
      "learning_rate": 5.9850107066381156e-05,
      "loss": 0.0524,
      "step": 3928
    },
    {
      "epoch": 2.1033467202141902,
      "grad_norm": 0.9732786417007446,
      "learning_rate": 5.981441827266239e-05,
      "loss": 0.0462,
      "step": 3929
    },
    {
      "epoch": 2.1038821954484606,
      "grad_norm": 0.9865310788154602,
      "learning_rate": 5.9778729478943616e-05,
      "loss": 0.078,
      "step": 3930
    },
    {
      "epoch": 2.104417670682731,
      "grad_norm": 1.033484697341919,
      "learning_rate": 5.9743040685224836e-05,
      "loss": 0.0669,
      "step": 3931
    },
    {
      "epoch": 2.1049531459170012,
      "grad_norm": 5.3616838455200195,
      "learning_rate": 5.970735189150607e-05,
      "loss": 0.1159,
      "step": 3932
    },
    {
      "epoch": 2.1054886211512716,
      "grad_norm": 1.218203067779541,
      "learning_rate": 5.96716630977873e-05,
      "loss": 0.0939,
      "step": 3933
    },
    {
      "epoch": 2.1060240963855423,
      "grad_norm": 0.7500290870666504,
      "learning_rate": 5.963597430406852e-05,
      "loss": 0.0766,
      "step": 3934
    },
    {
      "epoch": 2.1065595716198127,
      "grad_norm": 2.2213590145111084,
      "learning_rate": 5.960028551034975e-05,
      "loss": 0.1456,
      "step": 3935
    },
    {
      "epoch": 2.107095046854083,
      "grad_norm": 1.4725849628448486,
      "learning_rate": 5.956459671663098e-05,
      "loss": 0.1363,
      "step": 3936
    },
    {
      "epoch": 2.1076305220883533,
      "grad_norm": 1.647262692451477,
      "learning_rate": 5.9528907922912216e-05,
      "loss": 0.1369,
      "step": 3937
    },
    {
      "epoch": 2.1081659973226237,
      "grad_norm": 1.4199708700180054,
      "learning_rate": 5.9493219129193436e-05,
      "loss": 0.0672,
      "step": 3938
    },
    {
      "epoch": 2.1087014725568944,
      "grad_norm": 1.9267514944076538,
      "learning_rate": 5.945753033547466e-05,
      "loss": 0.0822,
      "step": 3939
    },
    {
      "epoch": 2.1092369477911648,
      "grad_norm": 1.1462106704711914,
      "learning_rate": 5.9421841541755896e-05,
      "loss": 0.0758,
      "step": 3940
    },
    {
      "epoch": 2.109772423025435,
      "grad_norm": 0.8919227123260498,
      "learning_rate": 5.9386152748037116e-05,
      "loss": 0.0523,
      "step": 3941
    },
    {
      "epoch": 2.1103078982597054,
      "grad_norm": 1.330751657485962,
      "learning_rate": 5.935046395431835e-05,
      "loss": 0.125,
      "step": 3942
    },
    {
      "epoch": 2.1108433734939758,
      "grad_norm": 1.615897536277771,
      "learning_rate": 5.9314775160599576e-05,
      "loss": 0.1036,
      "step": 3943
    },
    {
      "epoch": 2.111378848728246,
      "grad_norm": 1.771759271621704,
      "learning_rate": 5.9279086366880796e-05,
      "loss": 0.0882,
      "step": 3944
    },
    {
      "epoch": 2.111914323962517,
      "grad_norm": 2.1349194049835205,
      "learning_rate": 5.924339757316203e-05,
      "loss": 0.0898,
      "step": 3945
    },
    {
      "epoch": 2.112449799196787,
      "grad_norm": 2.866126775741577,
      "learning_rate": 5.920770877944326e-05,
      "loss": 0.0799,
      "step": 3946
    },
    {
      "epoch": 2.1129852744310575,
      "grad_norm": 1.428027629852295,
      "learning_rate": 5.917201998572448e-05,
      "loss": 0.0606,
      "step": 3947
    },
    {
      "epoch": 2.113520749665328,
      "grad_norm": 1.3487462997436523,
      "learning_rate": 5.913633119200571e-05,
      "loss": 0.0894,
      "step": 3948
    },
    {
      "epoch": 2.114056224899598,
      "grad_norm": 0.4021828770637512,
      "learning_rate": 5.910064239828694e-05,
      "loss": 0.0226,
      "step": 3949
    },
    {
      "epoch": 2.114591700133869,
      "grad_norm": 0.8717003464698792,
      "learning_rate": 5.9064953604568176e-05,
      "loss": 0.0266,
      "step": 3950
    },
    {
      "epoch": 2.1151271753681393,
      "grad_norm": 0.9912039637565613,
      "learning_rate": 5.9029264810849396e-05,
      "loss": 0.0562,
      "step": 3951
    },
    {
      "epoch": 2.1156626506024097,
      "grad_norm": 1.37679922580719,
      "learning_rate": 5.899357601713062e-05,
      "loss": 0.1021,
      "step": 3952
    },
    {
      "epoch": 2.11619812583668,
      "grad_norm": 1.3155750036239624,
      "learning_rate": 5.8957887223411856e-05,
      "loss": 0.1113,
      "step": 3953
    },
    {
      "epoch": 2.1167336010709503,
      "grad_norm": 0.4346778392791748,
      "learning_rate": 5.8922198429693076e-05,
      "loss": 0.0158,
      "step": 3954
    },
    {
      "epoch": 2.117269076305221,
      "grad_norm": 1.454546570777893,
      "learning_rate": 5.888650963597431e-05,
      "loss": 0.1376,
      "step": 3955
    },
    {
      "epoch": 2.1178045515394914,
      "grad_norm": 1.4539775848388672,
      "learning_rate": 5.8850820842255536e-05,
      "loss": 0.0982,
      "step": 3956
    },
    {
      "epoch": 2.1183400267737618,
      "grad_norm": 0.9274773001670837,
      "learning_rate": 5.8815132048536756e-05,
      "loss": 0.062,
      "step": 3957
    },
    {
      "epoch": 2.118875502008032,
      "grad_norm": 1.1136945486068726,
      "learning_rate": 5.877944325481799e-05,
      "loss": 0.0829,
      "step": 3958
    },
    {
      "epoch": 2.1194109772423024,
      "grad_norm": 0.9210418462753296,
      "learning_rate": 5.874375446109922e-05,
      "loss": 0.0615,
      "step": 3959
    },
    {
      "epoch": 2.1199464524765728,
      "grad_norm": 1.6001032590866089,
      "learning_rate": 5.870806566738044e-05,
      "loss": 0.1348,
      "step": 3960
    },
    {
      "epoch": 2.1204819277108435,
      "grad_norm": 2.2872817516326904,
      "learning_rate": 5.867237687366167e-05,
      "loss": 0.0915,
      "step": 3961
    },
    {
      "epoch": 2.121017402945114,
      "grad_norm": 1.1146615743637085,
      "learning_rate": 5.86366880799429e-05,
      "loss": 0.0724,
      "step": 3962
    },
    {
      "epoch": 2.121552878179384,
      "grad_norm": 1.3333773612976074,
      "learning_rate": 5.860099928622412e-05,
      "loss": 0.0797,
      "step": 3963
    },
    {
      "epoch": 2.1220883534136545,
      "grad_norm": 0.2452981173992157,
      "learning_rate": 5.8565310492505356e-05,
      "loss": 0.0094,
      "step": 3964
    },
    {
      "epoch": 2.122623828647925,
      "grad_norm": 1.4982373714447021,
      "learning_rate": 5.852962169878658e-05,
      "loss": 0.078,
      "step": 3965
    },
    {
      "epoch": 2.1231593038821956,
      "grad_norm": 1.1712942123413086,
      "learning_rate": 5.8493932905067816e-05,
      "loss": 0.107,
      "step": 3966
    },
    {
      "epoch": 2.123694779116466,
      "grad_norm": 1.812198519706726,
      "learning_rate": 5.8458244111349036e-05,
      "loss": 0.092,
      "step": 3967
    },
    {
      "epoch": 2.1242302543507363,
      "grad_norm": 1.7806875705718994,
      "learning_rate": 5.842255531763027e-05,
      "loss": 0.102,
      "step": 3968
    },
    {
      "epoch": 2.1247657295850066,
      "grad_norm": 2.065612316131592,
      "learning_rate": 5.8386866523911496e-05,
      "loss": 0.0561,
      "step": 3969
    },
    {
      "epoch": 2.125301204819277,
      "grad_norm": 0.7382712960243225,
      "learning_rate": 5.8351177730192716e-05,
      "loss": 0.0781,
      "step": 3970
    },
    {
      "epoch": 2.1258366800535473,
      "grad_norm": 1.1828529834747314,
      "learning_rate": 5.831548893647395e-05,
      "loss": 0.065,
      "step": 3971
    },
    {
      "epoch": 2.126372155287818,
      "grad_norm": 7.989053249359131,
      "learning_rate": 5.827980014275518e-05,
      "loss": 0.1692,
      "step": 3972
    },
    {
      "epoch": 2.1269076305220884,
      "grad_norm": 1.248479962348938,
      "learning_rate": 5.82441113490364e-05,
      "loss": 0.1168,
      "step": 3973
    },
    {
      "epoch": 2.1274431057563588,
      "grad_norm": 0.6104889512062073,
      "learning_rate": 5.820842255531763e-05,
      "loss": 0.0785,
      "step": 3974
    },
    {
      "epoch": 2.127978580990629,
      "grad_norm": 1.0264549255371094,
      "learning_rate": 5.817273376159886e-05,
      "loss": 0.0782,
      "step": 3975
    },
    {
      "epoch": 2.1285140562248994,
      "grad_norm": 0.9103062152862549,
      "learning_rate": 5.813704496788008e-05,
      "loss": 0.0566,
      "step": 3976
    },
    {
      "epoch": 2.12904953145917,
      "grad_norm": 1.2945866584777832,
      "learning_rate": 5.8101356174161316e-05,
      "loss": 0.0639,
      "step": 3977
    },
    {
      "epoch": 2.1295850066934405,
      "grad_norm": 0.663630485534668,
      "learning_rate": 5.806566738044254e-05,
      "loss": 0.0436,
      "step": 3978
    },
    {
      "epoch": 2.130120481927711,
      "grad_norm": 0.7211244702339172,
      "learning_rate": 5.8029978586723776e-05,
      "loss": 0.0394,
      "step": 3979
    },
    {
      "epoch": 2.130655957161981,
      "grad_norm": 1.0785056352615356,
      "learning_rate": 5.7994289793004996e-05,
      "loss": 0.1167,
      "step": 3980
    },
    {
      "epoch": 2.1311914323962515,
      "grad_norm": 1.7529617547988892,
      "learning_rate": 5.795860099928623e-05,
      "loss": 0.057,
      "step": 3981
    },
    {
      "epoch": 2.1317269076305223,
      "grad_norm": 0.3460248112678528,
      "learning_rate": 5.7922912205567456e-05,
      "loss": 0.0179,
      "step": 3982
    },
    {
      "epoch": 2.1322623828647926,
      "grad_norm": 0.846202552318573,
      "learning_rate": 5.7887223411848676e-05,
      "loss": 0.0591,
      "step": 3983
    },
    {
      "epoch": 2.132797858099063,
      "grad_norm": 1.100271224975586,
      "learning_rate": 5.785153461812991e-05,
      "loss": 0.0543,
      "step": 3984
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 0.8894585371017456,
      "learning_rate": 5.781584582441114e-05,
      "loss": 0.0566,
      "step": 3985
    },
    {
      "epoch": 2.1338688085676036,
      "grad_norm": 0.5963336825370789,
      "learning_rate": 5.778015703069236e-05,
      "loss": 0.0207,
      "step": 3986
    },
    {
      "epoch": 2.134404283801874,
      "grad_norm": 4.052174091339111,
      "learning_rate": 5.774446823697359e-05,
      "loss": 0.0877,
      "step": 3987
    },
    {
      "epoch": 2.1349397590361447,
      "grad_norm": 0.732401430606842,
      "learning_rate": 5.770877944325482e-05,
      "loss": 0.0284,
      "step": 3988
    },
    {
      "epoch": 2.135475234270415,
      "grad_norm": 1.177761197090149,
      "learning_rate": 5.767309064953604e-05,
      "loss": 0.0755,
      "step": 3989
    },
    {
      "epoch": 2.1360107095046854,
      "grad_norm": 0.9079590439796448,
      "learning_rate": 5.7637401855817276e-05,
      "loss": 0.0528,
      "step": 3990
    },
    {
      "epoch": 2.1365461847389557,
      "grad_norm": 0.7250401973724365,
      "learning_rate": 5.76017130620985e-05,
      "loss": 0.0252,
      "step": 3991
    },
    {
      "epoch": 2.137081659973226,
      "grad_norm": 1.0975152254104614,
      "learning_rate": 5.7566024268379736e-05,
      "loss": 0.0458,
      "step": 3992
    },
    {
      "epoch": 2.137617135207497,
      "grad_norm": 0.6142187714576721,
      "learning_rate": 5.7530335474660956e-05,
      "loss": 0.0296,
      "step": 3993
    },
    {
      "epoch": 2.138152610441767,
      "grad_norm": 0.9133419394493103,
      "learning_rate": 5.749464668094219e-05,
      "loss": 0.0309,
      "step": 3994
    },
    {
      "epoch": 2.1386880856760375,
      "grad_norm": 1.2080512046813965,
      "learning_rate": 5.7458957887223416e-05,
      "loss": 0.0374,
      "step": 3995
    },
    {
      "epoch": 2.139223560910308,
      "grad_norm": 0.8555052876472473,
      "learning_rate": 5.7423269093504636e-05,
      "loss": 0.0676,
      "step": 3996
    },
    {
      "epoch": 2.139759036144578,
      "grad_norm": 0.42854073643684387,
      "learning_rate": 5.738758029978587e-05,
      "loss": 0.0237,
      "step": 3997
    },
    {
      "epoch": 2.1402945113788485,
      "grad_norm": 1.0240659713745117,
      "learning_rate": 5.73518915060671e-05,
      "loss": 0.101,
      "step": 3998
    },
    {
      "epoch": 2.1408299866131193,
      "grad_norm": 0.5992710590362549,
      "learning_rate": 5.731620271234832e-05,
      "loss": 0.0292,
      "step": 3999
    },
    {
      "epoch": 2.1413654618473896,
      "grad_norm": 1.3261889219284058,
      "learning_rate": 5.728051391862955e-05,
      "loss": 0.0934,
      "step": 4000
    },
    {
      "epoch": 2.14190093708166,
      "grad_norm": 0.9384840130805969,
      "learning_rate": 5.724482512491078e-05,
      "loss": 0.0719,
      "step": 4001
    },
    {
      "epoch": 2.1424364123159303,
      "grad_norm": 1.6196736097335815,
      "learning_rate": 5.7209136331192e-05,
      "loss": 0.0818,
      "step": 4002
    },
    {
      "epoch": 2.1429718875502006,
      "grad_norm": 1.0190739631652832,
      "learning_rate": 5.7173447537473236e-05,
      "loss": 0.0537,
      "step": 4003
    },
    {
      "epoch": 2.1435073627844714,
      "grad_norm": 3.758133888244629,
      "learning_rate": 5.713775874375446e-05,
      "loss": 0.0776,
      "step": 4004
    },
    {
      "epoch": 2.1440428380187417,
      "grad_norm": 2.0862414836883545,
      "learning_rate": 5.7102069950035696e-05,
      "loss": 0.2399,
      "step": 4005
    },
    {
      "epoch": 2.144578313253012,
      "grad_norm": 1.0856319665908813,
      "learning_rate": 5.7066381156316916e-05,
      "loss": 0.0917,
      "step": 4006
    },
    {
      "epoch": 2.1451137884872824,
      "grad_norm": 1.190091848373413,
      "learning_rate": 5.703069236259815e-05,
      "loss": 0.0861,
      "step": 4007
    },
    {
      "epoch": 2.1456492637215527,
      "grad_norm": 1.0692015886306763,
      "learning_rate": 5.6995003568879376e-05,
      "loss": 0.0351,
      "step": 4008
    },
    {
      "epoch": 2.1461847389558235,
      "grad_norm": 0.23160403966903687,
      "learning_rate": 5.6959314775160596e-05,
      "loss": 0.0145,
      "step": 4009
    },
    {
      "epoch": 2.146720214190094,
      "grad_norm": 0.6414220333099365,
      "learning_rate": 5.692362598144183e-05,
      "loss": 0.0547,
      "step": 4010
    },
    {
      "epoch": 2.147255689424364,
      "grad_norm": 1.1901010274887085,
      "learning_rate": 5.688793718772306e-05,
      "loss": 0.0581,
      "step": 4011
    },
    {
      "epoch": 2.1477911646586345,
      "grad_norm": 0.6560719013214111,
      "learning_rate": 5.685224839400428e-05,
      "loss": 0.0506,
      "step": 4012
    },
    {
      "epoch": 2.148326639892905,
      "grad_norm": 0.9223063588142395,
      "learning_rate": 5.6816559600285516e-05,
      "loss": 0.0516,
      "step": 4013
    },
    {
      "epoch": 2.148862115127175,
      "grad_norm": 0.9309479594230652,
      "learning_rate": 5.678087080656674e-05,
      "loss": 0.0512,
      "step": 4014
    },
    {
      "epoch": 2.149397590361446,
      "grad_norm": 1.3257620334625244,
      "learning_rate": 5.674518201284796e-05,
      "loss": 0.0585,
      "step": 4015
    },
    {
      "epoch": 2.1499330655957163,
      "grad_norm": 1.1276438236236572,
      "learning_rate": 5.6709493219129196e-05,
      "loss": 0.0615,
      "step": 4016
    },
    {
      "epoch": 2.1504685408299866,
      "grad_norm": 0.5537799596786499,
      "learning_rate": 5.667380442541043e-05,
      "loss": 0.0333,
      "step": 4017
    },
    {
      "epoch": 2.151004016064257,
      "grad_norm": 0.5972478985786438,
      "learning_rate": 5.663811563169165e-05,
      "loss": 0.0466,
      "step": 4018
    },
    {
      "epoch": 2.1515394912985273,
      "grad_norm": 1.0868536233901978,
      "learning_rate": 5.6602426837972876e-05,
      "loss": 0.0621,
      "step": 4019
    },
    {
      "epoch": 2.152074966532798,
      "grad_norm": 0.8352587223052979,
      "learning_rate": 5.656673804425411e-05,
      "loss": 0.0656,
      "step": 4020
    },
    {
      "epoch": 2.1526104417670684,
      "grad_norm": 1.1033670902252197,
      "learning_rate": 5.653104925053534e-05,
      "loss": 0.0609,
      "step": 4021
    },
    {
      "epoch": 2.1531459170013387,
      "grad_norm": 1.5403661727905273,
      "learning_rate": 5.649536045681656e-05,
      "loss": 0.0903,
      "step": 4022
    },
    {
      "epoch": 2.153681392235609,
      "grad_norm": 1.0336112976074219,
      "learning_rate": 5.645967166309779e-05,
      "loss": 0.0392,
      "step": 4023
    },
    {
      "epoch": 2.1542168674698794,
      "grad_norm": 0.8505182862281799,
      "learning_rate": 5.642398286937902e-05,
      "loss": 0.0679,
      "step": 4024
    },
    {
      "epoch": 2.1547523427041497,
      "grad_norm": 0.5933133959770203,
      "learning_rate": 5.638829407566024e-05,
      "loss": 0.0266,
      "step": 4025
    },
    {
      "epoch": 2.1552878179384205,
      "grad_norm": 0.8024275898933411,
      "learning_rate": 5.6352605281941476e-05,
      "loss": 0.0332,
      "step": 4026
    },
    {
      "epoch": 2.155823293172691,
      "grad_norm": 0.6713305115699768,
      "learning_rate": 5.63169164882227e-05,
      "loss": 0.0416,
      "step": 4027
    },
    {
      "epoch": 2.156358768406961,
      "grad_norm": 1.6595945358276367,
      "learning_rate": 5.628122769450392e-05,
      "loss": 0.1367,
      "step": 4028
    },
    {
      "epoch": 2.1568942436412315,
      "grad_norm": 1.2046761512756348,
      "learning_rate": 5.6245538900785156e-05,
      "loss": 0.0703,
      "step": 4029
    },
    {
      "epoch": 2.157429718875502,
      "grad_norm": 0.7645328640937805,
      "learning_rate": 5.620985010706639e-05,
      "loss": 0.0302,
      "step": 4030
    },
    {
      "epoch": 2.1579651941097726,
      "grad_norm": 0.8961918354034424,
      "learning_rate": 5.617416131334761e-05,
      "loss": 0.0315,
      "step": 4031
    },
    {
      "epoch": 2.158500669344043,
      "grad_norm": 1.4656996726989746,
      "learning_rate": 5.6138472519628836e-05,
      "loss": 0.0973,
      "step": 4032
    },
    {
      "epoch": 2.1590361445783133,
      "grad_norm": 1.7657743692398071,
      "learning_rate": 5.610278372591007e-05,
      "loss": 0.0928,
      "step": 4033
    },
    {
      "epoch": 2.1595716198125836,
      "grad_norm": 2.071960687637329,
      "learning_rate": 5.60670949321913e-05,
      "loss": 0.0948,
      "step": 4034
    },
    {
      "epoch": 2.160107095046854,
      "grad_norm": 1.8973851203918457,
      "learning_rate": 5.603140613847252e-05,
      "loss": 0.1225,
      "step": 4035
    },
    {
      "epoch": 2.1606425702811247,
      "grad_norm": 0.7943278551101685,
      "learning_rate": 5.599571734475375e-05,
      "loss": 0.0588,
      "step": 4036
    },
    {
      "epoch": 2.161178045515395,
      "grad_norm": 1.9977556467056274,
      "learning_rate": 5.596002855103498e-05,
      "loss": 0.0821,
      "step": 4037
    },
    {
      "epoch": 2.1617135207496654,
      "grad_norm": 1.2618743181228638,
      "learning_rate": 5.59243397573162e-05,
      "loss": 0.0723,
      "step": 4038
    },
    {
      "epoch": 2.1622489959839357,
      "grad_norm": 1.5127276182174683,
      "learning_rate": 5.5888650963597436e-05,
      "loss": 0.0848,
      "step": 4039
    },
    {
      "epoch": 2.162784471218206,
      "grad_norm": 0.900606095790863,
      "learning_rate": 5.585296216987866e-05,
      "loss": 0.1008,
      "step": 4040
    },
    {
      "epoch": 2.1633199464524764,
      "grad_norm": 1.1538586616516113,
      "learning_rate": 5.581727337615988e-05,
      "loss": 0.0816,
      "step": 4041
    },
    {
      "epoch": 2.163855421686747,
      "grad_norm": 1.6264728307724,
      "learning_rate": 5.5781584582441116e-05,
      "loss": 0.0919,
      "step": 4042
    },
    {
      "epoch": 2.1643908969210175,
      "grad_norm": 4.316429615020752,
      "learning_rate": 5.574589578872235e-05,
      "loss": 0.2045,
      "step": 4043
    },
    {
      "epoch": 2.164926372155288,
      "grad_norm": 1.426234483718872,
      "learning_rate": 5.571020699500357e-05,
      "loss": 0.0752,
      "step": 4044
    },
    {
      "epoch": 2.165461847389558,
      "grad_norm": 2.5760581493377686,
      "learning_rate": 5.5674518201284796e-05,
      "loss": 0.0968,
      "step": 4045
    },
    {
      "epoch": 2.1659973226238285,
      "grad_norm": 1.847506046295166,
      "learning_rate": 5.563882940756603e-05,
      "loss": 0.1568,
      "step": 4046
    },
    {
      "epoch": 2.1665327978580993,
      "grad_norm": 0.5456823706626892,
      "learning_rate": 5.560314061384726e-05,
      "loss": 0.0342,
      "step": 4047
    },
    {
      "epoch": 2.1670682730923696,
      "grad_norm": 3.341468572616577,
      "learning_rate": 5.556745182012848e-05,
      "loss": 0.0714,
      "step": 4048
    },
    {
      "epoch": 2.16760374832664,
      "grad_norm": 2.0438549518585205,
      "learning_rate": 5.553176302640971e-05,
      "loss": 0.0594,
      "step": 4049
    },
    {
      "epoch": 2.1681392235609103,
      "grad_norm": 0.843328595161438,
      "learning_rate": 5.549607423269094e-05,
      "loss": 0.052,
      "step": 4050
    },
    {
      "epoch": 2.1686746987951806,
      "grad_norm": 1.8875813484191895,
      "learning_rate": 5.546038543897216e-05,
      "loss": 0.0661,
      "step": 4051
    },
    {
      "epoch": 2.169210174029451,
      "grad_norm": 1.5140736103057861,
      "learning_rate": 5.5424696645253396e-05,
      "loss": 0.0558,
      "step": 4052
    },
    {
      "epoch": 2.1697456492637217,
      "grad_norm": 2.745551109313965,
      "learning_rate": 5.538900785153462e-05,
      "loss": 0.0912,
      "step": 4053
    },
    {
      "epoch": 2.170281124497992,
      "grad_norm": 1.4035303592681885,
      "learning_rate": 5.535331905781584e-05,
      "loss": 0.0775,
      "step": 4054
    },
    {
      "epoch": 2.1708165997322624,
      "grad_norm": 1.9941128492355347,
      "learning_rate": 5.5317630264097076e-05,
      "loss": 0.0879,
      "step": 4055
    },
    {
      "epoch": 2.1713520749665327,
      "grad_norm": 2.7004170417785645,
      "learning_rate": 5.528194147037831e-05,
      "loss": 0.1214,
      "step": 4056
    },
    {
      "epoch": 2.171887550200803,
      "grad_norm": 2.528193950653076,
      "learning_rate": 5.524625267665953e-05,
      "loss": 0.1423,
      "step": 4057
    },
    {
      "epoch": 2.172423025435074,
      "grad_norm": 0.5472686886787415,
      "learning_rate": 5.5210563882940756e-05,
      "loss": 0.0441,
      "step": 4058
    },
    {
      "epoch": 2.172958500669344,
      "grad_norm": 0.7350555062294006,
      "learning_rate": 5.517487508922199e-05,
      "loss": 0.0457,
      "step": 4059
    },
    {
      "epoch": 2.1734939759036145,
      "grad_norm": 1.4964914321899414,
      "learning_rate": 5.513918629550322e-05,
      "loss": 0.0858,
      "step": 4060
    },
    {
      "epoch": 2.174029451137885,
      "grad_norm": 2.442645311355591,
      "learning_rate": 5.510349750178444e-05,
      "loss": 0.0885,
      "step": 4061
    },
    {
      "epoch": 2.174564926372155,
      "grad_norm": 1.2529851198196411,
      "learning_rate": 5.506780870806567e-05,
      "loss": 0.1051,
      "step": 4062
    },
    {
      "epoch": 2.175100401606426,
      "grad_norm": 1.9150354862213135,
      "learning_rate": 5.50321199143469e-05,
      "loss": 0.0862,
      "step": 4063
    },
    {
      "epoch": 2.1756358768406963,
      "grad_norm": 0.5526925325393677,
      "learning_rate": 5.499643112062812e-05,
      "loss": 0.0276,
      "step": 4064
    },
    {
      "epoch": 2.1761713520749666,
      "grad_norm": 1.2303862571716309,
      "learning_rate": 5.4960742326909356e-05,
      "loss": 0.0952,
      "step": 4065
    },
    {
      "epoch": 2.176706827309237,
      "grad_norm": 0.6393527388572693,
      "learning_rate": 5.492505353319058e-05,
      "loss": 0.0422,
      "step": 4066
    },
    {
      "epoch": 2.1772423025435073,
      "grad_norm": 1.1704087257385254,
      "learning_rate": 5.48893647394718e-05,
      "loss": 0.117,
      "step": 4067
    },
    {
      "epoch": 2.1777777777777776,
      "grad_norm": 1.055505633354187,
      "learning_rate": 5.4853675945753036e-05,
      "loss": 0.0696,
      "step": 4068
    },
    {
      "epoch": 2.1783132530120484,
      "grad_norm": 1.5337657928466797,
      "learning_rate": 5.481798715203427e-05,
      "loss": 0.136,
      "step": 4069
    },
    {
      "epoch": 2.1788487282463187,
      "grad_norm": 6.8025078773498535,
      "learning_rate": 5.478229835831549e-05,
      "loss": 0.0709,
      "step": 4070
    },
    {
      "epoch": 2.179384203480589,
      "grad_norm": 1.799785852432251,
      "learning_rate": 5.4746609564596716e-05,
      "loss": 0.1443,
      "step": 4071
    },
    {
      "epoch": 2.1799196787148594,
      "grad_norm": 1.053378939628601,
      "learning_rate": 5.471092077087795e-05,
      "loss": 0.0471,
      "step": 4072
    },
    {
      "epoch": 2.1804551539491297,
      "grad_norm": 1.179397463798523,
      "learning_rate": 5.467523197715917e-05,
      "loss": 0.0517,
      "step": 4073
    },
    {
      "epoch": 2.1809906291834005,
      "grad_norm": 1.1960245370864868,
      "learning_rate": 5.46395431834404e-05,
      "loss": 0.1137,
      "step": 4074
    },
    {
      "epoch": 2.181526104417671,
      "grad_norm": 0.7176176905632019,
      "learning_rate": 5.460385438972163e-05,
      "loss": 0.0689,
      "step": 4075
    },
    {
      "epoch": 2.182061579651941,
      "grad_norm": 1.4081761837005615,
      "learning_rate": 5.456816559600286e-05,
      "loss": 0.1025,
      "step": 4076
    },
    {
      "epoch": 2.1825970548862115,
      "grad_norm": 1.5393770933151245,
      "learning_rate": 5.453247680228408e-05,
      "loss": 0.1132,
      "step": 4077
    },
    {
      "epoch": 2.183132530120482,
      "grad_norm": 0.9507346153259277,
      "learning_rate": 5.4496788008565316e-05,
      "loss": 0.0991,
      "step": 4078
    },
    {
      "epoch": 2.183668005354752,
      "grad_norm": 1.0828107595443726,
      "learning_rate": 5.446109921484654e-05,
      "loss": 0.126,
      "step": 4079
    },
    {
      "epoch": 2.184203480589023,
      "grad_norm": 1.4969992637634277,
      "learning_rate": 5.442541042112776e-05,
      "loss": 0.0933,
      "step": 4080
    },
    {
      "epoch": 2.1847389558232932,
      "grad_norm": 1.203020691871643,
      "learning_rate": 5.4389721627408996e-05,
      "loss": 0.1123,
      "step": 4081
    },
    {
      "epoch": 2.1852744310575636,
      "grad_norm": 0.8781272768974304,
      "learning_rate": 5.435403283369023e-05,
      "loss": 0.1037,
      "step": 4082
    },
    {
      "epoch": 2.185809906291834,
      "grad_norm": 0.4784485995769501,
      "learning_rate": 5.431834403997145e-05,
      "loss": 0.0492,
      "step": 4083
    },
    {
      "epoch": 2.1863453815261042,
      "grad_norm": 0.941339910030365,
      "learning_rate": 5.4282655246252676e-05,
      "loss": 0.075,
      "step": 4084
    },
    {
      "epoch": 2.186880856760375,
      "grad_norm": 0.5143077373504639,
      "learning_rate": 5.424696645253391e-05,
      "loss": 0.026,
      "step": 4085
    },
    {
      "epoch": 2.1874163319946454,
      "grad_norm": 1.3062386512756348,
      "learning_rate": 5.421127765881513e-05,
      "loss": 0.0496,
      "step": 4086
    },
    {
      "epoch": 2.1879518072289157,
      "grad_norm": 0.9992437362670898,
      "learning_rate": 5.417558886509636e-05,
      "loss": 0.0701,
      "step": 4087
    },
    {
      "epoch": 2.188487282463186,
      "grad_norm": 0.9524262547492981,
      "learning_rate": 5.413990007137759e-05,
      "loss": 0.05,
      "step": 4088
    },
    {
      "epoch": 2.1890227576974564,
      "grad_norm": 0.7604085206985474,
      "learning_rate": 5.410421127765882e-05,
      "loss": 0.0599,
      "step": 4089
    },
    {
      "epoch": 2.189558232931727,
      "grad_norm": 1.178038477897644,
      "learning_rate": 5.406852248394004e-05,
      "loss": 0.0782,
      "step": 4090
    },
    {
      "epoch": 2.1900937081659975,
      "grad_norm": 1.0832332372665405,
      "learning_rate": 5.4032833690221276e-05,
      "loss": 0.0616,
      "step": 4091
    },
    {
      "epoch": 2.190629183400268,
      "grad_norm": 0.6067655682563782,
      "learning_rate": 5.39971448965025e-05,
      "loss": 0.0504,
      "step": 4092
    },
    {
      "epoch": 2.191164658634538,
      "grad_norm": 1.2689660787582397,
      "learning_rate": 5.396145610278372e-05,
      "loss": 0.0724,
      "step": 4093
    },
    {
      "epoch": 2.1917001338688085,
      "grad_norm": 0.8049342632293701,
      "learning_rate": 5.3925767309064956e-05,
      "loss": 0.0716,
      "step": 4094
    },
    {
      "epoch": 2.192235609103079,
      "grad_norm": 1.6212427616119385,
      "learning_rate": 5.389007851534619e-05,
      "loss": 0.1216,
      "step": 4095
    },
    {
      "epoch": 2.1927710843373496,
      "grad_norm": 1.1533479690551758,
      "learning_rate": 5.385438972162741e-05,
      "loss": 0.0671,
      "step": 4096
    },
    {
      "epoch": 2.19330655957162,
      "grad_norm": 0.7769344449043274,
      "learning_rate": 5.3818700927908636e-05,
      "loss": 0.0878,
      "step": 4097
    },
    {
      "epoch": 2.1938420348058902,
      "grad_norm": 0.3881104290485382,
      "learning_rate": 5.378301213418987e-05,
      "loss": 0.0181,
      "step": 4098
    },
    {
      "epoch": 2.1943775100401606,
      "grad_norm": 0.7062041163444519,
      "learning_rate": 5.374732334047109e-05,
      "loss": 0.0239,
      "step": 4099
    },
    {
      "epoch": 2.194912985274431,
      "grad_norm": 2.686206579208374,
      "learning_rate": 5.371163454675232e-05,
      "loss": 0.1864,
      "step": 4100
    },
    {
      "epoch": 2.1954484605087012,
      "grad_norm": 0.3399568498134613,
      "learning_rate": 5.367594575303355e-05,
      "loss": 0.0192,
      "step": 4101
    },
    {
      "epoch": 2.195983935742972,
      "grad_norm": 1.6950769424438477,
      "learning_rate": 5.364025695931478e-05,
      "loss": 0.0952,
      "step": 4102
    },
    {
      "epoch": 2.1965194109772423,
      "grad_norm": 0.5465486645698547,
      "learning_rate": 5.3604568165596e-05,
      "loss": 0.0192,
      "step": 4103
    },
    {
      "epoch": 2.1970548862115127,
      "grad_norm": 0.4517972767353058,
      "learning_rate": 5.3568879371877236e-05,
      "loss": 0.0338,
      "step": 4104
    },
    {
      "epoch": 2.197590361445783,
      "grad_norm": 1.8495218753814697,
      "learning_rate": 5.353319057815846e-05,
      "loss": 0.1516,
      "step": 4105
    },
    {
      "epoch": 2.1981258366800533,
      "grad_norm": 1.5188871622085571,
      "learning_rate": 5.349750178443968e-05,
      "loss": 0.1174,
      "step": 4106
    },
    {
      "epoch": 2.198661311914324,
      "grad_norm": 1.3187131881713867,
      "learning_rate": 5.3461812990720916e-05,
      "loss": 0.0474,
      "step": 4107
    },
    {
      "epoch": 2.1991967871485945,
      "grad_norm": 2.07094144821167,
      "learning_rate": 5.342612419700215e-05,
      "loss": 0.0675,
      "step": 4108
    },
    {
      "epoch": 2.199732262382865,
      "grad_norm": 0.5993894934654236,
      "learning_rate": 5.339043540328337e-05,
      "loss": 0.0332,
      "step": 4109
    },
    {
      "epoch": 2.200267737617135,
      "grad_norm": 0.6380741000175476,
      "learning_rate": 5.3354746609564596e-05,
      "loss": 0.0336,
      "step": 4110
    },
    {
      "epoch": 2.2008032128514055,
      "grad_norm": 0.9084540009498596,
      "learning_rate": 5.331905781584583e-05,
      "loss": 0.0448,
      "step": 4111
    },
    {
      "epoch": 2.2013386880856762,
      "grad_norm": 1.0486844778060913,
      "learning_rate": 5.328336902212705e-05,
      "loss": 0.053,
      "step": 4112
    },
    {
      "epoch": 2.2018741633199466,
      "grad_norm": 0.588244616985321,
      "learning_rate": 5.324768022840828e-05,
      "loss": 0.041,
      "step": 4113
    },
    {
      "epoch": 2.202409638554217,
      "grad_norm": 1.029000163078308,
      "learning_rate": 5.321199143468951e-05,
      "loss": 0.0619,
      "step": 4114
    },
    {
      "epoch": 2.2029451137884872,
      "grad_norm": 1.241004467010498,
      "learning_rate": 5.317630264097074e-05,
      "loss": 0.1081,
      "step": 4115
    },
    {
      "epoch": 2.2034805890227576,
      "grad_norm": 0.7232536673545837,
      "learning_rate": 5.314061384725196e-05,
      "loss": 0.0421,
      "step": 4116
    },
    {
      "epoch": 2.2040160642570283,
      "grad_norm": 0.7983373403549194,
      "learning_rate": 5.3104925053533196e-05,
      "loss": 0.0375,
      "step": 4117
    },
    {
      "epoch": 2.2045515394912987,
      "grad_norm": 1.0136771202087402,
      "learning_rate": 5.306923625981442e-05,
      "loss": 0.0924,
      "step": 4118
    },
    {
      "epoch": 2.205087014725569,
      "grad_norm": 0.7303133010864258,
      "learning_rate": 5.303354746609564e-05,
      "loss": 0.0445,
      "step": 4119
    },
    {
      "epoch": 2.2056224899598393,
      "grad_norm": 0.8670485615730286,
      "learning_rate": 5.2997858672376876e-05,
      "loss": 0.0463,
      "step": 4120
    },
    {
      "epoch": 2.2061579651941097,
      "grad_norm": 1.657502293586731,
      "learning_rate": 5.296216987865811e-05,
      "loss": 0.1378,
      "step": 4121
    },
    {
      "epoch": 2.20669344042838,
      "grad_norm": 0.9613640904426575,
      "learning_rate": 5.292648108493933e-05,
      "loss": 0.0639,
      "step": 4122
    },
    {
      "epoch": 2.207228915662651,
      "grad_norm": 3.0054304599761963,
      "learning_rate": 5.2890792291220556e-05,
      "loss": 0.0466,
      "step": 4123
    },
    {
      "epoch": 2.207764390896921,
      "grad_norm": 1.481587290763855,
      "learning_rate": 5.285510349750179e-05,
      "loss": 0.1072,
      "step": 4124
    },
    {
      "epoch": 2.2082998661311914,
      "grad_norm": 1.0436519384384155,
      "learning_rate": 5.281941470378301e-05,
      "loss": 0.0593,
      "step": 4125
    },
    {
      "epoch": 2.208835341365462,
      "grad_norm": 1.19485604763031,
      "learning_rate": 5.278372591006424e-05,
      "loss": 0.0393,
      "step": 4126
    },
    {
      "epoch": 2.209370816599732,
      "grad_norm": 0.5369299054145813,
      "learning_rate": 5.274803711634547e-05,
      "loss": 0.0319,
      "step": 4127
    },
    {
      "epoch": 2.2099062918340024,
      "grad_norm": 0.5047128200531006,
      "learning_rate": 5.271234832262669e-05,
      "loss": 0.0247,
      "step": 4128
    },
    {
      "epoch": 2.2104417670682732,
      "grad_norm": 1.2165099382400513,
      "learning_rate": 5.267665952890792e-05,
      "loss": 0.0628,
      "step": 4129
    },
    {
      "epoch": 2.2109772423025436,
      "grad_norm": 0.7935431599617004,
      "learning_rate": 5.2640970735189156e-05,
      "loss": 0.0404,
      "step": 4130
    },
    {
      "epoch": 2.211512717536814,
      "grad_norm": 4.609903335571289,
      "learning_rate": 5.260528194147038e-05,
      "loss": 0.1714,
      "step": 4131
    },
    {
      "epoch": 2.212048192771084,
      "grad_norm": 0.8503442406654358,
      "learning_rate": 5.25695931477516e-05,
      "loss": 0.0355,
      "step": 4132
    },
    {
      "epoch": 2.2125836680053546,
      "grad_norm": 1.316530466079712,
      "learning_rate": 5.2533904354032836e-05,
      "loss": 0.0751,
      "step": 4133
    },
    {
      "epoch": 2.2131191432396253,
      "grad_norm": 0.6713535785675049,
      "learning_rate": 5.249821556031407e-05,
      "loss": 0.038,
      "step": 4134
    },
    {
      "epoch": 2.2136546184738957,
      "grad_norm": 0.9510065913200378,
      "learning_rate": 5.246252676659529e-05,
      "loss": 0.0699,
      "step": 4135
    },
    {
      "epoch": 2.214190093708166,
      "grad_norm": 1.5479806661605835,
      "learning_rate": 5.2426837972876516e-05,
      "loss": 0.0734,
      "step": 4136
    },
    {
      "epoch": 2.2147255689424363,
      "grad_norm": 0.6492801904678345,
      "learning_rate": 5.239114917915775e-05,
      "loss": 0.031,
      "step": 4137
    },
    {
      "epoch": 2.2152610441767067,
      "grad_norm": 1.2595292329788208,
      "learning_rate": 5.235546038543897e-05,
      "loss": 0.0421,
      "step": 4138
    },
    {
      "epoch": 2.2157965194109774,
      "grad_norm": 1.6244440078735352,
      "learning_rate": 5.23197715917202e-05,
      "loss": 0.0912,
      "step": 4139
    },
    {
      "epoch": 2.2163319946452478,
      "grad_norm": 2.274765729904175,
      "learning_rate": 5.228408279800143e-05,
      "loss": 0.113,
      "step": 4140
    },
    {
      "epoch": 2.216867469879518,
      "grad_norm": 1.22390878200531,
      "learning_rate": 5.2248394004282656e-05,
      "loss": 0.083,
      "step": 4141
    },
    {
      "epoch": 2.2174029451137884,
      "grad_norm": 2.12837553024292,
      "learning_rate": 5.221270521056388e-05,
      "loss": 0.1099,
      "step": 4142
    },
    {
      "epoch": 2.2179384203480588,
      "grad_norm": 1.4198561906814575,
      "learning_rate": 5.2177016416845116e-05,
      "loss": 0.0687,
      "step": 4143
    },
    {
      "epoch": 2.2184738955823295,
      "grad_norm": 1.2686530351638794,
      "learning_rate": 5.214132762312634e-05,
      "loss": 0.0324,
      "step": 4144
    },
    {
      "epoch": 2.2190093708166,
      "grad_norm": 0.5464491844177246,
      "learning_rate": 5.210563882940757e-05,
      "loss": 0.0204,
      "step": 4145
    },
    {
      "epoch": 2.21954484605087,
      "grad_norm": 1.6448054313659668,
      "learning_rate": 5.2069950035688796e-05,
      "loss": 0.0866,
      "step": 4146
    },
    {
      "epoch": 2.2200803212851405,
      "grad_norm": 1.5193289518356323,
      "learning_rate": 5.203426124197003e-05,
      "loss": 0.0613,
      "step": 4147
    },
    {
      "epoch": 2.220615796519411,
      "grad_norm": 1.7597216367721558,
      "learning_rate": 5.199857244825125e-05,
      "loss": 0.066,
      "step": 4148
    },
    {
      "epoch": 2.221151271753681,
      "grad_norm": 1.6365081071853638,
      "learning_rate": 5.196288365453248e-05,
      "loss": 0.087,
      "step": 4149
    },
    {
      "epoch": 2.221686746987952,
      "grad_norm": 2.0021138191223145,
      "learning_rate": 5.192719486081371e-05,
      "loss": 0.065,
      "step": 4150
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 1.131388545036316,
      "learning_rate": 5.189150606709493e-05,
      "loss": 0.0185,
      "step": 4151
    },
    {
      "epoch": 2.2227576974564927,
      "grad_norm": 2.0079848766326904,
      "learning_rate": 5.185581727337616e-05,
      "loss": 0.0331,
      "step": 4152
    },
    {
      "epoch": 2.223293172690763,
      "grad_norm": 0.9156384468078613,
      "learning_rate": 5.1820128479657396e-05,
      "loss": 0.0463,
      "step": 4153
    },
    {
      "epoch": 2.2238286479250333,
      "grad_norm": 2.4680395126342773,
      "learning_rate": 5.1784439685938616e-05,
      "loss": 0.1212,
      "step": 4154
    },
    {
      "epoch": 2.2243641231593037,
      "grad_norm": 3.75042986869812,
      "learning_rate": 5.174875089221984e-05,
      "loss": 0.1097,
      "step": 4155
    },
    {
      "epoch": 2.2248995983935744,
      "grad_norm": 1.4276686906814575,
      "learning_rate": 5.1713062098501076e-05,
      "loss": 0.0886,
      "step": 4156
    },
    {
      "epoch": 2.2254350736278448,
      "grad_norm": 3.2905383110046387,
      "learning_rate": 5.167737330478231e-05,
      "loss": 0.0564,
      "step": 4157
    },
    {
      "epoch": 2.225970548862115,
      "grad_norm": 1.1139538288116455,
      "learning_rate": 5.164168451106353e-05,
      "loss": 0.0875,
      "step": 4158
    },
    {
      "epoch": 2.2265060240963854,
      "grad_norm": 1.026931881904602,
      "learning_rate": 5.1605995717344756e-05,
      "loss": 0.0386,
      "step": 4159
    },
    {
      "epoch": 2.2270414993306558,
      "grad_norm": 1.4234176874160767,
      "learning_rate": 5.157030692362599e-05,
      "loss": 0.065,
      "step": 4160
    },
    {
      "epoch": 2.2275769745649265,
      "grad_norm": 1.0634180307388306,
      "learning_rate": 5.153461812990721e-05,
      "loss": 0.0678,
      "step": 4161
    },
    {
      "epoch": 2.228112449799197,
      "grad_norm": 3.5655460357666016,
      "learning_rate": 5.149892933618844e-05,
      "loss": 0.1347,
      "step": 4162
    },
    {
      "epoch": 2.228647925033467,
      "grad_norm": 0.9495283961296082,
      "learning_rate": 5.146324054246967e-05,
      "loss": 0.0488,
      "step": 4163
    },
    {
      "epoch": 2.2291834002677375,
      "grad_norm": 0.9051109552383423,
      "learning_rate": 5.142755174875089e-05,
      "loss": 0.0239,
      "step": 4164
    },
    {
      "epoch": 2.229718875502008,
      "grad_norm": 1.1203606128692627,
      "learning_rate": 5.139186295503212e-05,
      "loss": 0.0659,
      "step": 4165
    },
    {
      "epoch": 2.2302543507362786,
      "grad_norm": 1.0386086702346802,
      "learning_rate": 5.1356174161313356e-05,
      "loss": 0.0369,
      "step": 4166
    },
    {
      "epoch": 2.230789825970549,
      "grad_norm": 0.5314139127731323,
      "learning_rate": 5.1320485367594576e-05,
      "loss": 0.0146,
      "step": 4167
    },
    {
      "epoch": 2.2313253012048193,
      "grad_norm": 3.836259126663208,
      "learning_rate": 5.12847965738758e-05,
      "loss": 0.1926,
      "step": 4168
    },
    {
      "epoch": 2.2318607764390896,
      "grad_norm": 1.7489495277404785,
      "learning_rate": 5.1249107780157036e-05,
      "loss": 0.1214,
      "step": 4169
    },
    {
      "epoch": 2.23239625167336,
      "grad_norm": 2.2765233516693115,
      "learning_rate": 5.121341898643827e-05,
      "loss": 0.0962,
      "step": 4170
    },
    {
      "epoch": 2.2329317269076308,
      "grad_norm": 2.137887477874756,
      "learning_rate": 5.117773019271949e-05,
      "loss": 0.0538,
      "step": 4171
    },
    {
      "epoch": 2.233467202141901,
      "grad_norm": 0.5495654940605164,
      "learning_rate": 5.1142041399000716e-05,
      "loss": 0.0595,
      "step": 4172
    },
    {
      "epoch": 2.2340026773761714,
      "grad_norm": 1.9615072011947632,
      "learning_rate": 5.110635260528195e-05,
      "loss": 0.1268,
      "step": 4173
    },
    {
      "epoch": 2.2345381526104418,
      "grad_norm": 1.2222683429718018,
      "learning_rate": 5.107066381156317e-05,
      "loss": 0.0729,
      "step": 4174
    },
    {
      "epoch": 2.235073627844712,
      "grad_norm": 0.5222070217132568,
      "learning_rate": 5.10349750178444e-05,
      "loss": 0.0214,
      "step": 4175
    },
    {
      "epoch": 2.2356091030789824,
      "grad_norm": 1.582631230354309,
      "learning_rate": 5.099928622412563e-05,
      "loss": 0.0758,
      "step": 4176
    },
    {
      "epoch": 2.236144578313253,
      "grad_norm": 1.2414621114730835,
      "learning_rate": 5.096359743040685e-05,
      "loss": 0.0601,
      "step": 4177
    },
    {
      "epoch": 2.2366800535475235,
      "grad_norm": 3.3342368602752686,
      "learning_rate": 5.092790863668808e-05,
      "loss": 0.1302,
      "step": 4178
    },
    {
      "epoch": 2.237215528781794,
      "grad_norm": 0.6771016716957092,
      "learning_rate": 5.0892219842969316e-05,
      "loss": 0.0405,
      "step": 4179
    },
    {
      "epoch": 2.237751004016064,
      "grad_norm": 0.953847348690033,
      "learning_rate": 5.0856531049250536e-05,
      "loss": 0.066,
      "step": 4180
    },
    {
      "epoch": 2.2382864792503345,
      "grad_norm": 1.2779417037963867,
      "learning_rate": 5.082084225553176e-05,
      "loss": 0.0664,
      "step": 4181
    },
    {
      "epoch": 2.238821954484605,
      "grad_norm": 4.516056060791016,
      "learning_rate": 5.0785153461812996e-05,
      "loss": 0.1441,
      "step": 4182
    },
    {
      "epoch": 2.2393574297188756,
      "grad_norm": 1.2986596822738647,
      "learning_rate": 5.0749464668094216e-05,
      "loss": 0.1005,
      "step": 4183
    },
    {
      "epoch": 2.239892904953146,
      "grad_norm": 0.7872549295425415,
      "learning_rate": 5.071377587437545e-05,
      "loss": 0.0486,
      "step": 4184
    },
    {
      "epoch": 2.2404283801874163,
      "grad_norm": 1.5050501823425293,
      "learning_rate": 5.0678087080656676e-05,
      "loss": 0.0501,
      "step": 4185
    },
    {
      "epoch": 2.2409638554216866,
      "grad_norm": 0.7430815100669861,
      "learning_rate": 5.064239828693791e-05,
      "loss": 0.0512,
      "step": 4186
    },
    {
      "epoch": 2.241499330655957,
      "grad_norm": 1.3782689571380615,
      "learning_rate": 5.060670949321913e-05,
      "loss": 0.1329,
      "step": 4187
    },
    {
      "epoch": 2.2420348058902277,
      "grad_norm": 0.9620135426521301,
      "learning_rate": 5.057102069950036e-05,
      "loss": 0.043,
      "step": 4188
    },
    {
      "epoch": 2.242570281124498,
      "grad_norm": 0.7516111731529236,
      "learning_rate": 5.053533190578159e-05,
      "loss": 0.0905,
      "step": 4189
    },
    {
      "epoch": 2.2431057563587684,
      "grad_norm": 2.589641571044922,
      "learning_rate": 5.049964311206281e-05,
      "loss": 0.2114,
      "step": 4190
    },
    {
      "epoch": 2.2436412315930387,
      "grad_norm": 1.1364755630493164,
      "learning_rate": 5.046395431834404e-05,
      "loss": 0.0609,
      "step": 4191
    },
    {
      "epoch": 2.244176706827309,
      "grad_norm": 1.905564785003662,
      "learning_rate": 5.0428265524625276e-05,
      "loss": 0.1136,
      "step": 4192
    },
    {
      "epoch": 2.24471218206158,
      "grad_norm": 0.7593364715576172,
      "learning_rate": 5.0392576730906496e-05,
      "loss": 0.0337,
      "step": 4193
    },
    {
      "epoch": 2.24524765729585,
      "grad_norm": 2.0214381217956543,
      "learning_rate": 5.035688793718772e-05,
      "loss": 0.1903,
      "step": 4194
    },
    {
      "epoch": 2.2457831325301205,
      "grad_norm": 1.7612046003341675,
      "learning_rate": 5.0321199143468956e-05,
      "loss": 0.0575,
      "step": 4195
    },
    {
      "epoch": 2.246318607764391,
      "grad_norm": 1.5388712882995605,
      "learning_rate": 5.0285510349750176e-05,
      "loss": 0.1137,
      "step": 4196
    },
    {
      "epoch": 2.246854082998661,
      "grad_norm": 0.49142539501190186,
      "learning_rate": 5.024982155603141e-05,
      "loss": 0.0203,
      "step": 4197
    },
    {
      "epoch": 2.247389558232932,
      "grad_norm": 1.027806043624878,
      "learning_rate": 5.0214132762312636e-05,
      "loss": 0.0614,
      "step": 4198
    },
    {
      "epoch": 2.2479250334672023,
      "grad_norm": 0.9278836250305176,
      "learning_rate": 5.017844396859387e-05,
      "loss": 0.0664,
      "step": 4199
    },
    {
      "epoch": 2.2484605087014726,
      "grad_norm": 0.3446641266345978,
      "learning_rate": 5.014275517487509e-05,
      "loss": 0.0162,
      "step": 4200
    },
    {
      "epoch": 2.248995983935743,
      "grad_norm": 0.4761987626552582,
      "learning_rate": 5.010706638115632e-05,
      "loss": 0.0211,
      "step": 4201
    },
    {
      "epoch": 2.2495314591700133,
      "grad_norm": 0.7275294065475464,
      "learning_rate": 5.007137758743755e-05,
      "loss": 0.0573,
      "step": 4202
    },
    {
      "epoch": 2.2500669344042836,
      "grad_norm": 0.6521182060241699,
      "learning_rate": 5.003568879371877e-05,
      "loss": 0.0377,
      "step": 4203
    },
    {
      "epoch": 2.2506024096385544,
      "grad_norm": 0.44117575883865356,
      "learning_rate": 5e-05,
      "loss": 0.0202,
      "step": 4204
    },
    {
      "epoch": 2.2511378848728247,
      "grad_norm": 0.6222022175788879,
      "learning_rate": 4.996431120628123e-05,
      "loss": 0.0459,
      "step": 4205
    },
    {
      "epoch": 2.251673360107095,
      "grad_norm": 1.5151597261428833,
      "learning_rate": 4.992862241256246e-05,
      "loss": 0.0921,
      "step": 4206
    },
    {
      "epoch": 2.2522088353413654,
      "grad_norm": 1.844172477722168,
      "learning_rate": 4.989293361884368e-05,
      "loss": 0.0815,
      "step": 4207
    },
    {
      "epoch": 2.2527443105756357,
      "grad_norm": 1.1238384246826172,
      "learning_rate": 4.9857244825124916e-05,
      "loss": 0.0739,
      "step": 4208
    },
    {
      "epoch": 2.253279785809906,
      "grad_norm": 1.1525843143463135,
      "learning_rate": 4.982155603140614e-05,
      "loss": 0.0681,
      "step": 4209
    },
    {
      "epoch": 2.253815261044177,
      "grad_norm": 1.106954574584961,
      "learning_rate": 4.978586723768737e-05,
      "loss": 0.067,
      "step": 4210
    },
    {
      "epoch": 2.254350736278447,
      "grad_norm": 1.0086816549301147,
      "learning_rate": 4.9750178443968596e-05,
      "loss": 0.072,
      "step": 4211
    },
    {
      "epoch": 2.2548862115127175,
      "grad_norm": 0.5634700059890747,
      "learning_rate": 4.971448965024982e-05,
      "loss": 0.0308,
      "step": 4212
    },
    {
      "epoch": 2.255421686746988,
      "grad_norm": 1.0907248258590698,
      "learning_rate": 4.967880085653105e-05,
      "loss": 0.0917,
      "step": 4213
    },
    {
      "epoch": 2.255957161981258,
      "grad_norm": 0.7918105721473694,
      "learning_rate": 4.964311206281228e-05,
      "loss": 0.0545,
      "step": 4214
    },
    {
      "epoch": 2.256492637215529,
      "grad_norm": 1.6517215967178345,
      "learning_rate": 4.960742326909351e-05,
      "loss": 0.0748,
      "step": 4215
    },
    {
      "epoch": 2.2570281124497993,
      "grad_norm": 1.2610132694244385,
      "learning_rate": 4.9571734475374736e-05,
      "loss": 0.0732,
      "step": 4216
    },
    {
      "epoch": 2.2575635876840696,
      "grad_norm": 0.889764130115509,
      "learning_rate": 4.953604568165596e-05,
      "loss": 0.0519,
      "step": 4217
    },
    {
      "epoch": 2.25809906291834,
      "grad_norm": 1.4224697351455688,
      "learning_rate": 4.950035688793719e-05,
      "loss": 0.1115,
      "step": 4218
    },
    {
      "epoch": 2.2586345381526103,
      "grad_norm": 0.6059874892234802,
      "learning_rate": 4.946466809421842e-05,
      "loss": 0.057,
      "step": 4219
    },
    {
      "epoch": 2.259170013386881,
      "grad_norm": 1.4059722423553467,
      "learning_rate": 4.942897930049964e-05,
      "loss": 0.0799,
      "step": 4220
    },
    {
      "epoch": 2.2597054886211514,
      "grad_norm": 0.9780729413032532,
      "learning_rate": 4.939329050678087e-05,
      "loss": 0.1048,
      "step": 4221
    },
    {
      "epoch": 2.2602409638554217,
      "grad_norm": 0.906152606010437,
      "learning_rate": 4.93576017130621e-05,
      "loss": 0.0869,
      "step": 4222
    },
    {
      "epoch": 2.260776439089692,
      "grad_norm": 0.31474411487579346,
      "learning_rate": 4.932191291934333e-05,
      "loss": 0.0143,
      "step": 4223
    },
    {
      "epoch": 2.2613119143239624,
      "grad_norm": 0.8978193402290344,
      "learning_rate": 4.9286224125624556e-05,
      "loss": 0.0881,
      "step": 4224
    },
    {
      "epoch": 2.261847389558233,
      "grad_norm": 1.0628517866134644,
      "learning_rate": 4.925053533190578e-05,
      "loss": 0.0998,
      "step": 4225
    },
    {
      "epoch": 2.2623828647925035,
      "grad_norm": 0.13391530513763428,
      "learning_rate": 4.921484653818701e-05,
      "loss": 0.0034,
      "step": 4226
    },
    {
      "epoch": 2.262918340026774,
      "grad_norm": 1.8387806415557861,
      "learning_rate": 4.917915774446824e-05,
      "loss": 0.0498,
      "step": 4227
    },
    {
      "epoch": 2.263453815261044,
      "grad_norm": 2.148801803588867,
      "learning_rate": 4.914346895074947e-05,
      "loss": 0.1313,
      "step": 4228
    },
    {
      "epoch": 2.2639892904953145,
      "grad_norm": 0.9757756590843201,
      "learning_rate": 4.9107780157030696e-05,
      "loss": 0.0677,
      "step": 4229
    },
    {
      "epoch": 2.264524765729585,
      "grad_norm": 1.9876025915145874,
      "learning_rate": 4.907209136331192e-05,
      "loss": 0.0407,
      "step": 4230
    },
    {
      "epoch": 2.2650602409638556,
      "grad_norm": 1.3090579509735107,
      "learning_rate": 4.903640256959315e-05,
      "loss": 0.0642,
      "step": 4231
    },
    {
      "epoch": 2.265595716198126,
      "grad_norm": 1.0006084442138672,
      "learning_rate": 4.900071377587438e-05,
      "loss": 0.0582,
      "step": 4232
    },
    {
      "epoch": 2.2661311914323963,
      "grad_norm": 1.377852201461792,
      "learning_rate": 4.89650249821556e-05,
      "loss": 0.1247,
      "step": 4233
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 1.157476782798767,
      "learning_rate": 4.892933618843683e-05,
      "loss": 0.076,
      "step": 4234
    },
    {
      "epoch": 2.267202141900937,
      "grad_norm": 1.190184235572815,
      "learning_rate": 4.889364739471806e-05,
      "loss": 0.0574,
      "step": 4235
    },
    {
      "epoch": 2.2677376171352073,
      "grad_norm": 0.9046838283538818,
      "learning_rate": 4.885795860099929e-05,
      "loss": 0.0726,
      "step": 4236
    },
    {
      "epoch": 2.268273092369478,
      "grad_norm": 0.7636958360671997,
      "learning_rate": 4.8822269807280516e-05,
      "loss": 0.0459,
      "step": 4237
    },
    {
      "epoch": 2.2688085676037484,
      "grad_norm": 1.0343750715255737,
      "learning_rate": 4.878658101356174e-05,
      "loss": 0.0961,
      "step": 4238
    },
    {
      "epoch": 2.2693440428380187,
      "grad_norm": 1.0355929136276245,
      "learning_rate": 4.875089221984297e-05,
      "loss": 0.0619,
      "step": 4239
    },
    {
      "epoch": 2.269879518072289,
      "grad_norm": 1.0088127851486206,
      "learning_rate": 4.87152034261242e-05,
      "loss": 0.0491,
      "step": 4240
    },
    {
      "epoch": 2.2704149933065594,
      "grad_norm": 0.7850217819213867,
      "learning_rate": 4.867951463240543e-05,
      "loss": 0.0662,
      "step": 4241
    },
    {
      "epoch": 2.27095046854083,
      "grad_norm": 3.040558099746704,
      "learning_rate": 4.8643825838686656e-05,
      "loss": 0.0799,
      "step": 4242
    },
    {
      "epoch": 2.2714859437751005,
      "grad_norm": 2.0841686725616455,
      "learning_rate": 4.860813704496788e-05,
      "loss": 0.1349,
      "step": 4243
    },
    {
      "epoch": 2.272021419009371,
      "grad_norm": 1.0353962182998657,
      "learning_rate": 4.857244825124911e-05,
      "loss": 0.0466,
      "step": 4244
    },
    {
      "epoch": 2.272556894243641,
      "grad_norm": 1.4583905935287476,
      "learning_rate": 4.853675945753034e-05,
      "loss": 0.066,
      "step": 4245
    },
    {
      "epoch": 2.2730923694779115,
      "grad_norm": 1.566703200340271,
      "learning_rate": 4.850107066381156e-05,
      "loss": 0.0798,
      "step": 4246
    },
    {
      "epoch": 2.2736278447121823,
      "grad_norm": 1.0457957983016968,
      "learning_rate": 4.846538187009279e-05,
      "loss": 0.0804,
      "step": 4247
    },
    {
      "epoch": 2.2741633199464526,
      "grad_norm": 0.9595592617988586,
      "learning_rate": 4.842969307637402e-05,
      "loss": 0.0921,
      "step": 4248
    },
    {
      "epoch": 2.274698795180723,
      "grad_norm": 0.4336073398590088,
      "learning_rate": 4.839400428265525e-05,
      "loss": 0.0181,
      "step": 4249
    },
    {
      "epoch": 2.2752342704149933,
      "grad_norm": 1.138526439666748,
      "learning_rate": 4.8358315488936476e-05,
      "loss": 0.0726,
      "step": 4250
    },
    {
      "epoch": 2.2757697456492636,
      "grad_norm": 0.7662678360939026,
      "learning_rate": 4.83226266952177e-05,
      "loss": 0.0529,
      "step": 4251
    },
    {
      "epoch": 2.2763052208835344,
      "grad_norm": 1.1058785915374756,
      "learning_rate": 4.828693790149893e-05,
      "loss": 0.0693,
      "step": 4252
    },
    {
      "epoch": 2.2768406961178047,
      "grad_norm": 1.5555789470672607,
      "learning_rate": 4.825124910778016e-05,
      "loss": 0.0901,
      "step": 4253
    },
    {
      "epoch": 2.277376171352075,
      "grad_norm": 0.34070247411727905,
      "learning_rate": 4.821556031406139e-05,
      "loss": 0.0138,
      "step": 4254
    },
    {
      "epoch": 2.2779116465863454,
      "grad_norm": 1.0083215236663818,
      "learning_rate": 4.817987152034261e-05,
      "loss": 0.0834,
      "step": 4255
    },
    {
      "epoch": 2.2784471218206157,
      "grad_norm": 1.9947932958602905,
      "learning_rate": 4.814418272662384e-05,
      "loss": 0.0459,
      "step": 4256
    },
    {
      "epoch": 2.278982597054886,
      "grad_norm": 1.157557487487793,
      "learning_rate": 4.810849393290507e-05,
      "loss": 0.0918,
      "step": 4257
    },
    {
      "epoch": 2.279518072289157,
      "grad_norm": 1.9812071323394775,
      "learning_rate": 4.80728051391863e-05,
      "loss": 0.1215,
      "step": 4258
    },
    {
      "epoch": 2.280053547523427,
      "grad_norm": 1.0009654760360718,
      "learning_rate": 4.803711634546752e-05,
      "loss": 0.0468,
      "step": 4259
    },
    {
      "epoch": 2.2805890227576975,
      "grad_norm": 1.1698997020721436,
      "learning_rate": 4.800142755174875e-05,
      "loss": 0.0998,
      "step": 4260
    },
    {
      "epoch": 2.281124497991968,
      "grad_norm": 1.9199869632720947,
      "learning_rate": 4.796573875802998e-05,
      "loss": 0.0838,
      "step": 4261
    },
    {
      "epoch": 2.281659973226238,
      "grad_norm": 0.9430962800979614,
      "learning_rate": 4.793004996431121e-05,
      "loss": 0.0664,
      "step": 4262
    },
    {
      "epoch": 2.2821954484605085,
      "grad_norm": 1.0034520626068115,
      "learning_rate": 4.7894361170592436e-05,
      "loss": 0.0717,
      "step": 4263
    },
    {
      "epoch": 2.2827309236947793,
      "grad_norm": 1.3807741403579712,
      "learning_rate": 4.785867237687366e-05,
      "loss": 0.0368,
      "step": 4264
    },
    {
      "epoch": 2.2832663989290496,
      "grad_norm": 0.8050850033760071,
      "learning_rate": 4.782298358315489e-05,
      "loss": 0.0498,
      "step": 4265
    },
    {
      "epoch": 2.28380187416332,
      "grad_norm": 0.9847256541252136,
      "learning_rate": 4.778729478943612e-05,
      "loss": 0.0536,
      "step": 4266
    },
    {
      "epoch": 2.2843373493975903,
      "grad_norm": 0.563567578792572,
      "learning_rate": 4.775160599571735e-05,
      "loss": 0.0409,
      "step": 4267
    },
    {
      "epoch": 2.2848728246318606,
      "grad_norm": 1.1805503368377686,
      "learning_rate": 4.7715917201998576e-05,
      "loss": 0.0787,
      "step": 4268
    },
    {
      "epoch": 2.2854082998661314,
      "grad_norm": 2.4931318759918213,
      "learning_rate": 4.76802284082798e-05,
      "loss": 0.1943,
      "step": 4269
    },
    {
      "epoch": 2.2859437751004017,
      "grad_norm": 1.1181187629699707,
      "learning_rate": 4.764453961456103e-05,
      "loss": 0.0916,
      "step": 4270
    },
    {
      "epoch": 2.286479250334672,
      "grad_norm": 1.1723484992980957,
      "learning_rate": 4.760885082084226e-05,
      "loss": 0.0629,
      "step": 4271
    },
    {
      "epoch": 2.2870147255689424,
      "grad_norm": 1.1284388303756714,
      "learning_rate": 4.757316202712349e-05,
      "loss": 0.0723,
      "step": 4272
    },
    {
      "epoch": 2.2875502008032127,
      "grad_norm": 2.1317930221557617,
      "learning_rate": 4.753747323340471e-05,
      "loss": 0.1466,
      "step": 4273
    },
    {
      "epoch": 2.2880856760374835,
      "grad_norm": 0.8459895253181458,
      "learning_rate": 4.750178443968594e-05,
      "loss": 0.0821,
      "step": 4274
    },
    {
      "epoch": 2.288621151271754,
      "grad_norm": 1.9668262004852295,
      "learning_rate": 4.746609564596717e-05,
      "loss": 0.1535,
      "step": 4275
    },
    {
      "epoch": 2.289156626506024,
      "grad_norm": 0.6417326331138611,
      "learning_rate": 4.7430406852248396e-05,
      "loss": 0.049,
      "step": 4276
    },
    {
      "epoch": 2.2896921017402945,
      "grad_norm": 1.2222791910171509,
      "learning_rate": 4.739471805852962e-05,
      "loss": 0.084,
      "step": 4277
    },
    {
      "epoch": 2.290227576974565,
      "grad_norm": 0.6385748982429504,
      "learning_rate": 4.735902926481085e-05,
      "loss": 0.0298,
      "step": 4278
    },
    {
      "epoch": 2.2907630522088356,
      "grad_norm": 0.1774555742740631,
      "learning_rate": 4.732334047109208e-05,
      "loss": 0.0076,
      "step": 4279
    },
    {
      "epoch": 2.291298527443106,
      "grad_norm": 0.4257577359676361,
      "learning_rate": 4.728765167737331e-05,
      "loss": 0.0167,
      "step": 4280
    },
    {
      "epoch": 2.2918340026773762,
      "grad_norm": 0.507465124130249,
      "learning_rate": 4.7251962883654536e-05,
      "loss": 0.0172,
      "step": 4281
    },
    {
      "epoch": 2.2923694779116466,
      "grad_norm": 0.4913232922554016,
      "learning_rate": 4.721627408993576e-05,
      "loss": 0.0161,
      "step": 4282
    },
    {
      "epoch": 2.292904953145917,
      "grad_norm": 0.6741111874580383,
      "learning_rate": 4.718058529621699e-05,
      "loss": 0.0544,
      "step": 4283
    },
    {
      "epoch": 2.2934404283801872,
      "grad_norm": 1.4958655834197998,
      "learning_rate": 4.714489650249822e-05,
      "loss": 0.14,
      "step": 4284
    },
    {
      "epoch": 2.293975903614458,
      "grad_norm": 2.1142654418945312,
      "learning_rate": 4.710920770877945e-05,
      "loss": 0.1146,
      "step": 4285
    },
    {
      "epoch": 2.2945113788487284,
      "grad_norm": 2.2056045532226562,
      "learning_rate": 4.707351891506067e-05,
      "loss": 0.175,
      "step": 4286
    },
    {
      "epoch": 2.2950468540829987,
      "grad_norm": 1.429634690284729,
      "learning_rate": 4.70378301213419e-05,
      "loss": 0.0693,
      "step": 4287
    },
    {
      "epoch": 2.295582329317269,
      "grad_norm": 1.3641629219055176,
      "learning_rate": 4.700214132762313e-05,
      "loss": 0.0958,
      "step": 4288
    },
    {
      "epoch": 2.2961178045515394,
      "grad_norm": 0.36580097675323486,
      "learning_rate": 4.6966452533904356e-05,
      "loss": 0.021,
      "step": 4289
    },
    {
      "epoch": 2.2966532797858097,
      "grad_norm": 0.8135997653007507,
      "learning_rate": 4.693076374018558e-05,
      "loss": 0.1044,
      "step": 4290
    },
    {
      "epoch": 2.2971887550200805,
      "grad_norm": 0.09887564182281494,
      "learning_rate": 4.689507494646681e-05,
      "loss": 0.0045,
      "step": 4291
    },
    {
      "epoch": 2.297724230254351,
      "grad_norm": 1.1676850318908691,
      "learning_rate": 4.685938615274804e-05,
      "loss": 0.0821,
      "step": 4292
    },
    {
      "epoch": 2.298259705488621,
      "grad_norm": 1.7492499351501465,
      "learning_rate": 4.682369735902927e-05,
      "loss": 0.1602,
      "step": 4293
    },
    {
      "epoch": 2.2987951807228915,
      "grad_norm": 0.38471364974975586,
      "learning_rate": 4.6788008565310496e-05,
      "loss": 0.0277,
      "step": 4294
    },
    {
      "epoch": 2.299330655957162,
      "grad_norm": 1.0783183574676514,
      "learning_rate": 4.675231977159172e-05,
      "loss": 0.0843,
      "step": 4295
    },
    {
      "epoch": 2.2998661311914326,
      "grad_norm": 1.717414140701294,
      "learning_rate": 4.671663097787295e-05,
      "loss": 0.1011,
      "step": 4296
    },
    {
      "epoch": 2.300401606425703,
      "grad_norm": 1.576351284980774,
      "learning_rate": 4.668094218415418e-05,
      "loss": 0.0911,
      "step": 4297
    },
    {
      "epoch": 2.3009370816599732,
      "grad_norm": 0.9358554482460022,
      "learning_rate": 4.664525339043541e-05,
      "loss": 0.0763,
      "step": 4298
    },
    {
      "epoch": 2.3014725568942436,
      "grad_norm": 0.884720504283905,
      "learning_rate": 4.660956459671663e-05,
      "loss": 0.0445,
      "step": 4299
    },
    {
      "epoch": 2.302008032128514,
      "grad_norm": 0.7172556519508362,
      "learning_rate": 4.657387580299786e-05,
      "loss": 0.1018,
      "step": 4300
    },
    {
      "epoch": 2.3025435073627847,
      "grad_norm": 1.575035572052002,
      "learning_rate": 4.653818700927909e-05,
      "loss": 0.1101,
      "step": 4301
    },
    {
      "epoch": 2.303078982597055,
      "grad_norm": 0.2722739279270172,
      "learning_rate": 4.6502498215560316e-05,
      "loss": 0.0062,
      "step": 4302
    },
    {
      "epoch": 2.3036144578313253,
      "grad_norm": 0.3386930525302887,
      "learning_rate": 4.646680942184154e-05,
      "loss": 0.0121,
      "step": 4303
    },
    {
      "epoch": 2.3041499330655957,
      "grad_norm": 0.9875988364219666,
      "learning_rate": 4.643112062812277e-05,
      "loss": 0.0704,
      "step": 4304
    },
    {
      "epoch": 2.304685408299866,
      "grad_norm": 1.017213225364685,
      "learning_rate": 4.6395431834404e-05,
      "loss": 0.0704,
      "step": 4305
    },
    {
      "epoch": 2.305220883534137,
      "grad_norm": 2.558408260345459,
      "learning_rate": 4.635974304068523e-05,
      "loss": 0.1658,
      "step": 4306
    },
    {
      "epoch": 2.305756358768407,
      "grad_norm": 0.9762187004089355,
      "learning_rate": 4.6324054246966456e-05,
      "loss": 0.0659,
      "step": 4307
    },
    {
      "epoch": 2.3062918340026775,
      "grad_norm": 0.9351352453231812,
      "learning_rate": 4.628836545324768e-05,
      "loss": 0.0852,
      "step": 4308
    },
    {
      "epoch": 2.306827309236948,
      "grad_norm": 1.9239394664764404,
      "learning_rate": 4.625267665952891e-05,
      "loss": 0.1208,
      "step": 4309
    },
    {
      "epoch": 2.307362784471218,
      "grad_norm": 1.3239729404449463,
      "learning_rate": 4.6216987865810136e-05,
      "loss": 0.1037,
      "step": 4310
    },
    {
      "epoch": 2.3078982597054885,
      "grad_norm": 0.7937015295028687,
      "learning_rate": 4.618129907209137e-05,
      "loss": 0.0474,
      "step": 4311
    },
    {
      "epoch": 2.3084337349397592,
      "grad_norm": 0.8345341086387634,
      "learning_rate": 4.614561027837259e-05,
      "loss": 0.0758,
      "step": 4312
    },
    {
      "epoch": 2.3089692101740296,
      "grad_norm": 1.4709739685058594,
      "learning_rate": 4.610992148465382e-05,
      "loss": 0.117,
      "step": 4313
    },
    {
      "epoch": 2.3095046854083,
      "grad_norm": 0.788277268409729,
      "learning_rate": 4.607423269093505e-05,
      "loss": 0.0652,
      "step": 4314
    },
    {
      "epoch": 2.3100401606425702,
      "grad_norm": 1.1081864833831787,
      "learning_rate": 4.6038543897216276e-05,
      "loss": 0.1015,
      "step": 4315
    },
    {
      "epoch": 2.3105756358768406,
      "grad_norm": 9.914817810058594,
      "learning_rate": 4.60028551034975e-05,
      "loss": 0.185,
      "step": 4316
    },
    {
      "epoch": 2.311111111111111,
      "grad_norm": 0.9804317355155945,
      "learning_rate": 4.596716630977873e-05,
      "loss": 0.0554,
      "step": 4317
    },
    {
      "epoch": 2.3116465863453817,
      "grad_norm": 0.3014855682849884,
      "learning_rate": 4.593147751605996e-05,
      "loss": 0.0222,
      "step": 4318
    },
    {
      "epoch": 2.312182061579652,
      "grad_norm": 1.203279733657837,
      "learning_rate": 4.589578872234119e-05,
      "loss": 0.0686,
      "step": 4319
    },
    {
      "epoch": 2.3127175368139223,
      "grad_norm": 0.981194794178009,
      "learning_rate": 4.5860099928622416e-05,
      "loss": 0.0654,
      "step": 4320
    },
    {
      "epoch": 2.3132530120481927,
      "grad_norm": 2.21225643157959,
      "learning_rate": 4.582441113490364e-05,
      "loss": 0.0992,
      "step": 4321
    },
    {
      "epoch": 2.313788487282463,
      "grad_norm": 1.2686195373535156,
      "learning_rate": 4.578872234118487e-05,
      "loss": 0.0734,
      "step": 4322
    },
    {
      "epoch": 2.314323962516734,
      "grad_norm": 1.0416978597640991,
      "learning_rate": 4.5753033547466095e-05,
      "loss": 0.0766,
      "step": 4323
    },
    {
      "epoch": 2.314859437751004,
      "grad_norm": 0.5196871161460876,
      "learning_rate": 4.571734475374733e-05,
      "loss": 0.0204,
      "step": 4324
    },
    {
      "epoch": 2.3153949129852744,
      "grad_norm": 0.7066500782966614,
      "learning_rate": 4.568165596002855e-05,
      "loss": 0.0449,
      "step": 4325
    },
    {
      "epoch": 2.3159303882195448,
      "grad_norm": 1.3884316682815552,
      "learning_rate": 4.564596716630978e-05,
      "loss": 0.1345,
      "step": 4326
    },
    {
      "epoch": 2.316465863453815,
      "grad_norm": 2.16837477684021,
      "learning_rate": 4.561027837259101e-05,
      "loss": 0.1372,
      "step": 4327
    },
    {
      "epoch": 2.317001338688086,
      "grad_norm": 0.6262298822402954,
      "learning_rate": 4.5574589578872236e-05,
      "loss": 0.0579,
      "step": 4328
    },
    {
      "epoch": 2.317536813922356,
      "grad_norm": 1.6942551136016846,
      "learning_rate": 4.553890078515346e-05,
      "loss": 0.0783,
      "step": 4329
    },
    {
      "epoch": 2.3180722891566266,
      "grad_norm": 0.7038317918777466,
      "learning_rate": 4.550321199143469e-05,
      "loss": 0.0259,
      "step": 4330
    },
    {
      "epoch": 2.318607764390897,
      "grad_norm": 1.153893232345581,
      "learning_rate": 4.5467523197715915e-05,
      "loss": 0.0855,
      "step": 4331
    },
    {
      "epoch": 2.319143239625167,
      "grad_norm": 1.1807129383087158,
      "learning_rate": 4.543183440399715e-05,
      "loss": 0.0632,
      "step": 4332
    },
    {
      "epoch": 2.319678714859438,
      "grad_norm": 1.2596887350082397,
      "learning_rate": 4.5396145610278376e-05,
      "loss": 0.0682,
      "step": 4333
    },
    {
      "epoch": 2.3202141900937083,
      "grad_norm": 0.9897552728652954,
      "learning_rate": 4.53604568165596e-05,
      "loss": 0.0502,
      "step": 4334
    },
    {
      "epoch": 2.3207496653279787,
      "grad_norm": 0.2837589383125305,
      "learning_rate": 4.532476802284083e-05,
      "loss": 0.0164,
      "step": 4335
    },
    {
      "epoch": 2.321285140562249,
      "grad_norm": 2.9011168479919434,
      "learning_rate": 4.5289079229122055e-05,
      "loss": 0.109,
      "step": 4336
    },
    {
      "epoch": 2.3218206157965193,
      "grad_norm": 1.1426892280578613,
      "learning_rate": 4.525339043540329e-05,
      "loss": 0.1064,
      "step": 4337
    },
    {
      "epoch": 2.3223560910307897,
      "grad_norm": 1.54742431640625,
      "learning_rate": 4.5217701641684516e-05,
      "loss": 0.0779,
      "step": 4338
    },
    {
      "epoch": 2.32289156626506,
      "grad_norm": 0.31351807713508606,
      "learning_rate": 4.518201284796574e-05,
      "loss": 0.0098,
      "step": 4339
    },
    {
      "epoch": 2.3234270414993308,
      "grad_norm": 0.3746998608112335,
      "learning_rate": 4.514632405424697e-05,
      "loss": 0.0135,
      "step": 4340
    },
    {
      "epoch": 2.323962516733601,
      "grad_norm": 2.2045350074768066,
      "learning_rate": 4.5110635260528195e-05,
      "loss": 0.1354,
      "step": 4341
    },
    {
      "epoch": 2.3244979919678714,
      "grad_norm": 0.5595311522483826,
      "learning_rate": 4.507494646680943e-05,
      "loss": 0.037,
      "step": 4342
    },
    {
      "epoch": 2.3250334672021418,
      "grad_norm": 0.5455226302146912,
      "learning_rate": 4.503925767309065e-05,
      "loss": 0.0678,
      "step": 4343
    },
    {
      "epoch": 2.325568942436412,
      "grad_norm": 1.2140779495239258,
      "learning_rate": 4.5003568879371875e-05,
      "loss": 0.0582,
      "step": 4344
    },
    {
      "epoch": 2.326104417670683,
      "grad_norm": 1.112343192100525,
      "learning_rate": 4.496788008565311e-05,
      "loss": 0.0804,
      "step": 4345
    },
    {
      "epoch": 2.326639892904953,
      "grad_norm": 1.452864646911621,
      "learning_rate": 4.4932191291934335e-05,
      "loss": 0.0441,
      "step": 4346
    },
    {
      "epoch": 2.3271753681392235,
      "grad_norm": 1.413892388343811,
      "learning_rate": 4.489650249821556e-05,
      "loss": 0.0786,
      "step": 4347
    },
    {
      "epoch": 2.327710843373494,
      "grad_norm": 0.5113681554794312,
      "learning_rate": 4.486081370449679e-05,
      "loss": 0.0271,
      "step": 4348
    },
    {
      "epoch": 2.328246318607764,
      "grad_norm": 0.8837268352508545,
      "learning_rate": 4.4825124910778015e-05,
      "loss": 0.056,
      "step": 4349
    },
    {
      "epoch": 2.328781793842035,
      "grad_norm": 1.6118595600128174,
      "learning_rate": 4.478943611705925e-05,
      "loss": 0.0411,
      "step": 4350
    },
    {
      "epoch": 2.3293172690763053,
      "grad_norm": 2.7759156227111816,
      "learning_rate": 4.4753747323340476e-05,
      "loss": 0.2031,
      "step": 4351
    },
    {
      "epoch": 2.3298527443105757,
      "grad_norm": 0.6461710929870605,
      "learning_rate": 4.47180585296217e-05,
      "loss": 0.0481,
      "step": 4352
    },
    {
      "epoch": 2.330388219544846,
      "grad_norm": 1.2483664751052856,
      "learning_rate": 4.468236973590293e-05,
      "loss": 0.1088,
      "step": 4353
    },
    {
      "epoch": 2.3309236947791163,
      "grad_norm": 1.074924349784851,
      "learning_rate": 4.4646680942184155e-05,
      "loss": 0.0984,
      "step": 4354
    },
    {
      "epoch": 2.331459170013387,
      "grad_norm": 0.7657923102378845,
      "learning_rate": 4.461099214846539e-05,
      "loss": 0.0593,
      "step": 4355
    },
    {
      "epoch": 2.3319946452476574,
      "grad_norm": 2.4672889709472656,
      "learning_rate": 4.457530335474661e-05,
      "loss": 0.1138,
      "step": 4356
    },
    {
      "epoch": 2.3325301204819278,
      "grad_norm": 1.347218632698059,
      "learning_rate": 4.4539614561027835e-05,
      "loss": 0.0723,
      "step": 4357
    },
    {
      "epoch": 2.333065595716198,
      "grad_norm": 0.9467947483062744,
      "learning_rate": 4.450392576730907e-05,
      "loss": 0.0769,
      "step": 4358
    },
    {
      "epoch": 2.3336010709504684,
      "grad_norm": 1.1597107648849487,
      "learning_rate": 4.4468236973590295e-05,
      "loss": 0.0619,
      "step": 4359
    },
    {
      "epoch": 2.334136546184739,
      "grad_norm": 1.5395163297653198,
      "learning_rate": 4.443254817987152e-05,
      "loss": 0.0631,
      "step": 4360
    },
    {
      "epoch": 2.3346720214190095,
      "grad_norm": 0.7421459555625916,
      "learning_rate": 4.439685938615275e-05,
      "loss": 0.0503,
      "step": 4361
    },
    {
      "epoch": 2.33520749665328,
      "grad_norm": 0.573696494102478,
      "learning_rate": 4.4361170592433975e-05,
      "loss": 0.0238,
      "step": 4362
    },
    {
      "epoch": 2.33574297188755,
      "grad_norm": 0.6523699760437012,
      "learning_rate": 4.432548179871521e-05,
      "loss": 0.0379,
      "step": 4363
    },
    {
      "epoch": 2.3362784471218205,
      "grad_norm": 1.2527446746826172,
      "learning_rate": 4.4289793004996435e-05,
      "loss": 0.0625,
      "step": 4364
    },
    {
      "epoch": 2.336813922356091,
      "grad_norm": 2.1552295684814453,
      "learning_rate": 4.4254104211277655e-05,
      "loss": 0.0648,
      "step": 4365
    },
    {
      "epoch": 2.337349397590361,
      "grad_norm": 1.1515096426010132,
      "learning_rate": 4.421841541755889e-05,
      "loss": 0.0702,
      "step": 4366
    },
    {
      "epoch": 2.337884872824632,
      "grad_norm": 1.0478683710098267,
      "learning_rate": 4.4182726623840115e-05,
      "loss": 0.0697,
      "step": 4367
    },
    {
      "epoch": 2.3384203480589023,
      "grad_norm": 0.4186534583568573,
      "learning_rate": 4.414703783012135e-05,
      "loss": 0.0346,
      "step": 4368
    },
    {
      "epoch": 2.3389558232931726,
      "grad_norm": 0.9674234390258789,
      "learning_rate": 4.411134903640257e-05,
      "loss": 0.0828,
      "step": 4369
    },
    {
      "epoch": 2.339491298527443,
      "grad_norm": 1.2566406726837158,
      "learning_rate": 4.4075660242683795e-05,
      "loss": 0.0973,
      "step": 4370
    },
    {
      "epoch": 2.3400267737617133,
      "grad_norm": 0.8781225085258484,
      "learning_rate": 4.403997144896503e-05,
      "loss": 0.0688,
      "step": 4371
    },
    {
      "epoch": 2.340562248995984,
      "grad_norm": 1.6588795185089111,
      "learning_rate": 4.4004282655246255e-05,
      "loss": 0.1013,
      "step": 4372
    },
    {
      "epoch": 2.3410977242302544,
      "grad_norm": 1.6039848327636719,
      "learning_rate": 4.396859386152748e-05,
      "loss": 0.1417,
      "step": 4373
    },
    {
      "epoch": 2.3416331994645248,
      "grad_norm": 1.3164069652557373,
      "learning_rate": 4.393290506780871e-05,
      "loss": 0.1249,
      "step": 4374
    },
    {
      "epoch": 2.342168674698795,
      "grad_norm": 0.90320885181427,
      "learning_rate": 4.3897216274089935e-05,
      "loss": 0.069,
      "step": 4375
    },
    {
      "epoch": 2.3427041499330654,
      "grad_norm": 1.1778422594070435,
      "learning_rate": 4.386152748037117e-05,
      "loss": 0.0698,
      "step": 4376
    },
    {
      "epoch": 2.343239625167336,
      "grad_norm": 0.8089308738708496,
      "learning_rate": 4.3825838686652395e-05,
      "loss": 0.0416,
      "step": 4377
    },
    {
      "epoch": 2.3437751004016065,
      "grad_norm": 0.5983816385269165,
      "learning_rate": 4.3790149892933615e-05,
      "loss": 0.0432,
      "step": 4378
    },
    {
      "epoch": 2.344310575635877,
      "grad_norm": 1.6521813869476318,
      "learning_rate": 4.375446109921485e-05,
      "loss": 0.0964,
      "step": 4379
    },
    {
      "epoch": 2.344846050870147,
      "grad_norm": 0.43825504183769226,
      "learning_rate": 4.3718772305496075e-05,
      "loss": 0.0187,
      "step": 4380
    },
    {
      "epoch": 2.3453815261044175,
      "grad_norm": 1.2073662281036377,
      "learning_rate": 4.368308351177731e-05,
      "loss": 0.0486,
      "step": 4381
    },
    {
      "epoch": 2.3459170013386883,
      "grad_norm": 1.365910530090332,
      "learning_rate": 4.364739471805853e-05,
      "loss": 0.0717,
      "step": 4382
    },
    {
      "epoch": 2.3464524765729586,
      "grad_norm": 0.9699170589447021,
      "learning_rate": 4.3611705924339755e-05,
      "loss": 0.055,
      "step": 4383
    },
    {
      "epoch": 2.346987951807229,
      "grad_norm": 0.7778698205947876,
      "learning_rate": 4.357601713062099e-05,
      "loss": 0.0639,
      "step": 4384
    },
    {
      "epoch": 2.3475234270414993,
      "grad_norm": 0.7168347239494324,
      "learning_rate": 4.3540328336902215e-05,
      "loss": 0.0519,
      "step": 4385
    },
    {
      "epoch": 2.3480589022757696,
      "grad_norm": 1.0190584659576416,
      "learning_rate": 4.350463954318344e-05,
      "loss": 0.0773,
      "step": 4386
    },
    {
      "epoch": 2.3485943775100404,
      "grad_norm": 2.006049156188965,
      "learning_rate": 4.346895074946467e-05,
      "loss": 0.0605,
      "step": 4387
    },
    {
      "epoch": 2.3491298527443107,
      "grad_norm": 1.104783296585083,
      "learning_rate": 4.3433261955745895e-05,
      "loss": 0.0905,
      "step": 4388
    },
    {
      "epoch": 2.349665327978581,
      "grad_norm": 1.468190312385559,
      "learning_rate": 4.339757316202713e-05,
      "loss": 0.1059,
      "step": 4389
    },
    {
      "epoch": 2.3502008032128514,
      "grad_norm": 1.7465779781341553,
      "learning_rate": 4.3361884368308355e-05,
      "loss": 0.0606,
      "step": 4390
    },
    {
      "epoch": 2.3507362784471217,
      "grad_norm": 2.6867141723632812,
      "learning_rate": 4.3326195574589575e-05,
      "loss": 0.1181,
      "step": 4391
    },
    {
      "epoch": 2.351271753681392,
      "grad_norm": 0.9141747951507568,
      "learning_rate": 4.329050678087081e-05,
      "loss": 0.0554,
      "step": 4392
    },
    {
      "epoch": 2.3518072289156624,
      "grad_norm": 0.7500624060630798,
      "learning_rate": 4.3254817987152035e-05,
      "loss": 0.0563,
      "step": 4393
    },
    {
      "epoch": 2.352342704149933,
      "grad_norm": 0.5560998916625977,
      "learning_rate": 4.321912919343327e-05,
      "loss": 0.0293,
      "step": 4394
    },
    {
      "epoch": 2.3528781793842035,
      "grad_norm": 0.8816217184066772,
      "learning_rate": 4.318344039971449e-05,
      "loss": 0.0304,
      "step": 4395
    },
    {
      "epoch": 2.353413654618474,
      "grad_norm": 1.544778823852539,
      "learning_rate": 4.3147751605995715e-05,
      "loss": 0.1231,
      "step": 4396
    },
    {
      "epoch": 2.353949129852744,
      "grad_norm": 0.5851615071296692,
      "learning_rate": 4.311206281227695e-05,
      "loss": 0.0477,
      "step": 4397
    },
    {
      "epoch": 2.3544846050870145,
      "grad_norm": 1.2133835554122925,
      "learning_rate": 4.3076374018558175e-05,
      "loss": 0.093,
      "step": 4398
    },
    {
      "epoch": 2.3550200803212853,
      "grad_norm": 1.5390310287475586,
      "learning_rate": 4.30406852248394e-05,
      "loss": 0.1183,
      "step": 4399
    },
    {
      "epoch": 2.3555555555555556,
      "grad_norm": 1.1214799880981445,
      "learning_rate": 4.300499643112063e-05,
      "loss": 0.0683,
      "step": 4400
    },
    {
      "epoch": 2.356091030789826,
      "grad_norm": 1.8103480339050293,
      "learning_rate": 4.2969307637401855e-05,
      "loss": 0.1065,
      "step": 4401
    },
    {
      "epoch": 2.3566265060240963,
      "grad_norm": 0.6533021330833435,
      "learning_rate": 4.293361884368309e-05,
      "loss": 0.0331,
      "step": 4402
    },
    {
      "epoch": 2.3571619812583666,
      "grad_norm": 1.6237879991531372,
      "learning_rate": 4.2897930049964315e-05,
      "loss": 0.0352,
      "step": 4403
    },
    {
      "epoch": 2.3576974564926374,
      "grad_norm": 1.8803046941757202,
      "learning_rate": 4.286224125624554e-05,
      "loss": 0.1782,
      "step": 4404
    },
    {
      "epoch": 2.3582329317269077,
      "grad_norm": 1.044211745262146,
      "learning_rate": 4.282655246252677e-05,
      "loss": 0.0478,
      "step": 4405
    },
    {
      "epoch": 2.358768406961178,
      "grad_norm": 1.2933601140975952,
      "learning_rate": 4.2790863668807995e-05,
      "loss": 0.0572,
      "step": 4406
    },
    {
      "epoch": 2.3593038821954484,
      "grad_norm": 1.1842528581619263,
      "learning_rate": 4.275517487508923e-05,
      "loss": 0.0387,
      "step": 4407
    },
    {
      "epoch": 2.3598393574297187,
      "grad_norm": 0.660713791847229,
      "learning_rate": 4.2719486081370455e-05,
      "loss": 0.0501,
      "step": 4408
    },
    {
      "epoch": 2.3603748326639895,
      "grad_norm": 1.6309070587158203,
      "learning_rate": 4.2683797287651675e-05,
      "loss": 0.1001,
      "step": 4409
    },
    {
      "epoch": 2.36091030789826,
      "grad_norm": 3.0724871158599854,
      "learning_rate": 4.264810849393291e-05,
      "loss": 0.0759,
      "step": 4410
    },
    {
      "epoch": 2.36144578313253,
      "grad_norm": 0.49957090616226196,
      "learning_rate": 4.2612419700214135e-05,
      "loss": 0.0365,
      "step": 4411
    },
    {
      "epoch": 2.3619812583668005,
      "grad_norm": 0.7486532330513,
      "learning_rate": 4.257673090649536e-05,
      "loss": 0.0475,
      "step": 4412
    },
    {
      "epoch": 2.362516733601071,
      "grad_norm": 1.271975040435791,
      "learning_rate": 4.254104211277659e-05,
      "loss": 0.0741,
      "step": 4413
    },
    {
      "epoch": 2.363052208835341,
      "grad_norm": 1.8287993669509888,
      "learning_rate": 4.2505353319057815e-05,
      "loss": 0.107,
      "step": 4414
    },
    {
      "epoch": 2.363587684069612,
      "grad_norm": 4.740725517272949,
      "learning_rate": 4.246966452533905e-05,
      "loss": 0.1062,
      "step": 4415
    },
    {
      "epoch": 2.3641231593038823,
      "grad_norm": 2.0920963287353516,
      "learning_rate": 4.2433975731620275e-05,
      "loss": 0.1047,
      "step": 4416
    },
    {
      "epoch": 2.3646586345381526,
      "grad_norm": 0.7860802412033081,
      "learning_rate": 4.23982869379015e-05,
      "loss": 0.0395,
      "step": 4417
    },
    {
      "epoch": 2.365194109772423,
      "grad_norm": 0.7823154926300049,
      "learning_rate": 4.236259814418273e-05,
      "loss": 0.0355,
      "step": 4418
    },
    {
      "epoch": 2.3657295850066933,
      "grad_norm": 0.3527619540691376,
      "learning_rate": 4.2326909350463955e-05,
      "loss": 0.0235,
      "step": 4419
    },
    {
      "epoch": 2.3662650602409636,
      "grad_norm": 0.8610466718673706,
      "learning_rate": 4.229122055674518e-05,
      "loss": 0.0738,
      "step": 4420
    },
    {
      "epoch": 2.3668005354752344,
      "grad_norm": 0.7850039005279541,
      "learning_rate": 4.2255531763026415e-05,
      "loss": 0.0407,
      "step": 4421
    },
    {
      "epoch": 2.3673360107095047,
      "grad_norm": 0.3283001780509949,
      "learning_rate": 4.2219842969307635e-05,
      "loss": 0.0186,
      "step": 4422
    },
    {
      "epoch": 2.367871485943775,
      "grad_norm": 1.7390813827514648,
      "learning_rate": 4.218415417558887e-05,
      "loss": 0.1427,
      "step": 4423
    },
    {
      "epoch": 2.3684069611780454,
      "grad_norm": 0.9024007320404053,
      "learning_rate": 4.2148465381870095e-05,
      "loss": 0.065,
      "step": 4424
    },
    {
      "epoch": 2.3689424364123157,
      "grad_norm": 0.775477409362793,
      "learning_rate": 4.211277658815132e-05,
      "loss": 0.0822,
      "step": 4425
    },
    {
      "epoch": 2.3694779116465865,
      "grad_norm": 1.0261409282684326,
      "learning_rate": 4.207708779443255e-05,
      "loss": 0.0368,
      "step": 4426
    },
    {
      "epoch": 2.370013386880857,
      "grad_norm": 1.321755290031433,
      "learning_rate": 4.2041399000713775e-05,
      "loss": 0.0464,
      "step": 4427
    },
    {
      "epoch": 2.370548862115127,
      "grad_norm": 1.052208662033081,
      "learning_rate": 4.200571020699501e-05,
      "loss": 0.0735,
      "step": 4428
    },
    {
      "epoch": 2.3710843373493975,
      "grad_norm": 0.3785110116004944,
      "learning_rate": 4.1970021413276235e-05,
      "loss": 0.0249,
      "step": 4429
    },
    {
      "epoch": 2.371619812583668,
      "grad_norm": 1.3455158472061157,
      "learning_rate": 4.193433261955746e-05,
      "loss": 0.0325,
      "step": 4430
    },
    {
      "epoch": 2.3721552878179386,
      "grad_norm": 0.8433789014816284,
      "learning_rate": 4.189864382583869e-05,
      "loss": 0.0643,
      "step": 4431
    },
    {
      "epoch": 2.372690763052209,
      "grad_norm": 0.7092887163162231,
      "learning_rate": 4.1862955032119915e-05,
      "loss": 0.0275,
      "step": 4432
    },
    {
      "epoch": 2.3732262382864793,
      "grad_norm": 1.1592282056808472,
      "learning_rate": 4.182726623840114e-05,
      "loss": 0.0674,
      "step": 4433
    },
    {
      "epoch": 2.3737617135207496,
      "grad_norm": 1.0562011003494263,
      "learning_rate": 4.1791577444682375e-05,
      "loss": 0.1116,
      "step": 4434
    },
    {
      "epoch": 2.37429718875502,
      "grad_norm": 0.7088577747344971,
      "learning_rate": 4.1755888650963595e-05,
      "loss": 0.0237,
      "step": 4435
    },
    {
      "epoch": 2.3748326639892907,
      "grad_norm": 0.5000436305999756,
      "learning_rate": 4.172019985724483e-05,
      "loss": 0.0409,
      "step": 4436
    },
    {
      "epoch": 2.375368139223561,
      "grad_norm": 0.874046266078949,
      "learning_rate": 4.1684511063526055e-05,
      "loss": 0.0676,
      "step": 4437
    },
    {
      "epoch": 2.3759036144578314,
      "grad_norm": 1.452953577041626,
      "learning_rate": 4.164882226980728e-05,
      "loss": 0.1401,
      "step": 4438
    },
    {
      "epoch": 2.3764390896921017,
      "grad_norm": 1.772024154663086,
      "learning_rate": 4.161313347608851e-05,
      "loss": 0.091,
      "step": 4439
    },
    {
      "epoch": 2.376974564926372,
      "grad_norm": 1.7355477809906006,
      "learning_rate": 4.1577444682369735e-05,
      "loss": 0.0871,
      "step": 4440
    },
    {
      "epoch": 2.3775100401606424,
      "grad_norm": 0.9231686592102051,
      "learning_rate": 4.154175588865097e-05,
      "loss": 0.1139,
      "step": 4441
    },
    {
      "epoch": 2.378045515394913,
      "grad_norm": 0.7696021795272827,
      "learning_rate": 4.1506067094932195e-05,
      "loss": 0.0355,
      "step": 4442
    },
    {
      "epoch": 2.3785809906291835,
      "grad_norm": 2.0667567253112793,
      "learning_rate": 4.147037830121342e-05,
      "loss": 0.1147,
      "step": 4443
    },
    {
      "epoch": 2.379116465863454,
      "grad_norm": 2.20329213142395,
      "learning_rate": 4.143468950749465e-05,
      "loss": 0.1761,
      "step": 4444
    },
    {
      "epoch": 2.379651941097724,
      "grad_norm": 1.535090446472168,
      "learning_rate": 4.1399000713775875e-05,
      "loss": 0.1006,
      "step": 4445
    },
    {
      "epoch": 2.3801874163319945,
      "grad_norm": 0.7188798785209656,
      "learning_rate": 4.13633119200571e-05,
      "loss": 0.0232,
      "step": 4446
    },
    {
      "epoch": 2.380722891566265,
      "grad_norm": 1.2866768836975098,
      "learning_rate": 4.1327623126338335e-05,
      "loss": 0.0558,
      "step": 4447
    },
    {
      "epoch": 2.3812583668005356,
      "grad_norm": 1.5279881954193115,
      "learning_rate": 4.1291934332619555e-05,
      "loss": 0.1167,
      "step": 4448
    },
    {
      "epoch": 2.381793842034806,
      "grad_norm": 1.7663904428482056,
      "learning_rate": 4.125624553890079e-05,
      "loss": 0.0876,
      "step": 4449
    },
    {
      "epoch": 2.3823293172690763,
      "grad_norm": 1.0284878015518188,
      "learning_rate": 4.1220556745182015e-05,
      "loss": 0.0741,
      "step": 4450
    },
    {
      "epoch": 2.3828647925033466,
      "grad_norm": 0.808506965637207,
      "learning_rate": 4.118486795146324e-05,
      "loss": 0.0471,
      "step": 4451
    },
    {
      "epoch": 2.383400267737617,
      "grad_norm": 0.8665151596069336,
      "learning_rate": 4.114917915774447e-05,
      "loss": 0.0571,
      "step": 4452
    },
    {
      "epoch": 2.3839357429718877,
      "grad_norm": 2.8886618614196777,
      "learning_rate": 4.1113490364025695e-05,
      "loss": 0.139,
      "step": 4453
    },
    {
      "epoch": 2.384471218206158,
      "grad_norm": 0.6946025490760803,
      "learning_rate": 4.107780157030692e-05,
      "loss": 0.0296,
      "step": 4454
    },
    {
      "epoch": 2.3850066934404284,
      "grad_norm": 0.7742209434509277,
      "learning_rate": 4.1042112776588155e-05,
      "loss": 0.0488,
      "step": 4455
    },
    {
      "epoch": 2.3855421686746987,
      "grad_norm": 0.4768788516521454,
      "learning_rate": 4.100642398286938e-05,
      "loss": 0.0302,
      "step": 4456
    },
    {
      "epoch": 2.386077643908969,
      "grad_norm": 0.9396818280220032,
      "learning_rate": 4.097073518915061e-05,
      "loss": 0.0319,
      "step": 4457
    },
    {
      "epoch": 2.38661311914324,
      "grad_norm": 3.955233097076416,
      "learning_rate": 4.0935046395431835e-05,
      "loss": 0.1418,
      "step": 4458
    },
    {
      "epoch": 2.38714859437751,
      "grad_norm": 1.0608912706375122,
      "learning_rate": 4.089935760171306e-05,
      "loss": 0.0714,
      "step": 4459
    },
    {
      "epoch": 2.3876840696117805,
      "grad_norm": 2.491300344467163,
      "learning_rate": 4.0863668807994295e-05,
      "loss": 0.1175,
      "step": 4460
    },
    {
      "epoch": 2.388219544846051,
      "grad_norm": 1.7065480947494507,
      "learning_rate": 4.082798001427552e-05,
      "loss": 0.1449,
      "step": 4461
    },
    {
      "epoch": 2.388755020080321,
      "grad_norm": 4.110750198364258,
      "learning_rate": 4.079229122055675e-05,
      "loss": 0.1908,
      "step": 4462
    },
    {
      "epoch": 2.389290495314592,
      "grad_norm": 1.3508490324020386,
      "learning_rate": 4.0756602426837975e-05,
      "loss": 0.0956,
      "step": 4463
    },
    {
      "epoch": 2.3898259705488623,
      "grad_norm": 0.8594192862510681,
      "learning_rate": 4.07209136331192e-05,
      "loss": 0.0486,
      "step": 4464
    },
    {
      "epoch": 2.3903614457831326,
      "grad_norm": 0.8586239814758301,
      "learning_rate": 4.0685224839400435e-05,
      "loss": 0.0588,
      "step": 4465
    },
    {
      "epoch": 2.390896921017403,
      "grad_norm": 1.594354271888733,
      "learning_rate": 4.0649536045681655e-05,
      "loss": 0.1079,
      "step": 4466
    },
    {
      "epoch": 2.3914323962516733,
      "grad_norm": 1.0921276807785034,
      "learning_rate": 4.061384725196288e-05,
      "loss": 0.0796,
      "step": 4467
    },
    {
      "epoch": 2.3919678714859436,
      "grad_norm": 0.6001104712486267,
      "learning_rate": 4.0578158458244115e-05,
      "loss": 0.0213,
      "step": 4468
    },
    {
      "epoch": 2.3925033467202144,
      "grad_norm": 1.2909003496170044,
      "learning_rate": 4.054246966452534e-05,
      "loss": 0.1093,
      "step": 4469
    },
    {
      "epoch": 2.3930388219544847,
      "grad_norm": 0.9008697867393494,
      "learning_rate": 4.050678087080657e-05,
      "loss": 0.0543,
      "step": 4470
    },
    {
      "epoch": 2.393574297188755,
      "grad_norm": 2.340402364730835,
      "learning_rate": 4.0471092077087795e-05,
      "loss": 0.1321,
      "step": 4471
    },
    {
      "epoch": 2.3941097724230254,
      "grad_norm": 0.14322154223918915,
      "learning_rate": 4.043540328336902e-05,
      "loss": 0.0044,
      "step": 4472
    },
    {
      "epoch": 2.3946452476572957,
      "grad_norm": 0.9347428679466248,
      "learning_rate": 4.0399714489650255e-05,
      "loss": 0.0711,
      "step": 4473
    },
    {
      "epoch": 2.395180722891566,
      "grad_norm": 0.6858873963356018,
      "learning_rate": 4.036402569593148e-05,
      "loss": 0.0358,
      "step": 4474
    },
    {
      "epoch": 2.395716198125837,
      "grad_norm": 1.0177991390228271,
      "learning_rate": 4.03283369022127e-05,
      "loss": 0.0761,
      "step": 4475
    },
    {
      "epoch": 2.396251673360107,
      "grad_norm": 1.5243650674819946,
      "learning_rate": 4.0292648108493935e-05,
      "loss": 0.0795,
      "step": 4476
    },
    {
      "epoch": 2.3967871485943775,
      "grad_norm": 0.5483337044715881,
      "learning_rate": 4.025695931477516e-05,
      "loss": 0.0369,
      "step": 4477
    },
    {
      "epoch": 2.397322623828648,
      "grad_norm": 1.749346375465393,
      "learning_rate": 4.0221270521056395e-05,
      "loss": 0.1354,
      "step": 4478
    },
    {
      "epoch": 2.397858099062918,
      "grad_norm": 1.6152379512786865,
      "learning_rate": 4.0185581727337615e-05,
      "loss": 0.0372,
      "step": 4479
    },
    {
      "epoch": 2.398393574297189,
      "grad_norm": 0.6798818707466125,
      "learning_rate": 4.014989293361884e-05,
      "loss": 0.052,
      "step": 4480
    },
    {
      "epoch": 2.3989290495314592,
      "grad_norm": 1.28585684299469,
      "learning_rate": 4.0114204139900075e-05,
      "loss": 0.1184,
      "step": 4481
    },
    {
      "epoch": 2.3994645247657296,
      "grad_norm": 0.318135142326355,
      "learning_rate": 4.00785153461813e-05,
      "loss": 0.0191,
      "step": 4482
    },
    {
      "epoch": 2.4,
      "grad_norm": 1.0021181106567383,
      "learning_rate": 4.004282655246253e-05,
      "loss": 0.0511,
      "step": 4483
    },
    {
      "epoch": 2.4005354752342702,
      "grad_norm": 1.6135830879211426,
      "learning_rate": 4.0007137758743755e-05,
      "loss": 0.0976,
      "step": 4484
    },
    {
      "epoch": 2.401070950468541,
      "grad_norm": 1.3772848844528198,
      "learning_rate": 3.997144896502498e-05,
      "loss": 0.1088,
      "step": 4485
    },
    {
      "epoch": 2.4016064257028114,
      "grad_norm": 1.4468719959259033,
      "learning_rate": 3.9935760171306215e-05,
      "loss": 0.1113,
      "step": 4486
    },
    {
      "epoch": 2.4021419009370817,
      "grad_norm": 1.2047781944274902,
      "learning_rate": 3.990007137758744e-05,
      "loss": 0.1022,
      "step": 4487
    },
    {
      "epoch": 2.402677376171352,
      "grad_norm": 0.4189986288547516,
      "learning_rate": 3.986438258386866e-05,
      "loss": 0.0241,
      "step": 4488
    },
    {
      "epoch": 2.4032128514056224,
      "grad_norm": 0.35091403126716614,
      "learning_rate": 3.9828693790149895e-05,
      "loss": 0.0214,
      "step": 4489
    },
    {
      "epoch": 2.403748326639893,
      "grad_norm": 0.8970671892166138,
      "learning_rate": 3.979300499643112e-05,
      "loss": 0.0539,
      "step": 4490
    },
    {
      "epoch": 2.4042838018741635,
      "grad_norm": 1.8215056657791138,
      "learning_rate": 3.9757316202712355e-05,
      "loss": 0.0702,
      "step": 4491
    },
    {
      "epoch": 2.404819277108434,
      "grad_norm": 2.0678870677948,
      "learning_rate": 3.9721627408993575e-05,
      "loss": 0.1361,
      "step": 4492
    },
    {
      "epoch": 2.405354752342704,
      "grad_norm": 0.7202406525611877,
      "learning_rate": 3.96859386152748e-05,
      "loss": 0.0313,
      "step": 4493
    },
    {
      "epoch": 2.4058902275769745,
      "grad_norm": 0.6549927592277527,
      "learning_rate": 3.9650249821556035e-05,
      "loss": 0.0255,
      "step": 4494
    },
    {
      "epoch": 2.406425702811245,
      "grad_norm": 1.0127782821655273,
      "learning_rate": 3.961456102783726e-05,
      "loss": 0.0661,
      "step": 4495
    },
    {
      "epoch": 2.4069611780455156,
      "grad_norm": 0.9253652691841125,
      "learning_rate": 3.957887223411849e-05,
      "loss": 0.0821,
      "step": 4496
    },
    {
      "epoch": 2.407496653279786,
      "grad_norm": 1.5159602165222168,
      "learning_rate": 3.9543183440399715e-05,
      "loss": 0.0948,
      "step": 4497
    },
    {
      "epoch": 2.4080321285140562,
      "grad_norm": 0.5330788493156433,
      "learning_rate": 3.950749464668094e-05,
      "loss": 0.0221,
      "step": 4498
    },
    {
      "epoch": 2.4085676037483266,
      "grad_norm": 0.5941320061683655,
      "learning_rate": 3.9471805852962175e-05,
      "loss": 0.0227,
      "step": 4499
    },
    {
      "epoch": 2.409103078982597,
      "grad_norm": 1.1520142555236816,
      "learning_rate": 3.94361170592434e-05,
      "loss": 0.0905,
      "step": 4500
    },
    {
      "epoch": 2.4096385542168672,
      "grad_norm": 1.7798789739608765,
      "learning_rate": 3.940042826552462e-05,
      "loss": 0.0329,
      "step": 4501
    },
    {
      "epoch": 2.410174029451138,
      "grad_norm": 2.041264295578003,
      "learning_rate": 3.9364739471805855e-05,
      "loss": 0.0912,
      "step": 4502
    },
    {
      "epoch": 2.4107095046854083,
      "grad_norm": 4.13461971282959,
      "learning_rate": 3.932905067808708e-05,
      "loss": 0.1101,
      "step": 4503
    },
    {
      "epoch": 2.4112449799196787,
      "grad_norm": 0.580599308013916,
      "learning_rate": 3.9293361884368315e-05,
      "loss": 0.0379,
      "step": 4504
    },
    {
      "epoch": 2.411780455153949,
      "grad_norm": 2.085578441619873,
      "learning_rate": 3.9257673090649535e-05,
      "loss": 0.1228,
      "step": 4505
    },
    {
      "epoch": 2.4123159303882193,
      "grad_norm": 1.9449615478515625,
      "learning_rate": 3.922198429693076e-05,
      "loss": 0.0862,
      "step": 4506
    },
    {
      "epoch": 2.41285140562249,
      "grad_norm": 0.8205088973045349,
      "learning_rate": 3.9186295503211995e-05,
      "loss": 0.0436,
      "step": 4507
    },
    {
      "epoch": 2.4133868808567605,
      "grad_norm": 0.6672709584236145,
      "learning_rate": 3.915060670949322e-05,
      "loss": 0.0339,
      "step": 4508
    },
    {
      "epoch": 2.413922356091031,
      "grad_norm": 1.1706998348236084,
      "learning_rate": 3.911491791577445e-05,
      "loss": 0.0681,
      "step": 4509
    },
    {
      "epoch": 2.414457831325301,
      "grad_norm": 1.0371607542037964,
      "learning_rate": 3.9079229122055675e-05,
      "loss": 0.0506,
      "step": 4510
    },
    {
      "epoch": 2.4149933065595715,
      "grad_norm": 1.260735034942627,
      "learning_rate": 3.90435403283369e-05,
      "loss": 0.1227,
      "step": 4511
    },
    {
      "epoch": 2.4155287817938422,
      "grad_norm": 0.6188567280769348,
      "learning_rate": 3.9007851534618135e-05,
      "loss": 0.0358,
      "step": 4512
    },
    {
      "epoch": 2.4160642570281126,
      "grad_norm": 0.896036684513092,
      "learning_rate": 3.897216274089936e-05,
      "loss": 0.0773,
      "step": 4513
    },
    {
      "epoch": 2.416599732262383,
      "grad_norm": 1.1331331729888916,
      "learning_rate": 3.893647394718058e-05,
      "loss": 0.0407,
      "step": 4514
    },
    {
      "epoch": 2.4171352074966532,
      "grad_norm": 0.7406383156776428,
      "learning_rate": 3.8900785153461815e-05,
      "loss": 0.0611,
      "step": 4515
    },
    {
      "epoch": 2.4176706827309236,
      "grad_norm": 1.1599533557891846,
      "learning_rate": 3.886509635974304e-05,
      "loss": 0.045,
      "step": 4516
    },
    {
      "epoch": 2.4182061579651943,
      "grad_norm": 1.1175589561462402,
      "learning_rate": 3.8829407566024275e-05,
      "loss": 0.0736,
      "step": 4517
    },
    {
      "epoch": 2.4187416331994647,
      "grad_norm": 1.6660069227218628,
      "learning_rate": 3.8793718772305495e-05,
      "loss": 0.0658,
      "step": 4518
    },
    {
      "epoch": 2.419277108433735,
      "grad_norm": 2.253945827484131,
      "learning_rate": 3.875802997858672e-05,
      "loss": 0.1359,
      "step": 4519
    },
    {
      "epoch": 2.4198125836680053,
      "grad_norm": 1.343790054321289,
      "learning_rate": 3.8722341184867955e-05,
      "loss": 0.051,
      "step": 4520
    },
    {
      "epoch": 2.4203480589022757,
      "grad_norm": 1.2463006973266602,
      "learning_rate": 3.868665239114918e-05,
      "loss": 0.0751,
      "step": 4521
    },
    {
      "epoch": 2.420883534136546,
      "grad_norm": 0.46283578872680664,
      "learning_rate": 3.865096359743041e-05,
      "loss": 0.0251,
      "step": 4522
    },
    {
      "epoch": 2.4214190093708168,
      "grad_norm": 0.69228196144104,
      "learning_rate": 3.8615274803711635e-05,
      "loss": 0.0337,
      "step": 4523
    },
    {
      "epoch": 2.421954484605087,
      "grad_norm": 0.844426155090332,
      "learning_rate": 3.857958600999286e-05,
      "loss": 0.079,
      "step": 4524
    },
    {
      "epoch": 2.4224899598393574,
      "grad_norm": 1.3435311317443848,
      "learning_rate": 3.8543897216274095e-05,
      "loss": 0.0634,
      "step": 4525
    },
    {
      "epoch": 2.4230254350736278,
      "grad_norm": 1.5463001728057861,
      "learning_rate": 3.850820842255532e-05,
      "loss": 0.0919,
      "step": 4526
    },
    {
      "epoch": 2.423560910307898,
      "grad_norm": 0.9270123243331909,
      "learning_rate": 3.847251962883655e-05,
      "loss": 0.0778,
      "step": 4527
    },
    {
      "epoch": 2.4240963855421684,
      "grad_norm": 2.251516580581665,
      "learning_rate": 3.8436830835117775e-05,
      "loss": 0.1199,
      "step": 4528
    },
    {
      "epoch": 2.424631860776439,
      "grad_norm": 1.657029390335083,
      "learning_rate": 3.8401142041399e-05,
      "loss": 0.1064,
      "step": 4529
    },
    {
      "epoch": 2.4251673360107096,
      "grad_norm": 1.6277562379837036,
      "learning_rate": 3.836545324768023e-05,
      "loss": 0.1109,
      "step": 4530
    },
    {
      "epoch": 2.42570281124498,
      "grad_norm": 3.100447177886963,
      "learning_rate": 3.832976445396146e-05,
      "loss": 0.1215,
      "step": 4531
    },
    {
      "epoch": 2.42623828647925,
      "grad_norm": 1.5020263195037842,
      "learning_rate": 3.829407566024268e-05,
      "loss": 0.0766,
      "step": 4532
    },
    {
      "epoch": 2.4267737617135205,
      "grad_norm": 0.9424285888671875,
      "learning_rate": 3.8258386866523915e-05,
      "loss": 0.03,
      "step": 4533
    },
    {
      "epoch": 2.4273092369477913,
      "grad_norm": 1.184093952178955,
      "learning_rate": 3.822269807280514e-05,
      "loss": 0.071,
      "step": 4534
    },
    {
      "epoch": 2.4278447121820617,
      "grad_norm": 1.3626558780670166,
      "learning_rate": 3.818700927908637e-05,
      "loss": 0.1286,
      "step": 4535
    },
    {
      "epoch": 2.428380187416332,
      "grad_norm": 0.5361173748970032,
      "learning_rate": 3.8151320485367595e-05,
      "loss": 0.0312,
      "step": 4536
    },
    {
      "epoch": 2.4289156626506023,
      "grad_norm": 1.8834105730056763,
      "learning_rate": 3.811563169164882e-05,
      "loss": 0.0857,
      "step": 4537
    },
    {
      "epoch": 2.4294511378848727,
      "grad_norm": 2.339810371398926,
      "learning_rate": 3.8079942897930055e-05,
      "loss": 0.1906,
      "step": 4538
    },
    {
      "epoch": 2.4299866131191434,
      "grad_norm": 0.4692094027996063,
      "learning_rate": 3.804425410421128e-05,
      "loss": 0.0224,
      "step": 4539
    },
    {
      "epoch": 2.4305220883534138,
      "grad_norm": 1.0828543901443481,
      "learning_rate": 3.800856531049251e-05,
      "loss": 0.1127,
      "step": 4540
    },
    {
      "epoch": 2.431057563587684,
      "grad_norm": 1.3125196695327759,
      "learning_rate": 3.7972876516773735e-05,
      "loss": 0.1148,
      "step": 4541
    },
    {
      "epoch": 2.4315930388219544,
      "grad_norm": 0.559504508972168,
      "learning_rate": 3.793718772305496e-05,
      "loss": 0.0182,
      "step": 4542
    },
    {
      "epoch": 2.4321285140562248,
      "grad_norm": 0.6466535329818726,
      "learning_rate": 3.790149892933619e-05,
      "loss": 0.0427,
      "step": 4543
    },
    {
      "epoch": 2.4326639892904955,
      "grad_norm": 1.290334701538086,
      "learning_rate": 3.786581013561742e-05,
      "loss": 0.0737,
      "step": 4544
    },
    {
      "epoch": 2.433199464524766,
      "grad_norm": 0.8964279890060425,
      "learning_rate": 3.783012134189864e-05,
      "loss": 0.1012,
      "step": 4545
    },
    {
      "epoch": 2.433734939759036,
      "grad_norm": 1.2084007263183594,
      "learning_rate": 3.7794432548179875e-05,
      "loss": 0.0817,
      "step": 4546
    },
    {
      "epoch": 2.4342704149933065,
      "grad_norm": 1.8439092636108398,
      "learning_rate": 3.77587437544611e-05,
      "loss": 0.1429,
      "step": 4547
    },
    {
      "epoch": 2.434805890227577,
      "grad_norm": 0.7707993984222412,
      "learning_rate": 3.772305496074233e-05,
      "loss": 0.036,
      "step": 4548
    },
    {
      "epoch": 2.435341365461847,
      "grad_norm": 1.7219246625900269,
      "learning_rate": 3.7687366167023555e-05,
      "loss": 0.1206,
      "step": 4549
    },
    {
      "epoch": 2.435876840696118,
      "grad_norm": 1.406866192817688,
      "learning_rate": 3.765167737330478e-05,
      "loss": 0.0633,
      "step": 4550
    },
    {
      "epoch": 2.4364123159303883,
      "grad_norm": 0.6390540599822998,
      "learning_rate": 3.7615988579586015e-05,
      "loss": 0.0553,
      "step": 4551
    },
    {
      "epoch": 2.4369477911646586,
      "grad_norm": 0.3437088131904602,
      "learning_rate": 3.758029978586724e-05,
      "loss": 0.0216,
      "step": 4552
    },
    {
      "epoch": 2.437483266398929,
      "grad_norm": 1.0762033462524414,
      "learning_rate": 3.754461099214847e-05,
      "loss": 0.0733,
      "step": 4553
    },
    {
      "epoch": 2.4380187416331993,
      "grad_norm": 1.0123164653778076,
      "learning_rate": 3.7508922198429695e-05,
      "loss": 0.0728,
      "step": 4554
    },
    {
      "epoch": 2.4385542168674696,
      "grad_norm": 1.2073249816894531,
      "learning_rate": 3.747323340471092e-05,
      "loss": 0.0747,
      "step": 4555
    },
    {
      "epoch": 2.4390896921017404,
      "grad_norm": 0.7927762866020203,
      "learning_rate": 3.743754461099215e-05,
      "loss": 0.0818,
      "step": 4556
    },
    {
      "epoch": 2.4396251673360108,
      "grad_norm": 0.8046789765357971,
      "learning_rate": 3.740185581727338e-05,
      "loss": 0.0856,
      "step": 4557
    },
    {
      "epoch": 2.440160642570281,
      "grad_norm": 0.9610404968261719,
      "learning_rate": 3.73661670235546e-05,
      "loss": 0.0745,
      "step": 4558
    },
    {
      "epoch": 2.4406961178045514,
      "grad_norm": 0.6474609375,
      "learning_rate": 3.7330478229835835e-05,
      "loss": 0.0476,
      "step": 4559
    },
    {
      "epoch": 2.4412315930388218,
      "grad_norm": 0.6214297413825989,
      "learning_rate": 3.729478943611706e-05,
      "loss": 0.0492,
      "step": 4560
    },
    {
      "epoch": 2.4417670682730925,
      "grad_norm": 1.039721131324768,
      "learning_rate": 3.725910064239829e-05,
      "loss": 0.0712,
      "step": 4561
    },
    {
      "epoch": 2.442302543507363,
      "grad_norm": 1.5713690519332886,
      "learning_rate": 3.7223411848679515e-05,
      "loss": 0.1147,
      "step": 4562
    },
    {
      "epoch": 2.442838018741633,
      "grad_norm": 15.74020767211914,
      "learning_rate": 3.718772305496074e-05,
      "loss": 0.167,
      "step": 4563
    },
    {
      "epoch": 2.4433734939759035,
      "grad_norm": 0.4896690845489502,
      "learning_rate": 3.715203426124197e-05,
      "loss": 0.0391,
      "step": 4564
    },
    {
      "epoch": 2.443908969210174,
      "grad_norm": 2.1783018112182617,
      "learning_rate": 3.71163454675232e-05,
      "loss": 0.1144,
      "step": 4565
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 1.4033406972885132,
      "learning_rate": 3.708065667380443e-05,
      "loss": 0.0591,
      "step": 4566
    },
    {
      "epoch": 2.444979919678715,
      "grad_norm": 1.256461262702942,
      "learning_rate": 3.7044967880085655e-05,
      "loss": 0.0946,
      "step": 4567
    },
    {
      "epoch": 2.4455153949129853,
      "grad_norm": 1.796812653541565,
      "learning_rate": 3.700927908636688e-05,
      "loss": 0.1283,
      "step": 4568
    },
    {
      "epoch": 2.4460508701472556,
      "grad_norm": 0.6502424478530884,
      "learning_rate": 3.697359029264811e-05,
      "loss": 0.0464,
      "step": 4569
    },
    {
      "epoch": 2.446586345381526,
      "grad_norm": 1.2510918378829956,
      "learning_rate": 3.693790149892934e-05,
      "loss": 0.1406,
      "step": 4570
    },
    {
      "epoch": 2.4471218206157968,
      "grad_norm": 1.6473374366760254,
      "learning_rate": 3.690221270521056e-05,
      "loss": 0.0983,
      "step": 4571
    },
    {
      "epoch": 2.447657295850067,
      "grad_norm": 1.3523393869400024,
      "learning_rate": 3.6866523911491795e-05,
      "loss": 0.137,
      "step": 4572
    },
    {
      "epoch": 2.4481927710843374,
      "grad_norm": 0.39480558037757874,
      "learning_rate": 3.683083511777302e-05,
      "loss": 0.0184,
      "step": 4573
    },
    {
      "epoch": 2.4487282463186077,
      "grad_norm": 0.7340441942214966,
      "learning_rate": 3.679514632405425e-05,
      "loss": 0.0502,
      "step": 4574
    },
    {
      "epoch": 2.449263721552878,
      "grad_norm": 1.0837929248809814,
      "learning_rate": 3.6759457530335475e-05,
      "loss": 0.0779,
      "step": 4575
    },
    {
      "epoch": 2.4497991967871484,
      "grad_norm": 0.9686737060546875,
      "learning_rate": 3.67237687366167e-05,
      "loss": 0.064,
      "step": 4576
    },
    {
      "epoch": 2.450334672021419,
      "grad_norm": 1.13930344581604,
      "learning_rate": 3.668807994289793e-05,
      "loss": 0.0933,
      "step": 4577
    },
    {
      "epoch": 2.4508701472556895,
      "grad_norm": 0.7598196268081665,
      "learning_rate": 3.665239114917916e-05,
      "loss": 0.0421,
      "step": 4578
    },
    {
      "epoch": 2.45140562248996,
      "grad_norm": 1.2160536050796509,
      "learning_rate": 3.661670235546039e-05,
      "loss": 0.0818,
      "step": 4579
    },
    {
      "epoch": 2.45194109772423,
      "grad_norm": 0.8306953310966492,
      "learning_rate": 3.6581013561741615e-05,
      "loss": 0.0982,
      "step": 4580
    },
    {
      "epoch": 2.4524765729585005,
      "grad_norm": 1.3075276613235474,
      "learning_rate": 3.654532476802284e-05,
      "loss": 0.0844,
      "step": 4581
    },
    {
      "epoch": 2.453012048192771,
      "grad_norm": 0.9569877982139587,
      "learning_rate": 3.650963597430407e-05,
      "loss": 0.0725,
      "step": 4582
    },
    {
      "epoch": 2.4535475234270416,
      "grad_norm": 0.5155856609344482,
      "learning_rate": 3.64739471805853e-05,
      "loss": 0.029,
      "step": 4583
    },
    {
      "epoch": 2.454082998661312,
      "grad_norm": 1.3927242755889893,
      "learning_rate": 3.643825838686652e-05,
      "loss": 0.0939,
      "step": 4584
    },
    {
      "epoch": 2.4546184738955823,
      "grad_norm": 1.4033766984939575,
      "learning_rate": 3.640256959314775e-05,
      "loss": 0.0832,
      "step": 4585
    },
    {
      "epoch": 2.4551539491298526,
      "grad_norm": 1.2286856174468994,
      "learning_rate": 3.636688079942898e-05,
      "loss": 0.1074,
      "step": 4586
    },
    {
      "epoch": 2.455689424364123,
      "grad_norm": 0.5784425735473633,
      "learning_rate": 3.633119200571021e-05,
      "loss": 0.0362,
      "step": 4587
    },
    {
      "epoch": 2.4562248995983937,
      "grad_norm": 1.1854972839355469,
      "learning_rate": 3.6295503211991435e-05,
      "loss": 0.0733,
      "step": 4588
    },
    {
      "epoch": 2.456760374832664,
      "grad_norm": 0.7423561215400696,
      "learning_rate": 3.625981441827266e-05,
      "loss": 0.0239,
      "step": 4589
    },
    {
      "epoch": 2.4572958500669344,
      "grad_norm": 2.0756404399871826,
      "learning_rate": 3.622412562455389e-05,
      "loss": 0.1548,
      "step": 4590
    },
    {
      "epoch": 2.4578313253012047,
      "grad_norm": 1.7802581787109375,
      "learning_rate": 3.618843683083512e-05,
      "loss": 0.1038,
      "step": 4591
    },
    {
      "epoch": 2.458366800535475,
      "grad_norm": 0.9242827296257019,
      "learning_rate": 3.615274803711635e-05,
      "loss": 0.066,
      "step": 4592
    },
    {
      "epoch": 2.458902275769746,
      "grad_norm": 1.8028819561004639,
      "learning_rate": 3.6117059243397575e-05,
      "loss": 0.0848,
      "step": 4593
    },
    {
      "epoch": 2.459437751004016,
      "grad_norm": 1.9018661975860596,
      "learning_rate": 3.60813704496788e-05,
      "loss": 0.1182,
      "step": 4594
    },
    {
      "epoch": 2.4599732262382865,
      "grad_norm": 1.974532127380371,
      "learning_rate": 3.604568165596003e-05,
      "loss": 0.0982,
      "step": 4595
    },
    {
      "epoch": 2.460508701472557,
      "grad_norm": 1.083452820777893,
      "learning_rate": 3.600999286224126e-05,
      "loss": 0.0582,
      "step": 4596
    },
    {
      "epoch": 2.461044176706827,
      "grad_norm": 1.237973928451538,
      "learning_rate": 3.597430406852249e-05,
      "loss": 0.0545,
      "step": 4597
    },
    {
      "epoch": 2.461579651941098,
      "grad_norm": 0.6947273015975952,
      "learning_rate": 3.593861527480371e-05,
      "loss": 0.0489,
      "step": 4598
    },
    {
      "epoch": 2.4621151271753683,
      "grad_norm": 0.5823507905006409,
      "learning_rate": 3.590292648108494e-05,
      "loss": 0.0427,
      "step": 4599
    },
    {
      "epoch": 2.4626506024096386,
      "grad_norm": 0.6995070576667786,
      "learning_rate": 3.586723768736617e-05,
      "loss": 0.052,
      "step": 4600
    },
    {
      "epoch": 2.463186077643909,
      "grad_norm": 1.979681134223938,
      "learning_rate": 3.58315488936474e-05,
      "loss": 0.0803,
      "step": 4601
    },
    {
      "epoch": 2.4637215528781793,
      "grad_norm": 1.665179967880249,
      "learning_rate": 3.579586009992862e-05,
      "loss": 0.0904,
      "step": 4602
    },
    {
      "epoch": 2.4642570281124496,
      "grad_norm": 0.9046363234519958,
      "learning_rate": 3.576017130620985e-05,
      "loss": 0.0433,
      "step": 4603
    },
    {
      "epoch": 2.4647925033467204,
      "grad_norm": 0.8909470438957214,
      "learning_rate": 3.572448251249108e-05,
      "loss": 0.0442,
      "step": 4604
    },
    {
      "epoch": 2.4653279785809907,
      "grad_norm": 1.0825425386428833,
      "learning_rate": 3.568879371877231e-05,
      "loss": 0.1113,
      "step": 4605
    },
    {
      "epoch": 2.465863453815261,
      "grad_norm": 1.0803711414337158,
      "learning_rate": 3.5653104925053535e-05,
      "loss": 0.0796,
      "step": 4606
    },
    {
      "epoch": 2.4663989290495314,
      "grad_norm": 1.182069182395935,
      "learning_rate": 3.561741613133476e-05,
      "loss": 0.0775,
      "step": 4607
    },
    {
      "epoch": 2.4669344042838017,
      "grad_norm": 1.43057119846344,
      "learning_rate": 3.558172733761599e-05,
      "loss": 0.0937,
      "step": 4608
    },
    {
      "epoch": 2.467469879518072,
      "grad_norm": 0.7146713137626648,
      "learning_rate": 3.554603854389722e-05,
      "loss": 0.0585,
      "step": 4609
    },
    {
      "epoch": 2.468005354752343,
      "grad_norm": 1.0283052921295166,
      "learning_rate": 3.551034975017845e-05,
      "loss": 0.0451,
      "step": 4610
    },
    {
      "epoch": 2.468540829986613,
      "grad_norm": 0.6795549392700195,
      "learning_rate": 3.547466095645967e-05,
      "loss": 0.0338,
      "step": 4611
    },
    {
      "epoch": 2.4690763052208835,
      "grad_norm": 2.1562414169311523,
      "learning_rate": 3.54389721627409e-05,
      "loss": 0.1554,
      "step": 4612
    },
    {
      "epoch": 2.469611780455154,
      "grad_norm": 0.9041019082069397,
      "learning_rate": 3.540328336902213e-05,
      "loss": 0.0509,
      "step": 4613
    },
    {
      "epoch": 2.470147255689424,
      "grad_norm": 1.1800636053085327,
      "learning_rate": 3.536759457530336e-05,
      "loss": 0.0573,
      "step": 4614
    },
    {
      "epoch": 2.470682730923695,
      "grad_norm": 1.8656961917877197,
      "learning_rate": 3.533190578158458e-05,
      "loss": 0.0817,
      "step": 4615
    },
    {
      "epoch": 2.4712182061579653,
      "grad_norm": 2.98330020904541,
      "learning_rate": 3.529621698786581e-05,
      "loss": 0.1028,
      "step": 4616
    },
    {
      "epoch": 2.4717536813922356,
      "grad_norm": 1.5338389873504639,
      "learning_rate": 3.526052819414704e-05,
      "loss": 0.1473,
      "step": 4617
    },
    {
      "epoch": 2.472289156626506,
      "grad_norm": 1.4037609100341797,
      "learning_rate": 3.522483940042827e-05,
      "loss": 0.0703,
      "step": 4618
    },
    {
      "epoch": 2.4728246318607763,
      "grad_norm": 1.2998368740081787,
      "learning_rate": 3.5189150606709495e-05,
      "loss": 0.0917,
      "step": 4619
    },
    {
      "epoch": 2.473360107095047,
      "grad_norm": 0.8864850401878357,
      "learning_rate": 3.515346181299072e-05,
      "loss": 0.0622,
      "step": 4620
    },
    {
      "epoch": 2.4738955823293174,
      "grad_norm": 1.7746381759643555,
      "learning_rate": 3.511777301927195e-05,
      "loss": 0.0545,
      "step": 4621
    },
    {
      "epoch": 2.4744310575635877,
      "grad_norm": 1.075032114982605,
      "learning_rate": 3.508208422555318e-05,
      "loss": 0.0471,
      "step": 4622
    },
    {
      "epoch": 2.474966532797858,
      "grad_norm": 2.938894748687744,
      "learning_rate": 3.504639543183441e-05,
      "loss": 0.1855,
      "step": 4623
    },
    {
      "epoch": 2.4755020080321284,
      "grad_norm": 1.0467162132263184,
      "learning_rate": 3.501070663811563e-05,
      "loss": 0.0613,
      "step": 4624
    },
    {
      "epoch": 2.476037483266399,
      "grad_norm": 1.0674508810043335,
      "learning_rate": 3.497501784439686e-05,
      "loss": 0.0649,
      "step": 4625
    },
    {
      "epoch": 2.4765729585006695,
      "grad_norm": 0.8865566849708557,
      "learning_rate": 3.493932905067809e-05,
      "loss": 0.0793,
      "step": 4626
    },
    {
      "epoch": 2.47710843373494,
      "grad_norm": 0.3521879315376282,
      "learning_rate": 3.490364025695932e-05,
      "loss": 0.0165,
      "step": 4627
    },
    {
      "epoch": 2.47764390896921,
      "grad_norm": 1.079675316810608,
      "learning_rate": 3.486795146324054e-05,
      "loss": 0.0634,
      "step": 4628
    },
    {
      "epoch": 2.4781793842034805,
      "grad_norm": 0.6533011198043823,
      "learning_rate": 3.483226266952177e-05,
      "loss": 0.0333,
      "step": 4629
    },
    {
      "epoch": 2.478714859437751,
      "grad_norm": 1.538340449333191,
      "learning_rate": 3.4796573875803e-05,
      "loss": 0.131,
      "step": 4630
    },
    {
      "epoch": 2.4792503346720216,
      "grad_norm": 1.829179286956787,
      "learning_rate": 3.476088508208423e-05,
      "loss": 0.1174,
      "step": 4631
    },
    {
      "epoch": 2.479785809906292,
      "grad_norm": 2.0590033531188965,
      "learning_rate": 3.4725196288365455e-05,
      "loss": 0.1108,
      "step": 4632
    },
    {
      "epoch": 2.4803212851405623,
      "grad_norm": 0.37232568860054016,
      "learning_rate": 3.468950749464668e-05,
      "loss": 0.0186,
      "step": 4633
    },
    {
      "epoch": 2.4808567603748326,
      "grad_norm": 0.7399341464042664,
      "learning_rate": 3.465381870092791e-05,
      "loss": 0.0622,
      "step": 4634
    },
    {
      "epoch": 2.481392235609103,
      "grad_norm": 1.832080364227295,
      "learning_rate": 3.461812990720914e-05,
      "loss": 0.0862,
      "step": 4635
    },
    {
      "epoch": 2.4819277108433733,
      "grad_norm": 0.6532886624336243,
      "learning_rate": 3.458244111349037e-05,
      "loss": 0.0285,
      "step": 4636
    },
    {
      "epoch": 2.482463186077644,
      "grad_norm": 1.4458558559417725,
      "learning_rate": 3.454675231977159e-05,
      "loss": 0.0924,
      "step": 4637
    },
    {
      "epoch": 2.4829986613119144,
      "grad_norm": 1.05648672580719,
      "learning_rate": 3.451106352605282e-05,
      "loss": 0.0425,
      "step": 4638
    },
    {
      "epoch": 2.4835341365461847,
      "grad_norm": 1.1314606666564941,
      "learning_rate": 3.447537473233405e-05,
      "loss": 0.0685,
      "step": 4639
    },
    {
      "epoch": 2.484069611780455,
      "grad_norm": 0.5362935066223145,
      "learning_rate": 3.4439685938615275e-05,
      "loss": 0.0336,
      "step": 4640
    },
    {
      "epoch": 2.4846050870147254,
      "grad_norm": 1.3746106624603271,
      "learning_rate": 3.44039971448965e-05,
      "loss": 0.1008,
      "step": 4641
    },
    {
      "epoch": 2.485140562248996,
      "grad_norm": 1.1887507438659668,
      "learning_rate": 3.436830835117773e-05,
      "loss": 0.0412,
      "step": 4642
    },
    {
      "epoch": 2.4856760374832665,
      "grad_norm": 1.14847731590271,
      "learning_rate": 3.433261955745896e-05,
      "loss": 0.0729,
      "step": 4643
    },
    {
      "epoch": 2.486211512717537,
      "grad_norm": 0.5914244651794434,
      "learning_rate": 3.429693076374019e-05,
      "loss": 0.0679,
      "step": 4644
    },
    {
      "epoch": 2.486746987951807,
      "grad_norm": 0.9020718932151794,
      "learning_rate": 3.4261241970021415e-05,
      "loss": 0.0296,
      "step": 4645
    },
    {
      "epoch": 2.4872824631860775,
      "grad_norm": 0.9125601649284363,
      "learning_rate": 3.422555317630264e-05,
      "loss": 0.0388,
      "step": 4646
    },
    {
      "epoch": 2.4878179384203483,
      "grad_norm": 1.5486598014831543,
      "learning_rate": 3.418986438258387e-05,
      "loss": 0.0782,
      "step": 4647
    },
    {
      "epoch": 2.4883534136546186,
      "grad_norm": 0.6213856339454651,
      "learning_rate": 3.41541755888651e-05,
      "loss": 0.0549,
      "step": 4648
    },
    {
      "epoch": 2.488888888888889,
      "grad_norm": 0.7356289029121399,
      "learning_rate": 3.411848679514633e-05,
      "loss": 0.0463,
      "step": 4649
    },
    {
      "epoch": 2.4894243641231593,
      "grad_norm": 1.7574207782745361,
      "learning_rate": 3.4082798001427555e-05,
      "loss": 0.1374,
      "step": 4650
    },
    {
      "epoch": 2.4899598393574296,
      "grad_norm": 1.7794708013534546,
      "learning_rate": 3.404710920770878e-05,
      "loss": 0.1075,
      "step": 4651
    },
    {
      "epoch": 2.4904953145917004,
      "grad_norm": 0.3224259912967682,
      "learning_rate": 3.401142041399001e-05,
      "loss": 0.0201,
      "step": 4652
    },
    {
      "epoch": 2.4910307898259707,
      "grad_norm": 1.3791611194610596,
      "learning_rate": 3.3975731620271235e-05,
      "loss": 0.0631,
      "step": 4653
    },
    {
      "epoch": 2.491566265060241,
      "grad_norm": 0.8462325930595398,
      "learning_rate": 3.394004282655247e-05,
      "loss": 0.071,
      "step": 4654
    },
    {
      "epoch": 2.4921017402945114,
      "grad_norm": 1.110115885734558,
      "learning_rate": 3.390435403283369e-05,
      "loss": 0.0654,
      "step": 4655
    },
    {
      "epoch": 2.4926372155287817,
      "grad_norm": 1.3566454648971558,
      "learning_rate": 3.386866523911492e-05,
      "loss": 0.0266,
      "step": 4656
    },
    {
      "epoch": 2.493172690763052,
      "grad_norm": 1.7842319011688232,
      "learning_rate": 3.383297644539615e-05,
      "loss": 0.1317,
      "step": 4657
    },
    {
      "epoch": 2.493708165997323,
      "grad_norm": 0.7675771117210388,
      "learning_rate": 3.3797287651677375e-05,
      "loss": 0.0547,
      "step": 4658
    },
    {
      "epoch": 2.494243641231593,
      "grad_norm": 0.5803483724594116,
      "learning_rate": 3.37615988579586e-05,
      "loss": 0.0405,
      "step": 4659
    },
    {
      "epoch": 2.4947791164658635,
      "grad_norm": 2.5435214042663574,
      "learning_rate": 3.372591006423983e-05,
      "loss": 0.0378,
      "step": 4660
    },
    {
      "epoch": 2.495314591700134,
      "grad_norm": 2.3388121128082275,
      "learning_rate": 3.369022127052106e-05,
      "loss": 0.1708,
      "step": 4661
    },
    {
      "epoch": 2.495850066934404,
      "grad_norm": 0.6611866354942322,
      "learning_rate": 3.365453247680229e-05,
      "loss": 0.0305,
      "step": 4662
    },
    {
      "epoch": 2.4963855421686745,
      "grad_norm": 0.928955614566803,
      "learning_rate": 3.3618843683083515e-05,
      "loss": 0.0522,
      "step": 4663
    },
    {
      "epoch": 2.4969210174029453,
      "grad_norm": 2.1807103157043457,
      "learning_rate": 3.358315488936474e-05,
      "loss": 0.0606,
      "step": 4664
    },
    {
      "epoch": 2.4974564926372156,
      "grad_norm": 2.188499927520752,
      "learning_rate": 3.354746609564597e-05,
      "loss": 0.0592,
      "step": 4665
    },
    {
      "epoch": 2.497991967871486,
      "grad_norm": 2.5925018787384033,
      "learning_rate": 3.3511777301927195e-05,
      "loss": 0.1666,
      "step": 4666
    },
    {
      "epoch": 2.4985274431057563,
      "grad_norm": 1.4818891286849976,
      "learning_rate": 3.347608850820843e-05,
      "loss": 0.0474,
      "step": 4667
    },
    {
      "epoch": 2.4990629183400266,
      "grad_norm": 1.6775215864181519,
      "learning_rate": 3.344039971448965e-05,
      "loss": 0.0544,
      "step": 4668
    },
    {
      "epoch": 2.4995983935742974,
      "grad_norm": 1.743878960609436,
      "learning_rate": 3.340471092077088e-05,
      "loss": 0.1319,
      "step": 4669
    },
    {
      "epoch": 2.5001338688085677,
      "grad_norm": 0.773081362247467,
      "learning_rate": 3.336902212705211e-05,
      "loss": 0.0393,
      "step": 4670
    },
    {
      "epoch": 2.500669344042838,
      "grad_norm": 3.0749900341033936,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.0825,
      "step": 4671
    },
    {
      "epoch": 2.5012048192771084,
      "grad_norm": 1.0884578227996826,
      "learning_rate": 3.329764453961456e-05,
      "loss": 0.0595,
      "step": 4672
    },
    {
      "epoch": 2.5017402945113787,
      "grad_norm": 1.1862674951553345,
      "learning_rate": 3.326195574589579e-05,
      "loss": 0.068,
      "step": 4673
    },
    {
      "epoch": 2.5022757697456495,
      "grad_norm": 1.0103380680084229,
      "learning_rate": 3.3226266952177015e-05,
      "loss": 0.0647,
      "step": 4674
    },
    {
      "epoch": 2.50281124497992,
      "grad_norm": 0.9869787096977234,
      "learning_rate": 3.319057815845825e-05,
      "loss": 0.0596,
      "step": 4675
    },
    {
      "epoch": 2.50334672021419,
      "grad_norm": 1.8357818126678467,
      "learning_rate": 3.3154889364739475e-05,
      "loss": 0.0923,
      "step": 4676
    },
    {
      "epoch": 2.5038821954484605,
      "grad_norm": 1.4791386127471924,
      "learning_rate": 3.31192005710207e-05,
      "loss": 0.0457,
      "step": 4677
    },
    {
      "epoch": 2.504417670682731,
      "grad_norm": 1.2935987710952759,
      "learning_rate": 3.308351177730193e-05,
      "loss": 0.0724,
      "step": 4678
    },
    {
      "epoch": 2.5049531459170016,
      "grad_norm": 0.9690198302268982,
      "learning_rate": 3.3047822983583155e-05,
      "loss": 0.053,
      "step": 4679
    },
    {
      "epoch": 2.505488621151272,
      "grad_norm": 1.9504765272140503,
      "learning_rate": 3.301213418986439e-05,
      "loss": 0.0822,
      "step": 4680
    },
    {
      "epoch": 2.5060240963855422,
      "grad_norm": 3.761850595474243,
      "learning_rate": 3.297644539614561e-05,
      "loss": 0.0392,
      "step": 4681
    },
    {
      "epoch": 2.5065595716198126,
      "grad_norm": 1.5372415781021118,
      "learning_rate": 3.294075660242684e-05,
      "loss": 0.1206,
      "step": 4682
    },
    {
      "epoch": 2.507095046854083,
      "grad_norm": 1.7387006282806396,
      "learning_rate": 3.290506780870807e-05,
      "loss": 0.0512,
      "step": 4683
    },
    {
      "epoch": 2.5076305220883537,
      "grad_norm": 0.9308338165283203,
      "learning_rate": 3.2869379014989295e-05,
      "loss": 0.0578,
      "step": 4684
    },
    {
      "epoch": 2.5081659973226236,
      "grad_norm": 1.1419175863265991,
      "learning_rate": 3.283369022127052e-05,
      "loss": 0.0473,
      "step": 4685
    },
    {
      "epoch": 2.5087014725568944,
      "grad_norm": 0.6330565810203552,
      "learning_rate": 3.279800142755175e-05,
      "loss": 0.0391,
      "step": 4686
    },
    {
      "epoch": 2.5092369477911647,
      "grad_norm": 0.7188778519630432,
      "learning_rate": 3.2762312633832975e-05,
      "loss": 0.0506,
      "step": 4687
    },
    {
      "epoch": 2.509772423025435,
      "grad_norm": 0.8968990445137024,
      "learning_rate": 3.272662384011421e-05,
      "loss": 0.0351,
      "step": 4688
    },
    {
      "epoch": 2.5103078982597054,
      "grad_norm": 1.1565989255905151,
      "learning_rate": 3.2690935046395435e-05,
      "loss": 0.0451,
      "step": 4689
    },
    {
      "epoch": 2.5108433734939757,
      "grad_norm": 3.1471669673919678,
      "learning_rate": 3.265524625267666e-05,
      "loss": 0.0734,
      "step": 4690
    },
    {
      "epoch": 2.5113788487282465,
      "grad_norm": 0.6688090562820435,
      "learning_rate": 3.261955745895789e-05,
      "loss": 0.0407,
      "step": 4691
    },
    {
      "epoch": 2.511914323962517,
      "grad_norm": 1.220098614692688,
      "learning_rate": 3.2583868665239115e-05,
      "loss": 0.0537,
      "step": 4692
    },
    {
      "epoch": 2.512449799196787,
      "grad_norm": 2.45915150642395,
      "learning_rate": 3.254817987152035e-05,
      "loss": 0.1287,
      "step": 4693
    },
    {
      "epoch": 2.5129852744310575,
      "grad_norm": 1.1892917156219482,
      "learning_rate": 3.251249107780157e-05,
      "loss": 0.0495,
      "step": 4694
    },
    {
      "epoch": 2.513520749665328,
      "grad_norm": 0.9041091799736023,
      "learning_rate": 3.2476802284082795e-05,
      "loss": 0.0528,
      "step": 4695
    },
    {
      "epoch": 2.5140562248995986,
      "grad_norm": 1.063240885734558,
      "learning_rate": 3.244111349036403e-05,
      "loss": 0.0741,
      "step": 4696
    },
    {
      "epoch": 2.514591700133869,
      "grad_norm": 1.2465705871582031,
      "learning_rate": 3.2405424696645255e-05,
      "loss": 0.1067,
      "step": 4697
    },
    {
      "epoch": 2.5151271753681392,
      "grad_norm": 1.201688528060913,
      "learning_rate": 3.236973590292648e-05,
      "loss": 0.103,
      "step": 4698
    },
    {
      "epoch": 2.5156626506024096,
      "grad_norm": 2.0425944328308105,
      "learning_rate": 3.233404710920771e-05,
      "loss": 0.1196,
      "step": 4699
    },
    {
      "epoch": 2.51619812583668,
      "grad_norm": 0.6437654495239258,
      "learning_rate": 3.2298358315488935e-05,
      "loss": 0.037,
      "step": 4700
    },
    {
      "epoch": 2.5167336010709507,
      "grad_norm": 0.8560094833374023,
      "learning_rate": 3.226266952177017e-05,
      "loss": 0.0639,
      "step": 4701
    },
    {
      "epoch": 2.517269076305221,
      "grad_norm": 1.5501306056976318,
      "learning_rate": 3.2226980728051395e-05,
      "loss": 0.0973,
      "step": 4702
    },
    {
      "epoch": 2.5178045515394913,
      "grad_norm": 2.345719337463379,
      "learning_rate": 3.219129193433262e-05,
      "loss": 0.0928,
      "step": 4703
    },
    {
      "epoch": 2.5183400267737617,
      "grad_norm": 0.4033326208591461,
      "learning_rate": 3.215560314061385e-05,
      "loss": 0.0131,
      "step": 4704
    },
    {
      "epoch": 2.518875502008032,
      "grad_norm": 1.0035724639892578,
      "learning_rate": 3.2119914346895075e-05,
      "loss": 0.0466,
      "step": 4705
    },
    {
      "epoch": 2.519410977242303,
      "grad_norm": 1.298358678817749,
      "learning_rate": 3.208422555317631e-05,
      "loss": 0.0951,
      "step": 4706
    },
    {
      "epoch": 2.519946452476573,
      "grad_norm": 0.5516281127929688,
      "learning_rate": 3.204853675945753e-05,
      "loss": 0.0383,
      "step": 4707
    },
    {
      "epoch": 2.5204819277108435,
      "grad_norm": 1.2839016914367676,
      "learning_rate": 3.2012847965738755e-05,
      "loss": 0.0559,
      "step": 4708
    },
    {
      "epoch": 2.521017402945114,
      "grad_norm": 0.9370468258857727,
      "learning_rate": 3.197715917201999e-05,
      "loss": 0.0967,
      "step": 4709
    },
    {
      "epoch": 2.521552878179384,
      "grad_norm": 0.8170235753059387,
      "learning_rate": 3.1941470378301215e-05,
      "loss": 0.0495,
      "step": 4710
    },
    {
      "epoch": 2.522088353413655,
      "grad_norm": 0.8717570900917053,
      "learning_rate": 3.190578158458244e-05,
      "loss": 0.0869,
      "step": 4711
    },
    {
      "epoch": 2.522623828647925,
      "grad_norm": 1.4713131189346313,
      "learning_rate": 3.187009279086367e-05,
      "loss": 0.0954,
      "step": 4712
    },
    {
      "epoch": 2.5231593038821956,
      "grad_norm": 1.1662510633468628,
      "learning_rate": 3.1834403997144895e-05,
      "loss": 0.0736,
      "step": 4713
    },
    {
      "epoch": 2.523694779116466,
      "grad_norm": 2.7627007961273193,
      "learning_rate": 3.179871520342613e-05,
      "loss": 0.0992,
      "step": 4714
    },
    {
      "epoch": 2.5242302543507362,
      "grad_norm": 1.1780259609222412,
      "learning_rate": 3.1763026409707355e-05,
      "loss": 0.068,
      "step": 4715
    },
    {
      "epoch": 2.5247657295850066,
      "grad_norm": 1.7070170640945435,
      "learning_rate": 3.172733761598858e-05,
      "loss": 0.0772,
      "step": 4716
    },
    {
      "epoch": 2.525301204819277,
      "grad_norm": 1.1620972156524658,
      "learning_rate": 3.169164882226981e-05,
      "loss": 0.0714,
      "step": 4717
    },
    {
      "epoch": 2.5258366800535477,
      "grad_norm": 0.7634354829788208,
      "learning_rate": 3.1655960028551035e-05,
      "loss": 0.0632,
      "step": 4718
    },
    {
      "epoch": 2.526372155287818,
      "grad_norm": 0.714630126953125,
      "learning_rate": 3.162027123483227e-05,
      "loss": 0.0423,
      "step": 4719
    },
    {
      "epoch": 2.5269076305220883,
      "grad_norm": 0.7482818365097046,
      "learning_rate": 3.1584582441113495e-05,
      "loss": 0.0484,
      "step": 4720
    },
    {
      "epoch": 2.5274431057563587,
      "grad_norm": 1.4807322025299072,
      "learning_rate": 3.1548893647394715e-05,
      "loss": 0.0843,
      "step": 4721
    },
    {
      "epoch": 2.527978580990629,
      "grad_norm": 2.5696511268615723,
      "learning_rate": 3.151320485367595e-05,
      "loss": 0.1011,
      "step": 4722
    },
    {
      "epoch": 2.5285140562248998,
      "grad_norm": 0.818372368812561,
      "learning_rate": 3.1477516059957175e-05,
      "loss": 0.0534,
      "step": 4723
    },
    {
      "epoch": 2.52904953145917,
      "grad_norm": 1.623199462890625,
      "learning_rate": 3.144182726623841e-05,
      "loss": 0.0701,
      "step": 4724
    },
    {
      "epoch": 2.5295850066934404,
      "grad_norm": 1.4930553436279297,
      "learning_rate": 3.140613847251963e-05,
      "loss": 0.1289,
      "step": 4725
    },
    {
      "epoch": 2.5301204819277108,
      "grad_norm": 1.0733702182769775,
      "learning_rate": 3.1370449678800855e-05,
      "loss": 0.085,
      "step": 4726
    },
    {
      "epoch": 2.530655957161981,
      "grad_norm": 3.3646833896636963,
      "learning_rate": 3.133476088508209e-05,
      "loss": 0.1223,
      "step": 4727
    },
    {
      "epoch": 2.531191432396252,
      "grad_norm": 1.2617567777633667,
      "learning_rate": 3.1299072091363315e-05,
      "loss": 0.0504,
      "step": 4728
    },
    {
      "epoch": 2.531726907630522,
      "grad_norm": 0.9856628179550171,
      "learning_rate": 3.126338329764454e-05,
      "loss": 0.0638,
      "step": 4729
    },
    {
      "epoch": 2.5322623828647925,
      "grad_norm": 0.7457204461097717,
      "learning_rate": 3.122769450392577e-05,
      "loss": 0.0313,
      "step": 4730
    },
    {
      "epoch": 2.532797858099063,
      "grad_norm": 0.35130423307418823,
      "learning_rate": 3.1192005710206995e-05,
      "loss": 0.0292,
      "step": 4731
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 6.470909595489502,
      "learning_rate": 3.115631691648823e-05,
      "loss": 0.0861,
      "step": 4732
    },
    {
      "epoch": 2.533868808567604,
      "grad_norm": 0.7343881726264954,
      "learning_rate": 3.1120628122769455e-05,
      "loss": 0.0338,
      "step": 4733
    },
    {
      "epoch": 2.5344042838018743,
      "grad_norm": 0.7746394872665405,
      "learning_rate": 3.1084939329050675e-05,
      "loss": 0.0273,
      "step": 4734
    },
    {
      "epoch": 2.5349397590361447,
      "grad_norm": 2.235696792602539,
      "learning_rate": 3.104925053533191e-05,
      "loss": 0.1092,
      "step": 4735
    },
    {
      "epoch": 2.535475234270415,
      "grad_norm": 1.4653310775756836,
      "learning_rate": 3.1013561741613135e-05,
      "loss": 0.051,
      "step": 4736
    },
    {
      "epoch": 2.5360107095046853,
      "grad_norm": 1.8291261196136475,
      "learning_rate": 3.097787294789437e-05,
      "loss": 0.1583,
      "step": 4737
    },
    {
      "epoch": 2.536546184738956,
      "grad_norm": 0.857712984085083,
      "learning_rate": 3.094218415417559e-05,
      "loss": 0.0615,
      "step": 4738
    },
    {
      "epoch": 2.537081659973226,
      "grad_norm": 1.1393778324127197,
      "learning_rate": 3.0906495360456815e-05,
      "loss": 0.0601,
      "step": 4739
    },
    {
      "epoch": 2.5376171352074968,
      "grad_norm": 1.0911445617675781,
      "learning_rate": 3.087080656673805e-05,
      "loss": 0.045,
      "step": 4740
    },
    {
      "epoch": 2.538152610441767,
      "grad_norm": 1.299899697303772,
      "learning_rate": 3.0835117773019275e-05,
      "loss": 0.0332,
      "step": 4741
    },
    {
      "epoch": 2.5386880856760374,
      "grad_norm": 1.2103098630905151,
      "learning_rate": 3.07994289793005e-05,
      "loss": 0.053,
      "step": 4742
    },
    {
      "epoch": 2.5392235609103078,
      "grad_norm": 3.369145393371582,
      "learning_rate": 3.076374018558173e-05,
      "loss": 0.1946,
      "step": 4743
    },
    {
      "epoch": 2.539759036144578,
      "grad_norm": 1.1024713516235352,
      "learning_rate": 3.0728051391862955e-05,
      "loss": 0.0653,
      "step": 4744
    },
    {
      "epoch": 2.540294511378849,
      "grad_norm": 0.3892490267753601,
      "learning_rate": 3.069236259814419e-05,
      "loss": 0.015,
      "step": 4745
    },
    {
      "epoch": 2.540829986613119,
      "grad_norm": 1.9681167602539062,
      "learning_rate": 3.0656673804425415e-05,
      "loss": 0.0837,
      "step": 4746
    },
    {
      "epoch": 2.5413654618473895,
      "grad_norm": 1.3039264678955078,
      "learning_rate": 3.0620985010706635e-05,
      "loss": 0.1266,
      "step": 4747
    },
    {
      "epoch": 2.54190093708166,
      "grad_norm": 1.359493613243103,
      "learning_rate": 3.058529621698787e-05,
      "loss": 0.0672,
      "step": 4748
    },
    {
      "epoch": 2.54243641231593,
      "grad_norm": 2.0160961151123047,
      "learning_rate": 3.0549607423269095e-05,
      "loss": 0.1054,
      "step": 4749
    },
    {
      "epoch": 2.542971887550201,
      "grad_norm": 1.1870877742767334,
      "learning_rate": 3.0513918629550325e-05,
      "loss": 0.0559,
      "step": 4750
    },
    {
      "epoch": 2.5435073627844713,
      "grad_norm": 1.7003445625305176,
      "learning_rate": 3.047822983583155e-05,
      "loss": 0.0412,
      "step": 4751
    },
    {
      "epoch": 2.5440428380187416,
      "grad_norm": 1.0145069360733032,
      "learning_rate": 3.0442541042112778e-05,
      "loss": 0.0631,
      "step": 4752
    },
    {
      "epoch": 2.544578313253012,
      "grad_norm": 0.38067564368247986,
      "learning_rate": 3.0406852248394008e-05,
      "loss": 0.0179,
      "step": 4753
    },
    {
      "epoch": 2.5451137884872823,
      "grad_norm": 1.8574192523956299,
      "learning_rate": 3.0371163454675235e-05,
      "loss": 0.1208,
      "step": 4754
    },
    {
      "epoch": 2.545649263721553,
      "grad_norm": 1.1846100091934204,
      "learning_rate": 3.0335474660956458e-05,
      "loss": 0.0335,
      "step": 4755
    },
    {
      "epoch": 2.5461847389558234,
      "grad_norm": 0.9116097688674927,
      "learning_rate": 3.029978586723769e-05,
      "loss": 0.0658,
      "step": 4756
    },
    {
      "epoch": 2.5467202141900938,
      "grad_norm": 0.4494045078754425,
      "learning_rate": 3.0264097073518915e-05,
      "loss": 0.0239,
      "step": 4757
    },
    {
      "epoch": 2.547255689424364,
      "grad_norm": 0.7640905976295471,
      "learning_rate": 3.0228408279800148e-05,
      "loss": 0.0271,
      "step": 4758
    },
    {
      "epoch": 2.5477911646586344,
      "grad_norm": 1.9697818756103516,
      "learning_rate": 3.019271948608137e-05,
      "loss": 0.1166,
      "step": 4759
    },
    {
      "epoch": 2.548326639892905,
      "grad_norm": 1.428554654121399,
      "learning_rate": 3.0157030692362598e-05,
      "loss": 0.0774,
      "step": 4760
    },
    {
      "epoch": 2.5488621151271755,
      "grad_norm": 1.1657159328460693,
      "learning_rate": 3.0121341898643828e-05,
      "loss": 0.0794,
      "step": 4761
    },
    {
      "epoch": 2.549397590361446,
      "grad_norm": 0.583746612071991,
      "learning_rate": 3.0085653104925055e-05,
      "loss": 0.0288,
      "step": 4762
    },
    {
      "epoch": 2.549933065595716,
      "grad_norm": 2.0570785999298096,
      "learning_rate": 3.004996431120628e-05,
      "loss": 0.0866,
      "step": 4763
    },
    {
      "epoch": 2.5504685408299865,
      "grad_norm": 0.7028926610946655,
      "learning_rate": 3.001427551748751e-05,
      "loss": 0.0463,
      "step": 4764
    },
    {
      "epoch": 2.551004016064257,
      "grad_norm": 1.2125635147094727,
      "learning_rate": 2.9978586723768738e-05,
      "loss": 0.1019,
      "step": 4765
    },
    {
      "epoch": 2.551539491298527,
      "grad_norm": 0.8907698392868042,
      "learning_rate": 2.9942897930049968e-05,
      "loss": 0.0454,
      "step": 4766
    },
    {
      "epoch": 2.552074966532798,
      "grad_norm": 2.5563342571258545,
      "learning_rate": 2.9907209136331195e-05,
      "loss": 0.1496,
      "step": 4767
    },
    {
      "epoch": 2.5526104417670683,
      "grad_norm": 0.6796262860298157,
      "learning_rate": 2.9871520342612418e-05,
      "loss": 0.0355,
      "step": 4768
    },
    {
      "epoch": 2.5531459170013386,
      "grad_norm": 0.7942674160003662,
      "learning_rate": 2.983583154889365e-05,
      "loss": 0.0541,
      "step": 4769
    },
    {
      "epoch": 2.553681392235609,
      "grad_norm": 1.4349957704544067,
      "learning_rate": 2.9800142755174875e-05,
      "loss": 0.1263,
      "step": 4770
    },
    {
      "epoch": 2.5542168674698793,
      "grad_norm": 1.6660429239273071,
      "learning_rate": 2.9764453961456108e-05,
      "loss": 0.0789,
      "step": 4771
    },
    {
      "epoch": 2.55475234270415,
      "grad_norm": 0.44834354519844055,
      "learning_rate": 2.972876516773733e-05,
      "loss": 0.0234,
      "step": 4772
    },
    {
      "epoch": 2.5552878179384204,
      "grad_norm": 0.6776530742645264,
      "learning_rate": 2.9693076374018558e-05,
      "loss": 0.0262,
      "step": 4773
    },
    {
      "epoch": 2.5558232931726907,
      "grad_norm": 0.6391149163246155,
      "learning_rate": 2.9657387580299788e-05,
      "loss": 0.0274,
      "step": 4774
    },
    {
      "epoch": 2.556358768406961,
      "grad_norm": 1.7716660499572754,
      "learning_rate": 2.9621698786581015e-05,
      "loss": 0.0648,
      "step": 4775
    },
    {
      "epoch": 2.5568942436412314,
      "grad_norm": 2.1544129848480225,
      "learning_rate": 2.958600999286224e-05,
      "loss": 0.0963,
      "step": 4776
    },
    {
      "epoch": 2.557429718875502,
      "grad_norm": 0.5743445158004761,
      "learning_rate": 2.955032119914347e-05,
      "loss": 0.0176,
      "step": 4777
    },
    {
      "epoch": 2.5579651941097725,
      "grad_norm": 0.9372166991233826,
      "learning_rate": 2.9514632405424698e-05,
      "loss": 0.0395,
      "step": 4778
    },
    {
      "epoch": 2.558500669344043,
      "grad_norm": 1.931139588356018,
      "learning_rate": 2.9478943611705928e-05,
      "loss": 0.1292,
      "step": 4779
    },
    {
      "epoch": 2.559036144578313,
      "grad_norm": 0.5560660362243652,
      "learning_rate": 2.9443254817987155e-05,
      "loss": 0.0155,
      "step": 4780
    },
    {
      "epoch": 2.5595716198125835,
      "grad_norm": 1.601308822631836,
      "learning_rate": 2.9407566024268378e-05,
      "loss": 0.0808,
      "step": 4781
    },
    {
      "epoch": 2.5601070950468543,
      "grad_norm": 1.3190207481384277,
      "learning_rate": 2.937187723054961e-05,
      "loss": 0.0903,
      "step": 4782
    },
    {
      "epoch": 2.5606425702811246,
      "grad_norm": 0.5314237475395203,
      "learning_rate": 2.9336188436830835e-05,
      "loss": 0.0486,
      "step": 4783
    },
    {
      "epoch": 2.561178045515395,
      "grad_norm": 1.3822319507598877,
      "learning_rate": 2.930049964311206e-05,
      "loss": 0.0679,
      "step": 4784
    },
    {
      "epoch": 2.5617135207496653,
      "grad_norm": 1.094785213470459,
      "learning_rate": 2.926481084939329e-05,
      "loss": 0.0604,
      "step": 4785
    },
    {
      "epoch": 2.5622489959839356,
      "grad_norm": 1.2713946104049683,
      "learning_rate": 2.9229122055674518e-05,
      "loss": 0.0757,
      "step": 4786
    },
    {
      "epoch": 2.5627844712182064,
      "grad_norm": 1.250787615776062,
      "learning_rate": 2.9193433261955748e-05,
      "loss": 0.1047,
      "step": 4787
    },
    {
      "epoch": 2.5633199464524767,
      "grad_norm": 2.3902440071105957,
      "learning_rate": 2.9157744468236975e-05,
      "loss": 0.1727,
      "step": 4788
    },
    {
      "epoch": 2.563855421686747,
      "grad_norm": 1.4693671464920044,
      "learning_rate": 2.91220556745182e-05,
      "loss": 0.076,
      "step": 4789
    },
    {
      "epoch": 2.5643908969210174,
      "grad_norm": 0.6140497326850891,
      "learning_rate": 2.908636688079943e-05,
      "loss": 0.0201,
      "step": 4790
    },
    {
      "epoch": 2.5649263721552877,
      "grad_norm": 0.8638959527015686,
      "learning_rate": 2.9050678087080658e-05,
      "loss": 0.0382,
      "step": 4791
    },
    {
      "epoch": 2.565461847389558,
      "grad_norm": 0.8260481357574463,
      "learning_rate": 2.9014989293361888e-05,
      "loss": 0.0387,
      "step": 4792
    },
    {
      "epoch": 2.5659973226238284,
      "grad_norm": 1.7858330011367798,
      "learning_rate": 2.8979300499643115e-05,
      "loss": 0.0482,
      "step": 4793
    },
    {
      "epoch": 2.566532797858099,
      "grad_norm": 1.388833999633789,
      "learning_rate": 2.8943611705924338e-05,
      "loss": 0.0457,
      "step": 4794
    },
    {
      "epoch": 2.5670682730923695,
      "grad_norm": 0.8229779005050659,
      "learning_rate": 2.890792291220557e-05,
      "loss": 0.0785,
      "step": 4795
    },
    {
      "epoch": 2.56760374832664,
      "grad_norm": 1.5654574632644653,
      "learning_rate": 2.8872234118486795e-05,
      "loss": 0.1161,
      "step": 4796
    },
    {
      "epoch": 2.56813922356091,
      "grad_norm": 0.6357316374778748,
      "learning_rate": 2.883654532476802e-05,
      "loss": 0.0236,
      "step": 4797
    },
    {
      "epoch": 2.5686746987951805,
      "grad_norm": 1.2192847728729248,
      "learning_rate": 2.880085653104925e-05,
      "loss": 0.0502,
      "step": 4798
    },
    {
      "epoch": 2.5692101740294513,
      "grad_norm": 3.707587480545044,
      "learning_rate": 2.8765167737330478e-05,
      "loss": 0.1297,
      "step": 4799
    },
    {
      "epoch": 2.5697456492637216,
      "grad_norm": 1.3251773118972778,
      "learning_rate": 2.8729478943611708e-05,
      "loss": 0.079,
      "step": 4800
    },
    {
      "epoch": 2.570281124497992,
      "grad_norm": 1.2578014135360718,
      "learning_rate": 2.8693790149892935e-05,
      "loss": 0.0811,
      "step": 4801
    },
    {
      "epoch": 2.5708165997322623,
      "grad_norm": 1.2335076332092285,
      "learning_rate": 2.865810135617416e-05,
      "loss": 0.0521,
      "step": 4802
    },
    {
      "epoch": 2.5713520749665326,
      "grad_norm": 0.7217953205108643,
      "learning_rate": 2.862241256245539e-05,
      "loss": 0.0326,
      "step": 4803
    },
    {
      "epoch": 2.5718875502008034,
      "grad_norm": 3.3888046741485596,
      "learning_rate": 2.8586723768736618e-05,
      "loss": 0.1241,
      "step": 4804
    },
    {
      "epoch": 2.5724230254350737,
      "grad_norm": 1.0828180313110352,
      "learning_rate": 2.8551034975017848e-05,
      "loss": 0.0367,
      "step": 4805
    },
    {
      "epoch": 2.572958500669344,
      "grad_norm": 0.37213075160980225,
      "learning_rate": 2.8515346181299075e-05,
      "loss": 0.0068,
      "step": 4806
    },
    {
      "epoch": 2.5734939759036144,
      "grad_norm": 1.2731051445007324,
      "learning_rate": 2.8479657387580298e-05,
      "loss": 0.0439,
      "step": 4807
    },
    {
      "epoch": 2.5740294511378847,
      "grad_norm": 1.6482250690460205,
      "learning_rate": 2.844396859386153e-05,
      "loss": 0.0687,
      "step": 4808
    },
    {
      "epoch": 2.5745649263721555,
      "grad_norm": 1.8793874979019165,
      "learning_rate": 2.8408279800142758e-05,
      "loss": 0.0772,
      "step": 4809
    },
    {
      "epoch": 2.575100401606426,
      "grad_norm": 1.477279543876648,
      "learning_rate": 2.837259100642398e-05,
      "loss": 0.0758,
      "step": 4810
    },
    {
      "epoch": 2.575635876840696,
      "grad_norm": 2.631659746170044,
      "learning_rate": 2.8336902212705215e-05,
      "loss": 0.1211,
      "step": 4811
    },
    {
      "epoch": 2.5761713520749665,
      "grad_norm": 2.1539602279663086,
      "learning_rate": 2.8301213418986438e-05,
      "loss": 0.1088,
      "step": 4812
    },
    {
      "epoch": 2.576706827309237,
      "grad_norm": 0.5378016233444214,
      "learning_rate": 2.826552462526767e-05,
      "loss": 0.0272,
      "step": 4813
    },
    {
      "epoch": 2.5772423025435076,
      "grad_norm": 0.8251956105232239,
      "learning_rate": 2.8229835831548895e-05,
      "loss": 0.0289,
      "step": 4814
    },
    {
      "epoch": 2.5777777777777775,
      "grad_norm": 1.3193774223327637,
      "learning_rate": 2.819414703783012e-05,
      "loss": 0.0774,
      "step": 4815
    },
    {
      "epoch": 2.5783132530120483,
      "grad_norm": 1.4010711908340454,
      "learning_rate": 2.815845824411135e-05,
      "loss": 0.0457,
      "step": 4816
    },
    {
      "epoch": 2.5788487282463186,
      "grad_norm": 1.518408179283142,
      "learning_rate": 2.8122769450392578e-05,
      "loss": 0.0519,
      "step": 4817
    },
    {
      "epoch": 2.579384203480589,
      "grad_norm": 0.700494110584259,
      "learning_rate": 2.8087080656673805e-05,
      "loss": 0.0388,
      "step": 4818
    },
    {
      "epoch": 2.5799196787148593,
      "grad_norm": 1.1697386503219604,
      "learning_rate": 2.8051391862955035e-05,
      "loss": 0.0706,
      "step": 4819
    },
    {
      "epoch": 2.5804551539491296,
      "grad_norm": 0.7537082433700562,
      "learning_rate": 2.801570306923626e-05,
      "loss": 0.0263,
      "step": 4820
    },
    {
      "epoch": 2.5809906291834004,
      "grad_norm": 1.8256622552871704,
      "learning_rate": 2.798001427551749e-05,
      "loss": 0.089,
      "step": 4821
    },
    {
      "epoch": 2.5815261044176707,
      "grad_norm": 1.6007859706878662,
      "learning_rate": 2.7944325481798718e-05,
      "loss": 0.0588,
      "step": 4822
    },
    {
      "epoch": 2.582061579651941,
      "grad_norm": 6.548825263977051,
      "learning_rate": 2.790863668807994e-05,
      "loss": 0.1153,
      "step": 4823
    },
    {
      "epoch": 2.5825970548862114,
      "grad_norm": 1.4597482681274414,
      "learning_rate": 2.7872947894361175e-05,
      "loss": 0.0655,
      "step": 4824
    },
    {
      "epoch": 2.5831325301204817,
      "grad_norm": 7.40313720703125,
      "learning_rate": 2.7837259100642398e-05,
      "loss": 0.0681,
      "step": 4825
    },
    {
      "epoch": 2.5836680053547525,
      "grad_norm": 0.825459361076355,
      "learning_rate": 2.780157030692363e-05,
      "loss": 0.0345,
      "step": 4826
    },
    {
      "epoch": 2.584203480589023,
      "grad_norm": 0.975378692150116,
      "learning_rate": 2.7765881513204855e-05,
      "loss": 0.0785,
      "step": 4827
    },
    {
      "epoch": 2.584738955823293,
      "grad_norm": 1.6912555694580078,
      "learning_rate": 2.773019271948608e-05,
      "loss": 0.054,
      "step": 4828
    },
    {
      "epoch": 2.5852744310575635,
      "grad_norm": 3.199939489364624,
      "learning_rate": 2.769450392576731e-05,
      "loss": 0.0945,
      "step": 4829
    },
    {
      "epoch": 2.585809906291834,
      "grad_norm": 1.216941237449646,
      "learning_rate": 2.7658815132048538e-05,
      "loss": 0.067,
      "step": 4830
    },
    {
      "epoch": 2.5863453815261046,
      "grad_norm": 3.298713445663452,
      "learning_rate": 2.7623126338329765e-05,
      "loss": 0.0874,
      "step": 4831
    },
    {
      "epoch": 2.586880856760375,
      "grad_norm": 1.1684014797210693,
      "learning_rate": 2.7587437544610995e-05,
      "loss": 0.0729,
      "step": 4832
    },
    {
      "epoch": 2.5874163319946453,
      "grad_norm": 1.3422378301620483,
      "learning_rate": 2.755174875089222e-05,
      "loss": 0.0765,
      "step": 4833
    },
    {
      "epoch": 2.5879518072289156,
      "grad_norm": 1.0611424446105957,
      "learning_rate": 2.751605995717345e-05,
      "loss": 0.0667,
      "step": 4834
    },
    {
      "epoch": 2.588487282463186,
      "grad_norm": 1.3191779851913452,
      "learning_rate": 2.7480371163454678e-05,
      "loss": 0.1204,
      "step": 4835
    },
    {
      "epoch": 2.5890227576974567,
      "grad_norm": 0.9307891130447388,
      "learning_rate": 2.74446823697359e-05,
      "loss": 0.0724,
      "step": 4836
    },
    {
      "epoch": 2.589558232931727,
      "grad_norm": 1.4372859001159668,
      "learning_rate": 2.7408993576017135e-05,
      "loss": 0.0693,
      "step": 4837
    },
    {
      "epoch": 2.5900937081659974,
      "grad_norm": 1.054342269897461,
      "learning_rate": 2.7373304782298358e-05,
      "loss": 0.0578,
      "step": 4838
    },
    {
      "epoch": 2.5906291834002677,
      "grad_norm": 1.5954855680465698,
      "learning_rate": 2.7337615988579585e-05,
      "loss": 0.0502,
      "step": 4839
    },
    {
      "epoch": 2.591164658634538,
      "grad_norm": 0.8908244371414185,
      "learning_rate": 2.7301927194860815e-05,
      "loss": 0.0336,
      "step": 4840
    },
    {
      "epoch": 2.591700133868809,
      "grad_norm": 1.1341969966888428,
      "learning_rate": 2.726623840114204e-05,
      "loss": 0.0687,
      "step": 4841
    },
    {
      "epoch": 2.5922356091030787,
      "grad_norm": 2.3530449867248535,
      "learning_rate": 2.723054960742327e-05,
      "loss": 0.1219,
      "step": 4842
    },
    {
      "epoch": 2.5927710843373495,
      "grad_norm": 3.2094125747680664,
      "learning_rate": 2.7194860813704498e-05,
      "loss": 0.1417,
      "step": 4843
    },
    {
      "epoch": 2.59330655957162,
      "grad_norm": 2.2368624210357666,
      "learning_rate": 2.7159172019985725e-05,
      "loss": 0.0908,
      "step": 4844
    },
    {
      "epoch": 2.59384203480589,
      "grad_norm": 0.7195714116096497,
      "learning_rate": 2.7123483226266955e-05,
      "loss": 0.0356,
      "step": 4845
    },
    {
      "epoch": 2.5943775100401605,
      "grad_norm": 1.0484709739685059,
      "learning_rate": 2.708779443254818e-05,
      "loss": 0.0801,
      "step": 4846
    },
    {
      "epoch": 2.594912985274431,
      "grad_norm": 3.381291151046753,
      "learning_rate": 2.705210563882941e-05,
      "loss": 0.0961,
      "step": 4847
    },
    {
      "epoch": 2.5954484605087016,
      "grad_norm": 1.3181504011154175,
      "learning_rate": 2.7016416845110638e-05,
      "loss": 0.094,
      "step": 4848
    },
    {
      "epoch": 2.595983935742972,
      "grad_norm": 0.8554010391235352,
      "learning_rate": 2.698072805139186e-05,
      "loss": 0.0373,
      "step": 4849
    },
    {
      "epoch": 2.5965194109772423,
      "grad_norm": 0.9172959327697754,
      "learning_rate": 2.6945039257673095e-05,
      "loss": 0.0287,
      "step": 4850
    },
    {
      "epoch": 2.5970548862115126,
      "grad_norm": 3.239060163497925,
      "learning_rate": 2.6909350463954318e-05,
      "loss": 0.1123,
      "step": 4851
    },
    {
      "epoch": 2.597590361445783,
      "grad_norm": 1.2803906202316284,
      "learning_rate": 2.6873661670235545e-05,
      "loss": 0.0473,
      "step": 4852
    },
    {
      "epoch": 2.5981258366800537,
      "grad_norm": 0.4883660078048706,
      "learning_rate": 2.6837972876516775e-05,
      "loss": 0.0219,
      "step": 4853
    },
    {
      "epoch": 2.598661311914324,
      "grad_norm": 0.9417630434036255,
      "learning_rate": 2.6802284082798e-05,
      "loss": 0.0756,
      "step": 4854
    },
    {
      "epoch": 2.5991967871485944,
      "grad_norm": 1.6827785968780518,
      "learning_rate": 2.676659528907923e-05,
      "loss": 0.0994,
      "step": 4855
    },
    {
      "epoch": 2.5997322623828647,
      "grad_norm": 0.9878647923469543,
      "learning_rate": 2.6730906495360458e-05,
      "loss": 0.0496,
      "step": 4856
    },
    {
      "epoch": 2.600267737617135,
      "grad_norm": 1.5943042039871216,
      "learning_rate": 2.6695217701641685e-05,
      "loss": 0.0818,
      "step": 4857
    },
    {
      "epoch": 2.600803212851406,
      "grad_norm": 1.4477061033248901,
      "learning_rate": 2.6659528907922915e-05,
      "loss": 0.0595,
      "step": 4858
    },
    {
      "epoch": 2.601338688085676,
      "grad_norm": 0.8315472602844238,
      "learning_rate": 2.662384011420414e-05,
      "loss": 0.02,
      "step": 4859
    },
    {
      "epoch": 2.6018741633199465,
      "grad_norm": 3.811887741088867,
      "learning_rate": 2.658815132048537e-05,
      "loss": 0.1635,
      "step": 4860
    },
    {
      "epoch": 2.602409638554217,
      "grad_norm": 3.3563971519470215,
      "learning_rate": 2.6552462526766598e-05,
      "loss": 0.0377,
      "step": 4861
    },
    {
      "epoch": 2.602945113788487,
      "grad_norm": 1.721945881843567,
      "learning_rate": 2.651677373304782e-05,
      "loss": 0.0805,
      "step": 4862
    },
    {
      "epoch": 2.603480589022758,
      "grad_norm": 0.9218999743461609,
      "learning_rate": 2.6481084939329055e-05,
      "loss": 0.0534,
      "step": 4863
    },
    {
      "epoch": 2.6040160642570283,
      "grad_norm": 1.2140265703201294,
      "learning_rate": 2.6445396145610278e-05,
      "loss": 0.0676,
      "step": 4864
    },
    {
      "epoch": 2.6045515394912986,
      "grad_norm": 0.5933918356895447,
      "learning_rate": 2.6409707351891505e-05,
      "loss": 0.0686,
      "step": 4865
    },
    {
      "epoch": 2.605087014725569,
      "grad_norm": 0.706783652305603,
      "learning_rate": 2.6374018558172735e-05,
      "loss": 0.035,
      "step": 4866
    },
    {
      "epoch": 2.6056224899598392,
      "grad_norm": 1.14768648147583,
      "learning_rate": 2.633832976445396e-05,
      "loss": 0.059,
      "step": 4867
    },
    {
      "epoch": 2.60615796519411,
      "grad_norm": 0.39448869228363037,
      "learning_rate": 2.630264097073519e-05,
      "loss": 0.0161,
      "step": 4868
    },
    {
      "epoch": 2.60669344042838,
      "grad_norm": 0.6603187918663025,
      "learning_rate": 2.6266952177016418e-05,
      "loss": 0.0262,
      "step": 4869
    },
    {
      "epoch": 2.6072289156626507,
      "grad_norm": 1.169705867767334,
      "learning_rate": 2.6231263383297645e-05,
      "loss": 0.0892,
      "step": 4870
    },
    {
      "epoch": 2.607764390896921,
      "grad_norm": 0.994823694229126,
      "learning_rate": 2.6195574589578875e-05,
      "loss": 0.04,
      "step": 4871
    },
    {
      "epoch": 2.6082998661311914,
      "grad_norm": 0.6063233017921448,
      "learning_rate": 2.61598857958601e-05,
      "loss": 0.0407,
      "step": 4872
    },
    {
      "epoch": 2.6088353413654617,
      "grad_norm": 1.464550256729126,
      "learning_rate": 2.6124197002141328e-05,
      "loss": 0.1398,
      "step": 4873
    },
    {
      "epoch": 2.609370816599732,
      "grad_norm": 2.2744407653808594,
      "learning_rate": 2.6088508208422558e-05,
      "loss": 0.1191,
      "step": 4874
    },
    {
      "epoch": 2.609906291834003,
      "grad_norm": 1.8809518814086914,
      "learning_rate": 2.6052819414703785e-05,
      "loss": 0.0981,
      "step": 4875
    },
    {
      "epoch": 2.610441767068273,
      "grad_norm": 0.347450315952301,
      "learning_rate": 2.6017130620985015e-05,
      "loss": 0.0137,
      "step": 4876
    },
    {
      "epoch": 2.6109772423025435,
      "grad_norm": 1.6780668497085571,
      "learning_rate": 2.598144182726624e-05,
      "loss": 0.173,
      "step": 4877
    },
    {
      "epoch": 2.611512717536814,
      "grad_norm": 0.6394922733306885,
      "learning_rate": 2.5945753033547465e-05,
      "loss": 0.03,
      "step": 4878
    },
    {
      "epoch": 2.612048192771084,
      "grad_norm": 1.0619803667068481,
      "learning_rate": 2.5910064239828698e-05,
      "loss": 0.0539,
      "step": 4879
    },
    {
      "epoch": 2.612583668005355,
      "grad_norm": 1.9755363464355469,
      "learning_rate": 2.587437544610992e-05,
      "loss": 0.0935,
      "step": 4880
    },
    {
      "epoch": 2.6131191432396252,
      "grad_norm": 0.8775044083595276,
      "learning_rate": 2.5838686652391155e-05,
      "loss": 0.029,
      "step": 4881
    },
    {
      "epoch": 2.6136546184738956,
      "grad_norm": 0.6304011344909668,
      "learning_rate": 2.5802997858672378e-05,
      "loss": 0.0318,
      "step": 4882
    },
    {
      "epoch": 2.614190093708166,
      "grad_norm": 2.280766248703003,
      "learning_rate": 2.5767309064953605e-05,
      "loss": 0.0544,
      "step": 4883
    },
    {
      "epoch": 2.6147255689424362,
      "grad_norm": 0.3919103145599365,
      "learning_rate": 2.5731620271234835e-05,
      "loss": 0.0347,
      "step": 4884
    },
    {
      "epoch": 2.615261044176707,
      "grad_norm": 0.8024088740348816,
      "learning_rate": 2.569593147751606e-05,
      "loss": 0.0599,
      "step": 4885
    },
    {
      "epoch": 2.6157965194109773,
      "grad_norm": 0.9070700407028198,
      "learning_rate": 2.5660242683797288e-05,
      "loss": 0.0445,
      "step": 4886
    },
    {
      "epoch": 2.6163319946452477,
      "grad_norm": 2.007298707962036,
      "learning_rate": 2.5624553890078518e-05,
      "loss": 0.0732,
      "step": 4887
    },
    {
      "epoch": 2.616867469879518,
      "grad_norm": 1.307114839553833,
      "learning_rate": 2.5588865096359745e-05,
      "loss": 0.0495,
      "step": 4888
    },
    {
      "epoch": 2.6174029451137883,
      "grad_norm": 1.7318373918533325,
      "learning_rate": 2.5553176302640975e-05,
      "loss": 0.1101,
      "step": 4889
    },
    {
      "epoch": 2.617938420348059,
      "grad_norm": 1.006461501121521,
      "learning_rate": 2.55174875089222e-05,
      "loss": 0.0675,
      "step": 4890
    },
    {
      "epoch": 2.6184738955823295,
      "grad_norm": 1.9809308052062988,
      "learning_rate": 2.5481798715203425e-05,
      "loss": 0.0974,
      "step": 4891
    },
    {
      "epoch": 2.6190093708166,
      "grad_norm": 2.450941801071167,
      "learning_rate": 2.5446109921484658e-05,
      "loss": 0.14,
      "step": 4892
    },
    {
      "epoch": 2.61954484605087,
      "grad_norm": 1.4686630964279175,
      "learning_rate": 2.541042112776588e-05,
      "loss": 0.0825,
      "step": 4893
    },
    {
      "epoch": 2.6200803212851405,
      "grad_norm": 1.7039352655410767,
      "learning_rate": 2.5374732334047108e-05,
      "loss": 0.0924,
      "step": 4894
    },
    {
      "epoch": 2.6206157965194112,
      "grad_norm": 0.7602790594100952,
      "learning_rate": 2.5339043540328338e-05,
      "loss": 0.0415,
      "step": 4895
    },
    {
      "epoch": 2.621151271753681,
      "grad_norm": 1.6873418092727661,
      "learning_rate": 2.5303354746609565e-05,
      "loss": 0.1441,
      "step": 4896
    },
    {
      "epoch": 2.621686746987952,
      "grad_norm": 1.5330569744110107,
      "learning_rate": 2.5267665952890795e-05,
      "loss": 0.0445,
      "step": 4897
    },
    {
      "epoch": 2.6222222222222222,
      "grad_norm": 0.5094940662384033,
      "learning_rate": 2.523197715917202e-05,
      "loss": 0.027,
      "step": 4898
    },
    {
      "epoch": 2.6227576974564926,
      "grad_norm": 0.9811106324195862,
      "learning_rate": 2.5196288365453248e-05,
      "loss": 0.0495,
      "step": 4899
    },
    {
      "epoch": 2.623293172690763,
      "grad_norm": 0.8940758109092712,
      "learning_rate": 2.5160599571734478e-05,
      "loss": 0.0661,
      "step": 4900
    },
    {
      "epoch": 2.6238286479250332,
      "grad_norm": 0.6858319044113159,
      "learning_rate": 2.5124910778015705e-05,
      "loss": 0.0324,
      "step": 4901
    },
    {
      "epoch": 2.624364123159304,
      "grad_norm": 0.8441448211669922,
      "learning_rate": 2.5089221984296935e-05,
      "loss": 0.0607,
      "step": 4902
    },
    {
      "epoch": 2.6248995983935743,
      "grad_norm": 0.9721173644065857,
      "learning_rate": 2.505353319057816e-05,
      "loss": 0.0628,
      "step": 4903
    },
    {
      "epoch": 2.6254350736278447,
      "grad_norm": 1.047615885734558,
      "learning_rate": 2.5017844396859384e-05,
      "loss": 0.0586,
      "step": 4904
    },
    {
      "epoch": 2.625970548862115,
      "grad_norm": 0.5738034248352051,
      "learning_rate": 2.4982155603140615e-05,
      "loss": 0.0256,
      "step": 4905
    },
    {
      "epoch": 2.6265060240963853,
      "grad_norm": 1.4792078733444214,
      "learning_rate": 2.494646680942184e-05,
      "loss": 0.0255,
      "step": 4906
    },
    {
      "epoch": 2.627041499330656,
      "grad_norm": 0.8816162943840027,
      "learning_rate": 2.491077801570307e-05,
      "loss": 0.0604,
      "step": 4907
    },
    {
      "epoch": 2.6275769745649264,
      "grad_norm": 1.3743191957473755,
      "learning_rate": 2.4875089221984298e-05,
      "loss": 0.0959,
      "step": 4908
    },
    {
      "epoch": 2.628112449799197,
      "grad_norm": 0.6112574338912964,
      "learning_rate": 2.4839400428265525e-05,
      "loss": 0.025,
      "step": 4909
    },
    {
      "epoch": 2.628647925033467,
      "grad_norm": 3.365093231201172,
      "learning_rate": 2.4803711634546755e-05,
      "loss": 0.1015,
      "step": 4910
    },
    {
      "epoch": 2.6291834002677374,
      "grad_norm": 1.5182462930679321,
      "learning_rate": 2.476802284082798e-05,
      "loss": 0.0852,
      "step": 4911
    },
    {
      "epoch": 2.6297188755020082,
      "grad_norm": 1.0744037628173828,
      "learning_rate": 2.473233404710921e-05,
      "loss": 0.0597,
      "step": 4912
    },
    {
      "epoch": 2.6302543507362786,
      "grad_norm": 1.120883584022522,
      "learning_rate": 2.4696645253390434e-05,
      "loss": 0.0472,
      "step": 4913
    },
    {
      "epoch": 2.630789825970549,
      "grad_norm": 1.5642611980438232,
      "learning_rate": 2.4660956459671665e-05,
      "loss": 0.0663,
      "step": 4914
    },
    {
      "epoch": 2.6313253012048192,
      "grad_norm": 1.4086984395980835,
      "learning_rate": 2.462526766595289e-05,
      "loss": 0.0871,
      "step": 4915
    },
    {
      "epoch": 2.6318607764390896,
      "grad_norm": 0.7486171126365662,
      "learning_rate": 2.458957887223412e-05,
      "loss": 0.0444,
      "step": 4916
    },
    {
      "epoch": 2.6323962516733603,
      "grad_norm": 0.9259774684906006,
      "learning_rate": 2.4553890078515348e-05,
      "loss": 0.0382,
      "step": 4917
    },
    {
      "epoch": 2.6329317269076307,
      "grad_norm": 0.7684224843978882,
      "learning_rate": 2.4518201284796574e-05,
      "loss": 0.0144,
      "step": 4918
    },
    {
      "epoch": 2.633467202141901,
      "grad_norm": 1.0116995573043823,
      "learning_rate": 2.44825124910778e-05,
      "loss": 0.0532,
      "step": 4919
    },
    {
      "epoch": 2.6340026773761713,
      "grad_norm": 1.3643797636032104,
      "learning_rate": 2.444682369735903e-05,
      "loss": 0.0479,
      "step": 4920
    },
    {
      "epoch": 2.6345381526104417,
      "grad_norm": 2.2620389461517334,
      "learning_rate": 2.4411134903640258e-05,
      "loss": 0.0816,
      "step": 4921
    },
    {
      "epoch": 2.6350736278447124,
      "grad_norm": 1.5787031650543213,
      "learning_rate": 2.4375446109921484e-05,
      "loss": 0.0711,
      "step": 4922
    },
    {
      "epoch": 2.6356091030789823,
      "grad_norm": 0.988055408000946,
      "learning_rate": 2.4339757316202715e-05,
      "loss": 0.0454,
      "step": 4923
    },
    {
      "epoch": 2.636144578313253,
      "grad_norm": 0.9714420437812805,
      "learning_rate": 2.430406852248394e-05,
      "loss": 0.0422,
      "step": 4924
    },
    {
      "epoch": 2.6366800535475234,
      "grad_norm": 1.5308231115341187,
      "learning_rate": 2.426837972876517e-05,
      "loss": 0.114,
      "step": 4925
    },
    {
      "epoch": 2.6372155287817938,
      "grad_norm": 1.1771701574325562,
      "learning_rate": 2.4232690935046394e-05,
      "loss": 0.0518,
      "step": 4926
    },
    {
      "epoch": 2.637751004016064,
      "grad_norm": 2.094651460647583,
      "learning_rate": 2.4197002141327624e-05,
      "loss": 0.0696,
      "step": 4927
    },
    {
      "epoch": 2.6382864792503344,
      "grad_norm": 1.0934103727340698,
      "learning_rate": 2.416131334760885e-05,
      "loss": 0.0789,
      "step": 4928
    },
    {
      "epoch": 2.638821954484605,
      "grad_norm": 0.9214958548545837,
      "learning_rate": 2.412562455389008e-05,
      "loss": 0.0656,
      "step": 4929
    },
    {
      "epoch": 2.6393574297188755,
      "grad_norm": 1.1765210628509521,
      "learning_rate": 2.4089935760171304e-05,
      "loss": 0.0695,
      "step": 4930
    },
    {
      "epoch": 2.639892904953146,
      "grad_norm": 0.8720719218254089,
      "learning_rate": 2.4054246966452534e-05,
      "loss": 0.0412,
      "step": 4931
    },
    {
      "epoch": 2.640428380187416,
      "grad_norm": 0.9880173206329346,
      "learning_rate": 2.401855817273376e-05,
      "loss": 0.0572,
      "step": 4932
    },
    {
      "epoch": 2.6409638554216865,
      "grad_norm": 1.1939972639083862,
      "learning_rate": 2.398286937901499e-05,
      "loss": 0.0507,
      "step": 4933
    },
    {
      "epoch": 2.6414993306559573,
      "grad_norm": 1.4964476823806763,
      "learning_rate": 2.3947180585296218e-05,
      "loss": 0.0537,
      "step": 4934
    },
    {
      "epoch": 2.6420348058902277,
      "grad_norm": 2.1269514560699463,
      "learning_rate": 2.3911491791577444e-05,
      "loss": 0.0274,
      "step": 4935
    },
    {
      "epoch": 2.642570281124498,
      "grad_norm": 1.3566585779190063,
      "learning_rate": 2.3875802997858674e-05,
      "loss": 0.041,
      "step": 4936
    },
    {
      "epoch": 2.6431057563587683,
      "grad_norm": 2.282038927078247,
      "learning_rate": 2.38401142041399e-05,
      "loss": 0.0694,
      "step": 4937
    },
    {
      "epoch": 2.6436412315930387,
      "grad_norm": 1.0285693407058716,
      "learning_rate": 2.380442541042113e-05,
      "loss": 0.0139,
      "step": 4938
    },
    {
      "epoch": 2.6441767068273094,
      "grad_norm": 1.7744908332824707,
      "learning_rate": 2.3768736616702354e-05,
      "loss": 0.1203,
      "step": 4939
    },
    {
      "epoch": 2.6447121820615798,
      "grad_norm": 1.9663478136062622,
      "learning_rate": 2.3733047822983584e-05,
      "loss": 0.0813,
      "step": 4940
    },
    {
      "epoch": 2.64524765729585,
      "grad_norm": 1.5742493867874146,
      "learning_rate": 2.369735902926481e-05,
      "loss": 0.0511,
      "step": 4941
    },
    {
      "epoch": 2.6457831325301204,
      "grad_norm": 1.3056926727294922,
      "learning_rate": 2.366167023554604e-05,
      "loss": 0.0662,
      "step": 4942
    },
    {
      "epoch": 2.6463186077643908,
      "grad_norm": 1.9134955406188965,
      "learning_rate": 2.3625981441827268e-05,
      "loss": 0.1053,
      "step": 4943
    },
    {
      "epoch": 2.6468540829986615,
      "grad_norm": 1.4559930562973022,
      "learning_rate": 2.3590292648108494e-05,
      "loss": 0.0791,
      "step": 4944
    },
    {
      "epoch": 2.647389558232932,
      "grad_norm": 1.0992035865783691,
      "learning_rate": 2.3554603854389724e-05,
      "loss": 0.0734,
      "step": 4945
    },
    {
      "epoch": 2.647925033467202,
      "grad_norm": 1.5225958824157715,
      "learning_rate": 2.351891506067095e-05,
      "loss": 0.0283,
      "step": 4946
    },
    {
      "epoch": 2.6484605087014725,
      "grad_norm": 2.8385326862335205,
      "learning_rate": 2.3483226266952178e-05,
      "loss": 0.0712,
      "step": 4947
    },
    {
      "epoch": 2.648995983935743,
      "grad_norm": 0.8684457540512085,
      "learning_rate": 2.3447537473233404e-05,
      "loss": 0.0402,
      "step": 4948
    },
    {
      "epoch": 2.6495314591700136,
      "grad_norm": 0.9229487180709839,
      "learning_rate": 2.3411848679514634e-05,
      "loss": 0.0422,
      "step": 4949
    },
    {
      "epoch": 2.6500669344042835,
      "grad_norm": 1.4267171621322632,
      "learning_rate": 2.337615988579586e-05,
      "loss": 0.0663,
      "step": 4950
    },
    {
      "epoch": 2.6506024096385543,
      "grad_norm": 3.0433385372161865,
      "learning_rate": 2.334047109207709e-05,
      "loss": 0.1851,
      "step": 4951
    },
    {
      "epoch": 2.6511378848728246,
      "grad_norm": 3.86690092086792,
      "learning_rate": 2.3304782298358314e-05,
      "loss": 0.0831,
      "step": 4952
    },
    {
      "epoch": 2.651673360107095,
      "grad_norm": 1.3637733459472656,
      "learning_rate": 2.3269093504639544e-05,
      "loss": 0.0849,
      "step": 4953
    },
    {
      "epoch": 2.6522088353413653,
      "grad_norm": 1.8904924392700195,
      "learning_rate": 2.323340471092077e-05,
      "loss": 0.0687,
      "step": 4954
    },
    {
      "epoch": 2.6527443105756356,
      "grad_norm": 0.2407453954219818,
      "learning_rate": 2.3197715917202e-05,
      "loss": 0.0072,
      "step": 4955
    },
    {
      "epoch": 2.6532797858099064,
      "grad_norm": 0.6779350638389587,
      "learning_rate": 2.3162027123483228e-05,
      "loss": 0.0284,
      "step": 4956
    },
    {
      "epoch": 2.6538152610441768,
      "grad_norm": 1.6858967542648315,
      "learning_rate": 2.3126338329764454e-05,
      "loss": 0.0892,
      "step": 4957
    },
    {
      "epoch": 2.654350736278447,
      "grad_norm": 0.9588143825531006,
      "learning_rate": 2.3090649536045684e-05,
      "loss": 0.0646,
      "step": 4958
    },
    {
      "epoch": 2.6548862115127174,
      "grad_norm": 1.0883857011795044,
      "learning_rate": 2.305496074232691e-05,
      "loss": 0.0456,
      "step": 4959
    },
    {
      "epoch": 2.6554216867469878,
      "grad_norm": 0.638276994228363,
      "learning_rate": 2.3019271948608138e-05,
      "loss": 0.0319,
      "step": 4960
    },
    {
      "epoch": 2.6559571619812585,
      "grad_norm": 0.7458294034004211,
      "learning_rate": 2.2983583154889364e-05,
      "loss": 0.025,
      "step": 4961
    },
    {
      "epoch": 2.656492637215529,
      "grad_norm": 3.3841166496276855,
      "learning_rate": 2.2947894361170594e-05,
      "loss": 0.1098,
      "step": 4962
    },
    {
      "epoch": 2.657028112449799,
      "grad_norm": 1.887824535369873,
      "learning_rate": 2.291220556745182e-05,
      "loss": 0.0579,
      "step": 4963
    },
    {
      "epoch": 2.6575635876840695,
      "grad_norm": 1.245115041732788,
      "learning_rate": 2.2876516773733048e-05,
      "loss": 0.1339,
      "step": 4964
    },
    {
      "epoch": 2.65809906291834,
      "grad_norm": 2.2261199951171875,
      "learning_rate": 2.2840827980014274e-05,
      "loss": 0.1456,
      "step": 4965
    },
    {
      "epoch": 2.6586345381526106,
      "grad_norm": 0.19500042498111725,
      "learning_rate": 2.2805139186295504e-05,
      "loss": 0.0067,
      "step": 4966
    },
    {
      "epoch": 2.659170013386881,
      "grad_norm": 1.0963736772537231,
      "learning_rate": 2.276945039257673e-05,
      "loss": 0.0315,
      "step": 4967
    },
    {
      "epoch": 2.6597054886211513,
      "grad_norm": 0.3116297125816345,
      "learning_rate": 2.2733761598857958e-05,
      "loss": 0.0203,
      "step": 4968
    },
    {
      "epoch": 2.6602409638554216,
      "grad_norm": 0.8951917886734009,
      "learning_rate": 2.2698072805139188e-05,
      "loss": 0.0632,
      "step": 4969
    },
    {
      "epoch": 2.660776439089692,
      "grad_norm": 0.9492707252502441,
      "learning_rate": 2.2662384011420414e-05,
      "loss": 0.0414,
      "step": 4970
    },
    {
      "epoch": 2.6613119143239627,
      "grad_norm": 3.323625326156616,
      "learning_rate": 2.2626695217701644e-05,
      "loss": 0.0662,
      "step": 4971
    },
    {
      "epoch": 2.661847389558233,
      "grad_norm": 2.331935405731201,
      "learning_rate": 2.259100642398287e-05,
      "loss": 0.0561,
      "step": 4972
    },
    {
      "epoch": 2.6623828647925034,
      "grad_norm": 1.7284667491912842,
      "learning_rate": 2.2555317630264098e-05,
      "loss": 0.0805,
      "step": 4973
    },
    {
      "epoch": 2.6629183400267737,
      "grad_norm": 0.8579450845718384,
      "learning_rate": 2.2519628836545324e-05,
      "loss": 0.0412,
      "step": 4974
    },
    {
      "epoch": 2.663453815261044,
      "grad_norm": 1.1199674606323242,
      "learning_rate": 2.2483940042826554e-05,
      "loss": 0.0433,
      "step": 4975
    },
    {
      "epoch": 2.663989290495315,
      "grad_norm": 0.6716031432151794,
      "learning_rate": 2.244825124910778e-05,
      "loss": 0.0134,
      "step": 4976
    },
    {
      "epoch": 2.6645247657295847,
      "grad_norm": 0.8922170996665955,
      "learning_rate": 2.2412562455389008e-05,
      "loss": 0.0458,
      "step": 4977
    },
    {
      "epoch": 2.6650602409638555,
      "grad_norm": 1.0649715662002563,
      "learning_rate": 2.2376873661670238e-05,
      "loss": 0.0623,
      "step": 4978
    },
    {
      "epoch": 2.665595716198126,
      "grad_norm": 1.3644630908966064,
      "learning_rate": 2.2341184867951464e-05,
      "loss": 0.0848,
      "step": 4979
    },
    {
      "epoch": 2.666131191432396,
      "grad_norm": 1.326028823852539,
      "learning_rate": 2.2305496074232694e-05,
      "loss": 0.0794,
      "step": 4980
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.10717073827981949,
      "learning_rate": 2.2269807280513918e-05,
      "loss": 0.0042,
      "step": 4981
    },
    {
      "epoch": 2.667202141900937,
      "grad_norm": 1.1928441524505615,
      "learning_rate": 2.2234118486795148e-05,
      "loss": 0.0361,
      "step": 4982
    },
    {
      "epoch": 2.6677376171352076,
      "grad_norm": 0.4394337236881256,
      "learning_rate": 2.2198429693076374e-05,
      "loss": 0.0139,
      "step": 4983
    },
    {
      "epoch": 2.668273092369478,
      "grad_norm": 3.679410934448242,
      "learning_rate": 2.2162740899357604e-05,
      "loss": 0.0839,
      "step": 4984
    },
    {
      "epoch": 2.6688085676037483,
      "grad_norm": 1.6068073511123657,
      "learning_rate": 2.2127052105638828e-05,
      "loss": 0.0449,
      "step": 4985
    },
    {
      "epoch": 2.6693440428380186,
      "grad_norm": 1.3848055601119995,
      "learning_rate": 2.2091363311920058e-05,
      "loss": 0.0682,
      "step": 4986
    },
    {
      "epoch": 2.669879518072289,
      "grad_norm": 0.80722975730896,
      "learning_rate": 2.2055674518201284e-05,
      "loss": 0.0368,
      "step": 4987
    },
    {
      "epoch": 2.6704149933065597,
      "grad_norm": 0.6870614886283875,
      "learning_rate": 2.2019985724482514e-05,
      "loss": 0.0296,
      "step": 4988
    },
    {
      "epoch": 2.67095046854083,
      "grad_norm": 1.796730875968933,
      "learning_rate": 2.198429693076374e-05,
      "loss": 0.0491,
      "step": 4989
    },
    {
      "epoch": 2.6714859437751004,
      "grad_norm": 1.0010044574737549,
      "learning_rate": 2.1948608137044968e-05,
      "loss": 0.0241,
      "step": 4990
    },
    {
      "epoch": 2.6720214190093707,
      "grad_norm": 0.8363053202629089,
      "learning_rate": 2.1912919343326198e-05,
      "loss": 0.0469,
      "step": 4991
    },
    {
      "epoch": 2.672556894243641,
      "grad_norm": 0.3703691363334656,
      "learning_rate": 2.1877230549607424e-05,
      "loss": 0.0144,
      "step": 4992
    },
    {
      "epoch": 2.673092369477912,
      "grad_norm": 1.1167269945144653,
      "learning_rate": 2.1841541755888654e-05,
      "loss": 0.0439,
      "step": 4993
    },
    {
      "epoch": 2.673627844712182,
      "grad_norm": 2.296297311782837,
      "learning_rate": 2.1805852962169878e-05,
      "loss": 0.0779,
      "step": 4994
    },
    {
      "epoch": 2.6741633199464525,
      "grad_norm": 1.8558504581451416,
      "learning_rate": 2.1770164168451108e-05,
      "loss": 0.0865,
      "step": 4995
    },
    {
      "epoch": 2.674698795180723,
      "grad_norm": 0.675095796585083,
      "learning_rate": 2.1734475374732334e-05,
      "loss": 0.0407,
      "step": 4996
    },
    {
      "epoch": 2.675234270414993,
      "grad_norm": 0.7493419051170349,
      "learning_rate": 2.1698786581013564e-05,
      "loss": 0.0305,
      "step": 4997
    },
    {
      "epoch": 2.675769745649264,
      "grad_norm": 2.6089909076690674,
      "learning_rate": 2.1663097787294788e-05,
      "loss": 0.19,
      "step": 4998
    },
    {
      "epoch": 2.6763052208835343,
      "grad_norm": 1.182173728942871,
      "learning_rate": 2.1627408993576018e-05,
      "loss": 0.0557,
      "step": 4999
    },
    {
      "epoch": 2.6768406961178046,
      "grad_norm": 1.6248513460159302,
      "learning_rate": 2.1591720199857244e-05,
      "loss": 0.0953,
      "step": 5000
    },
    {
      "epoch": 2.677376171352075,
      "grad_norm": 0.3403643071651459,
      "learning_rate": 2.1556031406138474e-05,
      "loss": 0.0191,
      "step": 5001
    },
    {
      "epoch": 2.6779116465863453,
      "grad_norm": 1.586291790008545,
      "learning_rate": 2.15203426124197e-05,
      "loss": 0.0862,
      "step": 5002
    },
    {
      "epoch": 2.678447121820616,
      "grad_norm": 1.6436458826065063,
      "learning_rate": 2.1484653818700928e-05,
      "loss": 0.0834,
      "step": 5003
    },
    {
      "epoch": 2.678982597054886,
      "grad_norm": 1.8676693439483643,
      "learning_rate": 2.1448965024982158e-05,
      "loss": 0.1157,
      "step": 5004
    },
    {
      "epoch": 2.6795180722891567,
      "grad_norm": 1.0826834440231323,
      "learning_rate": 2.1413276231263384e-05,
      "loss": 0.046,
      "step": 5005
    },
    {
      "epoch": 2.680053547523427,
      "grad_norm": 1.1342589855194092,
      "learning_rate": 2.1377587437544614e-05,
      "loss": 0.0536,
      "step": 5006
    },
    {
      "epoch": 2.6805890227576974,
      "grad_norm": 1.7261080741882324,
      "learning_rate": 2.1341898643825838e-05,
      "loss": 0.095,
      "step": 5007
    },
    {
      "epoch": 2.6811244979919677,
      "grad_norm": 1.9008113145828247,
      "learning_rate": 2.1306209850107068e-05,
      "loss": 0.093,
      "step": 5008
    },
    {
      "epoch": 2.681659973226238,
      "grad_norm": 0.941470742225647,
      "learning_rate": 2.1270521056388294e-05,
      "loss": 0.0413,
      "step": 5009
    },
    {
      "epoch": 2.682195448460509,
      "grad_norm": 2.0099239349365234,
      "learning_rate": 2.1234832262669524e-05,
      "loss": 0.0859,
      "step": 5010
    },
    {
      "epoch": 2.682730923694779,
      "grad_norm": 1.914099097251892,
      "learning_rate": 2.119914346895075e-05,
      "loss": 0.0957,
      "step": 5011
    },
    {
      "epoch": 2.6832663989290495,
      "grad_norm": 2.2890963554382324,
      "learning_rate": 2.1163454675231978e-05,
      "loss": 0.1607,
      "step": 5012
    },
    {
      "epoch": 2.68380187416332,
      "grad_norm": 1.4573032855987549,
      "learning_rate": 2.1127765881513208e-05,
      "loss": 0.07,
      "step": 5013
    },
    {
      "epoch": 2.68433734939759,
      "grad_norm": 0.9829627275466919,
      "learning_rate": 2.1092077087794434e-05,
      "loss": 0.0694,
      "step": 5014
    },
    {
      "epoch": 2.684872824631861,
      "grad_norm": 1.6362614631652832,
      "learning_rate": 2.105638829407566e-05,
      "loss": 0.0547,
      "step": 5015
    },
    {
      "epoch": 2.6854082998661313,
      "grad_norm": 1.4874650239944458,
      "learning_rate": 2.1020699500356888e-05,
      "loss": 0.077,
      "step": 5016
    },
    {
      "epoch": 2.6859437751004016,
      "grad_norm": 1.1315475702285767,
      "learning_rate": 2.0985010706638118e-05,
      "loss": 0.0535,
      "step": 5017
    },
    {
      "epoch": 2.686479250334672,
      "grad_norm": 0.7974294424057007,
      "learning_rate": 2.0949321912919344e-05,
      "loss": 0.0329,
      "step": 5018
    },
    {
      "epoch": 2.6870147255689423,
      "grad_norm": 1.3629738092422485,
      "learning_rate": 2.091363311920057e-05,
      "loss": 0.0706,
      "step": 5019
    },
    {
      "epoch": 2.687550200803213,
      "grad_norm": 1.5388959646224976,
      "learning_rate": 2.0877944325481798e-05,
      "loss": 0.0606,
      "step": 5020
    },
    {
      "epoch": 2.6880856760374834,
      "grad_norm": 4.1255035400390625,
      "learning_rate": 2.0842255531763028e-05,
      "loss": 0.1315,
      "step": 5021
    },
    {
      "epoch": 2.6886211512717537,
      "grad_norm": 6.800953388214111,
      "learning_rate": 2.0806566738044254e-05,
      "loss": 0.03,
      "step": 5022
    },
    {
      "epoch": 2.689156626506024,
      "grad_norm": 0.904292643070221,
      "learning_rate": 2.0770877944325484e-05,
      "loss": 0.04,
      "step": 5023
    },
    {
      "epoch": 2.6896921017402944,
      "grad_norm": 1.9880340099334717,
      "learning_rate": 2.073518915060671e-05,
      "loss": 0.1034,
      "step": 5024
    },
    {
      "epoch": 2.690227576974565,
      "grad_norm": 1.2074329853057861,
      "learning_rate": 2.0699500356887938e-05,
      "loss": 0.0638,
      "step": 5025
    },
    {
      "epoch": 2.6907630522088355,
      "grad_norm": 2.9666051864624023,
      "learning_rate": 2.0663811563169168e-05,
      "loss": 0.161,
      "step": 5026
    },
    {
      "epoch": 2.691298527443106,
      "grad_norm": 1.1512795686721802,
      "learning_rate": 2.0628122769450394e-05,
      "loss": 0.0469,
      "step": 5027
    },
    {
      "epoch": 2.691834002677376,
      "grad_norm": 0.7637521028518677,
      "learning_rate": 2.059243397573162e-05,
      "loss": 0.0257,
      "step": 5028
    },
    {
      "epoch": 2.6923694779116465,
      "grad_norm": 1.2637267112731934,
      "learning_rate": 2.0556745182012848e-05,
      "loss": 0.0569,
      "step": 5029
    },
    {
      "epoch": 2.6929049531459173,
      "grad_norm": 1.4981685876846313,
      "learning_rate": 2.0521056388294078e-05,
      "loss": 0.0254,
      "step": 5030
    },
    {
      "epoch": 2.693440428380187,
      "grad_norm": 4.114391326904297,
      "learning_rate": 2.0485367594575304e-05,
      "loss": 0.098,
      "step": 5031
    },
    {
      "epoch": 2.693975903614458,
      "grad_norm": 2.4608118534088135,
      "learning_rate": 2.044967880085653e-05,
      "loss": 0.1796,
      "step": 5032
    },
    {
      "epoch": 2.6945113788487283,
      "grad_norm": 1.9278658628463745,
      "learning_rate": 2.041399000713776e-05,
      "loss": 0.0563,
      "step": 5033
    },
    {
      "epoch": 2.6950468540829986,
      "grad_norm": 4.384239196777344,
      "learning_rate": 2.0378301213418988e-05,
      "loss": 0.0968,
      "step": 5034
    },
    {
      "epoch": 2.695582329317269,
      "grad_norm": 0.7178875803947449,
      "learning_rate": 2.0342612419700218e-05,
      "loss": 0.0292,
      "step": 5035
    },
    {
      "epoch": 2.6961178045515393,
      "grad_norm": 1.0925506353378296,
      "learning_rate": 2.030692362598144e-05,
      "loss": 0.0491,
      "step": 5036
    },
    {
      "epoch": 2.69665327978581,
      "grad_norm": 1.7516485452651978,
      "learning_rate": 2.027123483226267e-05,
      "loss": 0.0609,
      "step": 5037
    },
    {
      "epoch": 2.6971887550200804,
      "grad_norm": 2.0499486923217773,
      "learning_rate": 2.0235546038543898e-05,
      "loss": 0.0858,
      "step": 5038
    },
    {
      "epoch": 2.6977242302543507,
      "grad_norm": 1.5547720193862915,
      "learning_rate": 2.0199857244825128e-05,
      "loss": 0.0968,
      "step": 5039
    },
    {
      "epoch": 2.698259705488621,
      "grad_norm": 0.9169367551803589,
      "learning_rate": 2.016416845110635e-05,
      "loss": 0.0696,
      "step": 5040
    },
    {
      "epoch": 2.6987951807228914,
      "grad_norm": 1.5665847063064575,
      "learning_rate": 2.012847965738758e-05,
      "loss": 0.0696,
      "step": 5041
    },
    {
      "epoch": 2.699330655957162,
      "grad_norm": 3.6055657863616943,
      "learning_rate": 2.0092790863668808e-05,
      "loss": 0.0525,
      "step": 5042
    },
    {
      "epoch": 2.6998661311914325,
      "grad_norm": 3.6701393127441406,
      "learning_rate": 2.0057102069950038e-05,
      "loss": 0.0437,
      "step": 5043
    },
    {
      "epoch": 2.700401606425703,
      "grad_norm": 0.9361634254455566,
      "learning_rate": 2.0021413276231264e-05,
      "loss": 0.0318,
      "step": 5044
    },
    {
      "epoch": 2.700937081659973,
      "grad_norm": 1.9642094373703003,
      "learning_rate": 1.998572448251249e-05,
      "loss": 0.0676,
      "step": 5045
    },
    {
      "epoch": 2.7014725568942435,
      "grad_norm": 1.705785870552063,
      "learning_rate": 1.995003568879372e-05,
      "loss": 0.0724,
      "step": 5046
    },
    {
      "epoch": 2.7020080321285143,
      "grad_norm": 1.120564579963684,
      "learning_rate": 1.9914346895074948e-05,
      "loss": 0.0585,
      "step": 5047
    },
    {
      "epoch": 2.7025435073627846,
      "grad_norm": 1.302770972251892,
      "learning_rate": 1.9878658101356178e-05,
      "loss": 0.0497,
      "step": 5048
    },
    {
      "epoch": 2.703078982597055,
      "grad_norm": 3.2011168003082275,
      "learning_rate": 1.98429693076374e-05,
      "loss": 0.2484,
      "step": 5049
    },
    {
      "epoch": 2.7036144578313253,
      "grad_norm": 0.9696494340896606,
      "learning_rate": 1.980728051391863e-05,
      "loss": 0.0258,
      "step": 5050
    },
    {
      "epoch": 2.7041499330655956,
      "grad_norm": 0.770667552947998,
      "learning_rate": 1.9771591720199858e-05,
      "loss": 0.0434,
      "step": 5051
    },
    {
      "epoch": 2.7046854082998664,
      "grad_norm": 0.35097572207450867,
      "learning_rate": 1.9735902926481088e-05,
      "loss": 0.0318,
      "step": 5052
    },
    {
      "epoch": 2.7052208835341367,
      "grad_norm": 1.5746326446533203,
      "learning_rate": 1.970021413276231e-05,
      "loss": 0.0648,
      "step": 5053
    },
    {
      "epoch": 2.705756358768407,
      "grad_norm": 1.3288092613220215,
      "learning_rate": 1.966452533904354e-05,
      "loss": 0.0349,
      "step": 5054
    },
    {
      "epoch": 2.7062918340026774,
      "grad_norm": 1.6788161993026733,
      "learning_rate": 1.9628836545324768e-05,
      "loss": 0.0571,
      "step": 5055
    },
    {
      "epoch": 2.7068273092369477,
      "grad_norm": 0.7481243014335632,
      "learning_rate": 1.9593147751605998e-05,
      "loss": 0.0283,
      "step": 5056
    },
    {
      "epoch": 2.7073627844712185,
      "grad_norm": 1.3110246658325195,
      "learning_rate": 1.9557458957887224e-05,
      "loss": 0.0845,
      "step": 5057
    },
    {
      "epoch": 2.7078982597054884,
      "grad_norm": 0.3669605255126953,
      "learning_rate": 1.952177016416845e-05,
      "loss": 0.013,
      "step": 5058
    },
    {
      "epoch": 2.708433734939759,
      "grad_norm": 1.8061583042144775,
      "learning_rate": 1.948608137044968e-05,
      "loss": 0.0557,
      "step": 5059
    },
    {
      "epoch": 2.7089692101740295,
      "grad_norm": 3.1617119312286377,
      "learning_rate": 1.9450392576730908e-05,
      "loss": 0.1135,
      "step": 5060
    },
    {
      "epoch": 2.7095046854083,
      "grad_norm": 2.4227607250213623,
      "learning_rate": 1.9414703783012138e-05,
      "loss": 0.0595,
      "step": 5061
    },
    {
      "epoch": 2.71004016064257,
      "grad_norm": 0.45474278926849365,
      "learning_rate": 1.937901498929336e-05,
      "loss": 0.015,
      "step": 5062
    },
    {
      "epoch": 2.7105756358768405,
      "grad_norm": 0.879915714263916,
      "learning_rate": 1.934332619557459e-05,
      "loss": 0.0489,
      "step": 5063
    },
    {
      "epoch": 2.7111111111111112,
      "grad_norm": 4.338885307312012,
      "learning_rate": 1.9307637401855818e-05,
      "loss": 0.1292,
      "step": 5064
    },
    {
      "epoch": 2.7116465863453816,
      "grad_norm": 1.853368878364563,
      "learning_rate": 1.9271948608137048e-05,
      "loss": 0.0417,
      "step": 5065
    },
    {
      "epoch": 2.712182061579652,
      "grad_norm": 3.220048666000366,
      "learning_rate": 1.9236259814418274e-05,
      "loss": 0.15,
      "step": 5066
    },
    {
      "epoch": 2.7127175368139222,
      "grad_norm": 1.595977544784546,
      "learning_rate": 1.92005710206995e-05,
      "loss": 0.1025,
      "step": 5067
    },
    {
      "epoch": 2.7132530120481926,
      "grad_norm": 1.8972573280334473,
      "learning_rate": 1.916488222698073e-05,
      "loss": 0.0843,
      "step": 5068
    },
    {
      "epoch": 2.7137884872824634,
      "grad_norm": 0.6098814606666565,
      "learning_rate": 1.9129193433261958e-05,
      "loss": 0.0158,
      "step": 5069
    },
    {
      "epoch": 2.7143239625167337,
      "grad_norm": 2.0075371265411377,
      "learning_rate": 1.9093504639543184e-05,
      "loss": 0.1082,
      "step": 5070
    },
    {
      "epoch": 2.714859437751004,
      "grad_norm": 1.2284879684448242,
      "learning_rate": 1.905781584582441e-05,
      "loss": 0.0427,
      "step": 5071
    },
    {
      "epoch": 2.7153949129852744,
      "grad_norm": 1.113661289215088,
      "learning_rate": 1.902212705210564e-05,
      "loss": 0.0553,
      "step": 5072
    },
    {
      "epoch": 2.7159303882195447,
      "grad_norm": 0.9843162894248962,
      "learning_rate": 1.8986438258386868e-05,
      "loss": 0.0449,
      "step": 5073
    },
    {
      "epoch": 2.7164658634538155,
      "grad_norm": 2.973339080810547,
      "learning_rate": 1.8950749464668094e-05,
      "loss": 0.0983,
      "step": 5074
    },
    {
      "epoch": 2.717001338688086,
      "grad_norm": 0.7078641057014465,
      "learning_rate": 1.891506067094932e-05,
      "loss": 0.0208,
      "step": 5075
    },
    {
      "epoch": 2.717536813922356,
      "grad_norm": 0.16042305529117584,
      "learning_rate": 1.887937187723055e-05,
      "loss": 0.0059,
      "step": 5076
    },
    {
      "epoch": 2.7180722891566265,
      "grad_norm": 1.3704122304916382,
      "learning_rate": 1.8843683083511778e-05,
      "loss": 0.0315,
      "step": 5077
    },
    {
      "epoch": 2.718607764390897,
      "grad_norm": 4.602728366851807,
      "learning_rate": 1.8807994289793008e-05,
      "loss": 0.0653,
      "step": 5078
    },
    {
      "epoch": 2.7191432396251676,
      "grad_norm": 1.0029945373535156,
      "learning_rate": 1.8772305496074234e-05,
      "loss": 0.0468,
      "step": 5079
    },
    {
      "epoch": 2.719678714859438,
      "grad_norm": 0.25326481461524963,
      "learning_rate": 1.873661670235546e-05,
      "loss": 0.0108,
      "step": 5080
    },
    {
      "epoch": 2.7202141900937082,
      "grad_norm": 1.2808700799942017,
      "learning_rate": 1.870092790863669e-05,
      "loss": 0.0562,
      "step": 5081
    },
    {
      "epoch": 2.7207496653279786,
      "grad_norm": 2.223121166229248,
      "learning_rate": 1.8665239114917918e-05,
      "loss": 0.0251,
      "step": 5082
    },
    {
      "epoch": 2.721285140562249,
      "grad_norm": 2.0863101482391357,
      "learning_rate": 1.8629550321199144e-05,
      "loss": 0.0834,
      "step": 5083
    },
    {
      "epoch": 2.7218206157965197,
      "grad_norm": 2.3413357734680176,
      "learning_rate": 1.859386152748037e-05,
      "loss": 0.0867,
      "step": 5084
    },
    {
      "epoch": 2.7223560910307896,
      "grad_norm": 1.5008529424667358,
      "learning_rate": 1.85581727337616e-05,
      "loss": 0.0191,
      "step": 5085
    },
    {
      "epoch": 2.7228915662650603,
      "grad_norm": 0.23671099543571472,
      "learning_rate": 1.8522483940042828e-05,
      "loss": 0.0117,
      "step": 5086
    },
    {
      "epoch": 2.7234270414993307,
      "grad_norm": 1.902098536491394,
      "learning_rate": 1.8486795146324054e-05,
      "loss": 0.0982,
      "step": 5087
    },
    {
      "epoch": 2.723962516733601,
      "grad_norm": 0.5378478169441223,
      "learning_rate": 1.845110635260528e-05,
      "loss": 0.0214,
      "step": 5088
    },
    {
      "epoch": 2.7244979919678713,
      "grad_norm": 0.7372868061065674,
      "learning_rate": 1.841541755888651e-05,
      "loss": 0.0134,
      "step": 5089
    },
    {
      "epoch": 2.7250334672021417,
      "grad_norm": 1.660624623298645,
      "learning_rate": 1.8379728765167738e-05,
      "loss": 0.0993,
      "step": 5090
    },
    {
      "epoch": 2.7255689424364125,
      "grad_norm": 0.5625735521316528,
      "learning_rate": 1.8344039971448964e-05,
      "loss": 0.0416,
      "step": 5091
    },
    {
      "epoch": 2.726104417670683,
      "grad_norm": 0.6816172003746033,
      "learning_rate": 1.8308351177730194e-05,
      "loss": 0.0249,
      "step": 5092
    },
    {
      "epoch": 2.726639892904953,
      "grad_norm": 0.9377294778823853,
      "learning_rate": 1.827266238401142e-05,
      "loss": 0.033,
      "step": 5093
    },
    {
      "epoch": 2.7271753681392235,
      "grad_norm": 1.2366422414779663,
      "learning_rate": 1.823697359029265e-05,
      "loss": 0.0627,
      "step": 5094
    },
    {
      "epoch": 2.727710843373494,
      "grad_norm": 1.5128793716430664,
      "learning_rate": 1.8201284796573874e-05,
      "loss": 0.0554,
      "step": 5095
    },
    {
      "epoch": 2.7282463186077646,
      "grad_norm": 2.9021098613739014,
      "learning_rate": 1.8165596002855104e-05,
      "loss": 0.0684,
      "step": 5096
    },
    {
      "epoch": 2.728781793842035,
      "grad_norm": 2.632058620452881,
      "learning_rate": 1.812990720913633e-05,
      "loss": 0.0408,
      "step": 5097
    },
    {
      "epoch": 2.7293172690763052,
      "grad_norm": 1.0916039943695068,
      "learning_rate": 1.809421841541756e-05,
      "loss": 0.0209,
      "step": 5098
    },
    {
      "epoch": 2.7298527443105756,
      "grad_norm": 1.9931585788726807,
      "learning_rate": 1.8058529621698788e-05,
      "loss": 0.0948,
      "step": 5099
    },
    {
      "epoch": 2.730388219544846,
      "grad_norm": 1.288933277130127,
      "learning_rate": 1.8022840827980014e-05,
      "loss": 0.0709,
      "step": 5100
    },
    {
      "epoch": 2.7309236947791167,
      "grad_norm": 0.468001127243042,
      "learning_rate": 1.7987152034261244e-05,
      "loss": 0.0241,
      "step": 5101
    },
    {
      "epoch": 2.731459170013387,
      "grad_norm": 5.864864826202393,
      "learning_rate": 1.795146324054247e-05,
      "loss": 0.06,
      "step": 5102
    },
    {
      "epoch": 2.7319946452476573,
      "grad_norm": 4.023240089416504,
      "learning_rate": 1.79157744468237e-05,
      "loss": 0.1282,
      "step": 5103
    },
    {
      "epoch": 2.7325301204819277,
      "grad_norm": 2.1843316555023193,
      "learning_rate": 1.7880085653104924e-05,
      "loss": 0.0804,
      "step": 5104
    },
    {
      "epoch": 2.733065595716198,
      "grad_norm": 1.5176758766174316,
      "learning_rate": 1.7844396859386154e-05,
      "loss": 0.0528,
      "step": 5105
    },
    {
      "epoch": 2.733601070950469,
      "grad_norm": 0.8769299387931824,
      "learning_rate": 1.780870806566738e-05,
      "loss": 0.0614,
      "step": 5106
    },
    {
      "epoch": 2.734136546184739,
      "grad_norm": 1.2648390531539917,
      "learning_rate": 1.777301927194861e-05,
      "loss": 0.089,
      "step": 5107
    },
    {
      "epoch": 2.7346720214190094,
      "grad_norm": 1.4472854137420654,
      "learning_rate": 1.7737330478229834e-05,
      "loss": 0.0998,
      "step": 5108
    },
    {
      "epoch": 2.73520749665328,
      "grad_norm": 2.2822351455688477,
      "learning_rate": 1.7701641684511064e-05,
      "loss": 0.0813,
      "step": 5109
    },
    {
      "epoch": 2.73574297188755,
      "grad_norm": 1.6837663650512695,
      "learning_rate": 1.766595289079229e-05,
      "loss": 0.0457,
      "step": 5110
    },
    {
      "epoch": 2.736278447121821,
      "grad_norm": 1.4128563404083252,
      "learning_rate": 1.763026409707352e-05,
      "loss": 0.0603,
      "step": 5111
    },
    {
      "epoch": 2.736813922356091,
      "grad_norm": 1.5763705968856812,
      "learning_rate": 1.7594575303354748e-05,
      "loss": 0.0874,
      "step": 5112
    },
    {
      "epoch": 2.7373493975903616,
      "grad_norm": 5.483240127563477,
      "learning_rate": 1.7558886509635974e-05,
      "loss": 0.0735,
      "step": 5113
    },
    {
      "epoch": 2.737884872824632,
      "grad_norm": 0.9944190382957458,
      "learning_rate": 1.7523197715917204e-05,
      "loss": 0.0464,
      "step": 5114
    },
    {
      "epoch": 2.738420348058902,
      "grad_norm": 1.429019808769226,
      "learning_rate": 1.748750892219843e-05,
      "loss": 0.0841,
      "step": 5115
    },
    {
      "epoch": 2.7389558232931726,
      "grad_norm": 1.0275341272354126,
      "learning_rate": 1.745182012847966e-05,
      "loss": 0.0282,
      "step": 5116
    },
    {
      "epoch": 2.739491298527443,
      "grad_norm": 1.1389951705932617,
      "learning_rate": 1.7416131334760884e-05,
      "loss": 0.0388,
      "step": 5117
    },
    {
      "epoch": 2.7400267737617137,
      "grad_norm": 1.2401111125946045,
      "learning_rate": 1.7380442541042114e-05,
      "loss": 0.0866,
      "step": 5118
    },
    {
      "epoch": 2.740562248995984,
      "grad_norm": 2.294931650161743,
      "learning_rate": 1.734475374732334e-05,
      "loss": 0.0322,
      "step": 5119
    },
    {
      "epoch": 2.7410977242302543,
      "grad_norm": 1.0763169527053833,
      "learning_rate": 1.730906495360457e-05,
      "loss": 0.043,
      "step": 5120
    },
    {
      "epoch": 2.7416331994645247,
      "grad_norm": 1.4551138877868652,
      "learning_rate": 1.7273376159885794e-05,
      "loss": 0.0927,
      "step": 5121
    },
    {
      "epoch": 2.742168674698795,
      "grad_norm": 1.3945915699005127,
      "learning_rate": 1.7237687366167024e-05,
      "loss": 0.0854,
      "step": 5122
    },
    {
      "epoch": 2.7427041499330658,
      "grad_norm": 1.0185673236846924,
      "learning_rate": 1.720199857244825e-05,
      "loss": 0.0672,
      "step": 5123
    },
    {
      "epoch": 2.743239625167336,
      "grad_norm": 1.1950523853302002,
      "learning_rate": 1.716630977872948e-05,
      "loss": 0.0237,
      "step": 5124
    },
    {
      "epoch": 2.7437751004016064,
      "grad_norm": 1.3796664476394653,
      "learning_rate": 1.7130620985010707e-05,
      "loss": 0.0386,
      "step": 5125
    },
    {
      "epoch": 2.7443105756358768,
      "grad_norm": 0.4087774157524109,
      "learning_rate": 1.7094932191291934e-05,
      "loss": 0.0177,
      "step": 5126
    },
    {
      "epoch": 2.744846050870147,
      "grad_norm": 0.7448211908340454,
      "learning_rate": 1.7059243397573164e-05,
      "loss": 0.0195,
      "step": 5127
    },
    {
      "epoch": 2.745381526104418,
      "grad_norm": 0.8421088457107544,
      "learning_rate": 1.702355460385439e-05,
      "loss": 0.0222,
      "step": 5128
    },
    {
      "epoch": 2.745917001338688,
      "grad_norm": 1.505939245223999,
      "learning_rate": 1.6987865810135617e-05,
      "loss": 0.0926,
      "step": 5129
    },
    {
      "epoch": 2.7464524765729585,
      "grad_norm": 0.8191454410552979,
      "learning_rate": 1.6952177016416844e-05,
      "loss": 0.0485,
      "step": 5130
    },
    {
      "epoch": 2.746987951807229,
      "grad_norm": 1.0016676187515259,
      "learning_rate": 1.6916488222698074e-05,
      "loss": 0.0477,
      "step": 5131
    },
    {
      "epoch": 2.747523427041499,
      "grad_norm": 4.774958610534668,
      "learning_rate": 1.68807994289793e-05,
      "loss": 0.1705,
      "step": 5132
    },
    {
      "epoch": 2.74805890227577,
      "grad_norm": 2.861633062362671,
      "learning_rate": 1.684511063526053e-05,
      "loss": 0.0458,
      "step": 5133
    },
    {
      "epoch": 2.7485943775100403,
      "grad_norm": 2.357447624206543,
      "learning_rate": 1.6809421841541757e-05,
      "loss": 0.0552,
      "step": 5134
    },
    {
      "epoch": 2.7491298527443107,
      "grad_norm": 0.7682479023933411,
      "learning_rate": 1.6773733047822984e-05,
      "loss": 0.0261,
      "step": 5135
    },
    {
      "epoch": 2.749665327978581,
      "grad_norm": 1.5048772096633911,
      "learning_rate": 1.6738044254104214e-05,
      "loss": 0.0572,
      "step": 5136
    },
    {
      "epoch": 2.7502008032128513,
      "grad_norm": 1.199131727218628,
      "learning_rate": 1.670235546038544e-05,
      "loss": 0.0352,
      "step": 5137
    },
    {
      "epoch": 2.750736278447122,
      "grad_norm": 0.6826725602149963,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.0273,
      "step": 5138
    },
    {
      "epoch": 2.751271753681392,
      "grad_norm": 1.2917678356170654,
      "learning_rate": 1.6630977872947894e-05,
      "loss": 0.0599,
      "step": 5139
    },
    {
      "epoch": 2.7518072289156628,
      "grad_norm": 4.851790904998779,
      "learning_rate": 1.6595289079229124e-05,
      "loss": 0.1182,
      "step": 5140
    },
    {
      "epoch": 2.752342704149933,
      "grad_norm": 2.5491108894348145,
      "learning_rate": 1.655960028551035e-05,
      "loss": 0.058,
      "step": 5141
    },
    {
      "epoch": 2.7528781793842034,
      "grad_norm": 1.471843957901001,
      "learning_rate": 1.6523911491791577e-05,
      "loss": 0.0587,
      "step": 5142
    },
    {
      "epoch": 2.7534136546184738,
      "grad_norm": 0.9105545282363892,
      "learning_rate": 1.6488222698072804e-05,
      "loss": 0.0282,
      "step": 5143
    },
    {
      "epoch": 2.753949129852744,
      "grad_norm": 1.2675875425338745,
      "learning_rate": 1.6452533904354034e-05,
      "loss": 0.0663,
      "step": 5144
    },
    {
      "epoch": 2.754484605087015,
      "grad_norm": 1.1327170133590698,
      "learning_rate": 1.641684511063526e-05,
      "loss": 0.0396,
      "step": 5145
    },
    {
      "epoch": 2.755020080321285,
      "grad_norm": 1.2209347486495972,
      "learning_rate": 1.6381156316916487e-05,
      "loss": 0.0367,
      "step": 5146
    },
    {
      "epoch": 2.7555555555555555,
      "grad_norm": 6.765260696411133,
      "learning_rate": 1.6345467523197717e-05,
      "loss": 0.1364,
      "step": 5147
    },
    {
      "epoch": 2.756091030789826,
      "grad_norm": 1.3974393606185913,
      "learning_rate": 1.6309778729478944e-05,
      "loss": 0.0734,
      "step": 5148
    },
    {
      "epoch": 2.756626506024096,
      "grad_norm": 1.4661020040512085,
      "learning_rate": 1.6274089935760174e-05,
      "loss": 0.0833,
      "step": 5149
    },
    {
      "epoch": 2.757161981258367,
      "grad_norm": 6.377071380615234,
      "learning_rate": 1.6238401142041397e-05,
      "loss": 0.0666,
      "step": 5150
    },
    {
      "epoch": 2.7576974564926373,
      "grad_norm": 3.153137445449829,
      "learning_rate": 1.6202712348322627e-05,
      "loss": 0.0743,
      "step": 5151
    },
    {
      "epoch": 2.7582329317269076,
      "grad_norm": 2.8634164333343506,
      "learning_rate": 1.6167023554603854e-05,
      "loss": 0.0817,
      "step": 5152
    },
    {
      "epoch": 2.758768406961178,
      "grad_norm": 0.8671584725379944,
      "learning_rate": 1.6131334760885084e-05,
      "loss": 0.0626,
      "step": 5153
    },
    {
      "epoch": 2.7593038821954483,
      "grad_norm": 1.6477108001708984,
      "learning_rate": 1.609564596716631e-05,
      "loss": 0.0882,
      "step": 5154
    },
    {
      "epoch": 2.759839357429719,
      "grad_norm": 1.1771564483642578,
      "learning_rate": 1.6059957173447537e-05,
      "loss": 0.041,
      "step": 5155
    },
    {
      "epoch": 2.7603748326639894,
      "grad_norm": 2.9950315952301025,
      "learning_rate": 1.6024268379728764e-05,
      "loss": 0.0668,
      "step": 5156
    },
    {
      "epoch": 2.7609103078982598,
      "grad_norm": 3.093690872192383,
      "learning_rate": 1.5988579586009994e-05,
      "loss": 0.1462,
      "step": 5157
    },
    {
      "epoch": 2.76144578313253,
      "grad_norm": 2.058516502380371,
      "learning_rate": 1.595289079229122e-05,
      "loss": 0.0179,
      "step": 5158
    },
    {
      "epoch": 2.7619812583668004,
      "grad_norm": 1.763724684715271,
      "learning_rate": 1.5917201998572447e-05,
      "loss": 0.0972,
      "step": 5159
    },
    {
      "epoch": 2.762516733601071,
      "grad_norm": 2.9862828254699707,
      "learning_rate": 1.5881513204853677e-05,
      "loss": 0.0621,
      "step": 5160
    },
    {
      "epoch": 2.7630522088353415,
      "grad_norm": 1.3490393161773682,
      "learning_rate": 1.5845824411134904e-05,
      "loss": 0.0688,
      "step": 5161
    },
    {
      "epoch": 2.763587684069612,
      "grad_norm": 1.2367610931396484,
      "learning_rate": 1.5810135617416134e-05,
      "loss": 0.0241,
      "step": 5162
    },
    {
      "epoch": 2.764123159303882,
      "grad_norm": 1.6975585222244263,
      "learning_rate": 1.5774446823697357e-05,
      "loss": 0.0627,
      "step": 5163
    },
    {
      "epoch": 2.7646586345381525,
      "grad_norm": 1.1445342302322388,
      "learning_rate": 1.5738758029978587e-05,
      "loss": 0.0609,
      "step": 5164
    },
    {
      "epoch": 2.765194109772423,
      "grad_norm": 0.37573277950286865,
      "learning_rate": 1.5703069236259814e-05,
      "loss": 0.0082,
      "step": 5165
    },
    {
      "epoch": 2.765729585006693,
      "grad_norm": 0.6631205081939697,
      "learning_rate": 1.5667380442541044e-05,
      "loss": 0.021,
      "step": 5166
    },
    {
      "epoch": 2.766265060240964,
      "grad_norm": 1.8849287033081055,
      "learning_rate": 1.563169164882227e-05,
      "loss": 0.0307,
      "step": 5167
    },
    {
      "epoch": 2.7668005354752343,
      "grad_norm": 0.8862694501876831,
      "learning_rate": 1.5596002855103497e-05,
      "loss": 0.0263,
      "step": 5168
    },
    {
      "epoch": 2.7673360107095046,
      "grad_norm": 3.4440131187438965,
      "learning_rate": 1.5560314061384727e-05,
      "loss": 0.1506,
      "step": 5169
    },
    {
      "epoch": 2.767871485943775,
      "grad_norm": 0.6925809979438782,
      "learning_rate": 1.5524625267665954e-05,
      "loss": 0.0383,
      "step": 5170
    },
    {
      "epoch": 2.7684069611780453,
      "grad_norm": 2.964644432067871,
      "learning_rate": 1.5488936473947184e-05,
      "loss": 0.108,
      "step": 5171
    },
    {
      "epoch": 2.768942436412316,
      "grad_norm": 1.4187240600585938,
      "learning_rate": 1.5453247680228407e-05,
      "loss": 0.0622,
      "step": 5172
    },
    {
      "epoch": 2.7694779116465864,
      "grad_norm": 3.708587884902954,
      "learning_rate": 1.5417558886509637e-05,
      "loss": 0.0853,
      "step": 5173
    },
    {
      "epoch": 2.7700133868808567,
      "grad_norm": 3.2410619258880615,
      "learning_rate": 1.5381870092790864e-05,
      "loss": 0.1497,
      "step": 5174
    },
    {
      "epoch": 2.770548862115127,
      "grad_norm": 2.754944324493408,
      "learning_rate": 1.5346181299072094e-05,
      "loss": 0.0543,
      "step": 5175
    },
    {
      "epoch": 2.7710843373493974,
      "grad_norm": 1.401227355003357,
      "learning_rate": 1.5310492505353317e-05,
      "loss": 0.0374,
      "step": 5176
    },
    {
      "epoch": 2.771619812583668,
      "grad_norm": 8.497902870178223,
      "learning_rate": 1.5274803711634547e-05,
      "loss": 0.1498,
      "step": 5177
    },
    {
      "epoch": 2.7721552878179385,
      "grad_norm": 1.1565948724746704,
      "learning_rate": 1.5239114917915776e-05,
      "loss": 0.0639,
      "step": 5178
    },
    {
      "epoch": 2.772690763052209,
      "grad_norm": 0.7838717699050903,
      "learning_rate": 1.5203426124197004e-05,
      "loss": 0.042,
      "step": 5179
    },
    {
      "epoch": 2.773226238286479,
      "grad_norm": 1.0783681869506836,
      "learning_rate": 1.5167737330478229e-05,
      "loss": 0.0579,
      "step": 5180
    },
    {
      "epoch": 2.7737617135207495,
      "grad_norm": 1.7044239044189453,
      "learning_rate": 1.5132048536759457e-05,
      "loss": 0.0578,
      "step": 5181
    },
    {
      "epoch": 2.7742971887550203,
      "grad_norm": 1.1470969915390015,
      "learning_rate": 1.5096359743040686e-05,
      "loss": 0.0662,
      "step": 5182
    },
    {
      "epoch": 2.7748326639892906,
      "grad_norm": 1.1572060585021973,
      "learning_rate": 1.5060670949321914e-05,
      "loss": 0.0615,
      "step": 5183
    },
    {
      "epoch": 2.775368139223561,
      "grad_norm": 1.3270978927612305,
      "learning_rate": 1.502498215560314e-05,
      "loss": 0.057,
      "step": 5184
    },
    {
      "epoch": 2.7759036144578313,
      "grad_norm": 3.471491813659668,
      "learning_rate": 1.4989293361884369e-05,
      "loss": 0.1227,
      "step": 5185
    },
    {
      "epoch": 2.7764390896921016,
      "grad_norm": 2.3979458808898926,
      "learning_rate": 1.4953604568165597e-05,
      "loss": 0.1088,
      "step": 5186
    },
    {
      "epoch": 2.7769745649263724,
      "grad_norm": 3.0652740001678467,
      "learning_rate": 1.4917915774446826e-05,
      "loss": 0.0607,
      "step": 5187
    },
    {
      "epoch": 2.7775100401606427,
      "grad_norm": 0.5010340213775635,
      "learning_rate": 1.4882226980728054e-05,
      "loss": 0.0141,
      "step": 5188
    },
    {
      "epoch": 2.778045515394913,
      "grad_norm": 2.499197006225586,
      "learning_rate": 1.4846538187009279e-05,
      "loss": 0.0583,
      "step": 5189
    },
    {
      "epoch": 2.7785809906291834,
      "grad_norm": 0.9206989407539368,
      "learning_rate": 1.4810849393290507e-05,
      "loss": 0.0765,
      "step": 5190
    },
    {
      "epoch": 2.7791164658634537,
      "grad_norm": 2.363337993621826,
      "learning_rate": 1.4775160599571736e-05,
      "loss": 0.0235,
      "step": 5191
    },
    {
      "epoch": 2.779651941097724,
      "grad_norm": 1.6418354511260986,
      "learning_rate": 1.4739471805852964e-05,
      "loss": 0.0612,
      "step": 5192
    },
    {
      "epoch": 2.7801874163319944,
      "grad_norm": 0.8829066753387451,
      "learning_rate": 1.4703783012134189e-05,
      "loss": 0.018,
      "step": 5193
    },
    {
      "epoch": 2.780722891566265,
      "grad_norm": 0.6560011506080627,
      "learning_rate": 1.4668094218415417e-05,
      "loss": 0.0468,
      "step": 5194
    },
    {
      "epoch": 2.7812583668005355,
      "grad_norm": 1.160601019859314,
      "learning_rate": 1.4632405424696646e-05,
      "loss": 0.0487,
      "step": 5195
    },
    {
      "epoch": 2.781793842034806,
      "grad_norm": 3.5377213954925537,
      "learning_rate": 1.4596716630977874e-05,
      "loss": 0.1394,
      "step": 5196
    },
    {
      "epoch": 2.782329317269076,
      "grad_norm": 0.7606512308120728,
      "learning_rate": 1.45610278372591e-05,
      "loss": 0.022,
      "step": 5197
    },
    {
      "epoch": 2.7828647925033465,
      "grad_norm": 0.294575035572052,
      "learning_rate": 1.4525339043540329e-05,
      "loss": 0.0084,
      "step": 5198
    },
    {
      "epoch": 2.7834002677376173,
      "grad_norm": 0.8029559850692749,
      "learning_rate": 1.4489650249821557e-05,
      "loss": 0.06,
      "step": 5199
    },
    {
      "epoch": 2.7839357429718876,
      "grad_norm": 2.334380865097046,
      "learning_rate": 1.4453961456102786e-05,
      "loss": 0.0892,
      "step": 5200
    },
    {
      "epoch": 2.784471218206158,
      "grad_norm": 1.159845232963562,
      "learning_rate": 1.441827266238401e-05,
      "loss": 0.0436,
      "step": 5201
    },
    {
      "epoch": 2.7850066934404283,
      "grad_norm": 0.7470091581344604,
      "learning_rate": 1.4382583868665239e-05,
      "loss": 0.024,
      "step": 5202
    },
    {
      "epoch": 2.7855421686746986,
      "grad_norm": 1.3037384748458862,
      "learning_rate": 1.4346895074946467e-05,
      "loss": 0.0319,
      "step": 5203
    },
    {
      "epoch": 2.7860776439089694,
      "grad_norm": 2.719512939453125,
      "learning_rate": 1.4311206281227696e-05,
      "loss": 0.1065,
      "step": 5204
    },
    {
      "epoch": 2.7866131191432397,
      "grad_norm": 1.1621439456939697,
      "learning_rate": 1.4275517487508924e-05,
      "loss": 0.0648,
      "step": 5205
    },
    {
      "epoch": 2.78714859437751,
      "grad_norm": 0.6012758612632751,
      "learning_rate": 1.4239828693790149e-05,
      "loss": 0.0322,
      "step": 5206
    },
    {
      "epoch": 2.7876840696117804,
      "grad_norm": 1.9599552154541016,
      "learning_rate": 1.4204139900071379e-05,
      "loss": 0.0478,
      "step": 5207
    },
    {
      "epoch": 2.7882195448460507,
      "grad_norm": 3.5103845596313477,
      "learning_rate": 1.4168451106352607e-05,
      "loss": 0.1045,
      "step": 5208
    },
    {
      "epoch": 2.7887550200803215,
      "grad_norm": 8.329017639160156,
      "learning_rate": 1.4132762312633836e-05,
      "loss": 0.1014,
      "step": 5209
    },
    {
      "epoch": 2.789290495314592,
      "grad_norm": 2.1306331157684326,
      "learning_rate": 1.409707351891506e-05,
      "loss": 0.0539,
      "step": 5210
    },
    {
      "epoch": 2.789825970548862,
      "grad_norm": 2.212770700454712,
      "learning_rate": 1.4061384725196289e-05,
      "loss": 0.0799,
      "step": 5211
    },
    {
      "epoch": 2.7903614457831325,
      "grad_norm": 3.965399980545044,
      "learning_rate": 1.4025695931477517e-05,
      "loss": 0.127,
      "step": 5212
    },
    {
      "epoch": 2.790896921017403,
      "grad_norm": 3.519554376602173,
      "learning_rate": 1.3990007137758746e-05,
      "loss": 0.1176,
      "step": 5213
    },
    {
      "epoch": 2.7914323962516736,
      "grad_norm": 1.2548267841339111,
      "learning_rate": 1.395431834403997e-05,
      "loss": 0.0444,
      "step": 5214
    },
    {
      "epoch": 2.7919678714859435,
      "grad_norm": 1.2836495637893677,
      "learning_rate": 1.3918629550321199e-05,
      "loss": 0.0298,
      "step": 5215
    },
    {
      "epoch": 2.7925033467202143,
      "grad_norm": 1.2209187746047974,
      "learning_rate": 1.3882940756602427e-05,
      "loss": 0.0455,
      "step": 5216
    },
    {
      "epoch": 2.7930388219544846,
      "grad_norm": 1.4821929931640625,
      "learning_rate": 1.3847251962883656e-05,
      "loss": 0.0443,
      "step": 5217
    },
    {
      "epoch": 2.793574297188755,
      "grad_norm": 1.1489945650100708,
      "learning_rate": 1.3811563169164882e-05,
      "loss": 0.0331,
      "step": 5218
    },
    {
      "epoch": 2.7941097724230253,
      "grad_norm": 0.630140483379364,
      "learning_rate": 1.377587437544611e-05,
      "loss": 0.0156,
      "step": 5219
    },
    {
      "epoch": 2.7946452476572956,
      "grad_norm": 2.855766534805298,
      "learning_rate": 1.3740185581727339e-05,
      "loss": 0.076,
      "step": 5220
    },
    {
      "epoch": 2.7951807228915664,
      "grad_norm": 0.6066242456436157,
      "learning_rate": 1.3704496788008567e-05,
      "loss": 0.0326,
      "step": 5221
    },
    {
      "epoch": 2.7957161981258367,
      "grad_norm": 1.2618904113769531,
      "learning_rate": 1.3668807994289792e-05,
      "loss": 0.059,
      "step": 5222
    },
    {
      "epoch": 2.796251673360107,
      "grad_norm": 1.0004169940948486,
      "learning_rate": 1.363311920057102e-05,
      "loss": 0.0718,
      "step": 5223
    },
    {
      "epoch": 2.7967871485943774,
      "grad_norm": 4.601876735687256,
      "learning_rate": 1.3597430406852249e-05,
      "loss": 0.1002,
      "step": 5224
    },
    {
      "epoch": 2.7973226238286477,
      "grad_norm": 2.4089581966400146,
      "learning_rate": 1.3561741613133477e-05,
      "loss": 0.098,
      "step": 5225
    },
    {
      "epoch": 2.7978580990629185,
      "grad_norm": 4.513001441955566,
      "learning_rate": 1.3526052819414706e-05,
      "loss": 0.0967,
      "step": 5226
    },
    {
      "epoch": 2.798393574297189,
      "grad_norm": 1.5475636720657349,
      "learning_rate": 1.349036402569593e-05,
      "loss": 0.0222,
      "step": 5227
    },
    {
      "epoch": 2.798929049531459,
      "grad_norm": 1.1568800210952759,
      "learning_rate": 1.3454675231977159e-05,
      "loss": 0.0483,
      "step": 5228
    },
    {
      "epoch": 2.7994645247657295,
      "grad_norm": 0.5500708818435669,
      "learning_rate": 1.3418986438258387e-05,
      "loss": 0.0278,
      "step": 5229
    },
    {
      "epoch": 2.8,
      "grad_norm": 1.9826085567474365,
      "learning_rate": 1.3383297644539616e-05,
      "loss": 0.0474,
      "step": 5230
    },
    {
      "epoch": 2.8005354752342706,
      "grad_norm": 2.8503382205963135,
      "learning_rate": 1.3347608850820842e-05,
      "loss": 0.0925,
      "step": 5231
    },
    {
      "epoch": 2.801070950468541,
      "grad_norm": 0.970679521560669,
      "learning_rate": 1.331192005710207e-05,
      "loss": 0.0406,
      "step": 5232
    },
    {
      "epoch": 2.8016064257028113,
      "grad_norm": 2.3466198444366455,
      "learning_rate": 1.3276231263383299e-05,
      "loss": 0.1034,
      "step": 5233
    },
    {
      "epoch": 2.8021419009370816,
      "grad_norm": 1.974856972694397,
      "learning_rate": 1.3240542469664527e-05,
      "loss": 0.0534,
      "step": 5234
    },
    {
      "epoch": 2.802677376171352,
      "grad_norm": 2.9626457691192627,
      "learning_rate": 1.3204853675945752e-05,
      "loss": 0.1062,
      "step": 5235
    },
    {
      "epoch": 2.8032128514056227,
      "grad_norm": 7.360840797424316,
      "learning_rate": 1.316916488222698e-05,
      "loss": 0.1269,
      "step": 5236
    },
    {
      "epoch": 2.803748326639893,
      "grad_norm": 3.6036293506622314,
      "learning_rate": 1.3133476088508209e-05,
      "loss": 0.1325,
      "step": 5237
    },
    {
      "epoch": 2.8042838018741634,
      "grad_norm": 1.1747742891311646,
      "learning_rate": 1.3097787294789437e-05,
      "loss": 0.0389,
      "step": 5238
    },
    {
      "epoch": 2.8048192771084337,
      "grad_norm": 0.7201884984970093,
      "learning_rate": 1.3062098501070664e-05,
      "loss": 0.021,
      "step": 5239
    },
    {
      "epoch": 2.805354752342704,
      "grad_norm": 0.9084308743476868,
      "learning_rate": 1.3026409707351892e-05,
      "loss": 0.0365,
      "step": 5240
    },
    {
      "epoch": 2.805890227576975,
      "grad_norm": 2.198756217956543,
      "learning_rate": 1.299072091363312e-05,
      "loss": 0.1336,
      "step": 5241
    },
    {
      "epoch": 2.8064257028112447,
      "grad_norm": 3.066657781600952,
      "learning_rate": 1.2955032119914349e-05,
      "loss": 0.1344,
      "step": 5242
    },
    {
      "epoch": 2.8069611780455155,
      "grad_norm": 1.5944253206253052,
      "learning_rate": 1.2919343326195577e-05,
      "loss": 0.0588,
      "step": 5243
    },
    {
      "epoch": 2.807496653279786,
      "grad_norm": 2.9447944164276123,
      "learning_rate": 1.2883654532476802e-05,
      "loss": 0.1653,
      "step": 5244
    },
    {
      "epoch": 2.808032128514056,
      "grad_norm": 2.2894036769866943,
      "learning_rate": 1.284796573875803e-05,
      "loss": 0.0489,
      "step": 5245
    },
    {
      "epoch": 2.8085676037483265,
      "grad_norm": 1.498712182044983,
      "learning_rate": 1.2812276945039259e-05,
      "loss": 0.0664,
      "step": 5246
    },
    {
      "epoch": 2.809103078982597,
      "grad_norm": 1.0530784130096436,
      "learning_rate": 1.2776588151320487e-05,
      "loss": 0.0236,
      "step": 5247
    },
    {
      "epoch": 2.8096385542168676,
      "grad_norm": 0.400331050157547,
      "learning_rate": 1.2740899357601712e-05,
      "loss": 0.0157,
      "step": 5248
    },
    {
      "epoch": 2.810174029451138,
      "grad_norm": 2.6549038887023926,
      "learning_rate": 1.270521056388294e-05,
      "loss": 0.0606,
      "step": 5249
    },
    {
      "epoch": 2.8107095046854083,
      "grad_norm": 2.620429515838623,
      "learning_rate": 1.2669521770164169e-05,
      "loss": 0.0593,
      "step": 5250
    },
    {
      "epoch": 2.8112449799196786,
      "grad_norm": 0.6917064189910889,
      "learning_rate": 1.2633832976445397e-05,
      "loss": 0.0358,
      "step": 5251
    },
    {
      "epoch": 2.811780455153949,
      "grad_norm": 2.4738781452178955,
      "learning_rate": 1.2598144182726624e-05,
      "loss": 0.0592,
      "step": 5252
    },
    {
      "epoch": 2.8123159303882197,
      "grad_norm": 2.5660881996154785,
      "learning_rate": 1.2562455389007852e-05,
      "loss": 0.0623,
      "step": 5253
    },
    {
      "epoch": 2.81285140562249,
      "grad_norm": 4.287432670593262,
      "learning_rate": 1.252676659528908e-05,
      "loss": 0.14,
      "step": 5254
    },
    {
      "epoch": 2.8133868808567604,
      "grad_norm": 0.9105403423309326,
      "learning_rate": 1.2491077801570307e-05,
      "loss": 0.0721,
      "step": 5255
    },
    {
      "epoch": 2.8139223560910307,
      "grad_norm": 0.810528039932251,
      "learning_rate": 1.2455389007851536e-05,
      "loss": 0.0317,
      "step": 5256
    },
    {
      "epoch": 2.814457831325301,
      "grad_norm": 1.9936710596084595,
      "learning_rate": 1.2419700214132762e-05,
      "loss": 0.1117,
      "step": 5257
    },
    {
      "epoch": 2.814993306559572,
      "grad_norm": 1.0039246082305908,
      "learning_rate": 1.238401142041399e-05,
      "loss": 0.0774,
      "step": 5258
    },
    {
      "epoch": 2.815528781793842,
      "grad_norm": 2.8405466079711914,
      "learning_rate": 1.2348322626695217e-05,
      "loss": 0.0942,
      "step": 5259
    },
    {
      "epoch": 2.8160642570281125,
      "grad_norm": 0.3771044909954071,
      "learning_rate": 1.2312633832976446e-05,
      "loss": 0.014,
      "step": 5260
    },
    {
      "epoch": 2.816599732262383,
      "grad_norm": 1.0477946996688843,
      "learning_rate": 1.2276945039257674e-05,
      "loss": 0.0544,
      "step": 5261
    },
    {
      "epoch": 2.817135207496653,
      "grad_norm": 1.7396870851516724,
      "learning_rate": 1.22412562455389e-05,
      "loss": 0.0898,
      "step": 5262
    },
    {
      "epoch": 2.817670682730924,
      "grad_norm": 3.657059907913208,
      "learning_rate": 1.2205567451820129e-05,
      "loss": 0.1192,
      "step": 5263
    },
    {
      "epoch": 2.8182061579651942,
      "grad_norm": 1.7018060684204102,
      "learning_rate": 1.2169878658101357e-05,
      "loss": 0.0826,
      "step": 5264
    },
    {
      "epoch": 2.8187416331994646,
      "grad_norm": 0.5542820692062378,
      "learning_rate": 1.2134189864382586e-05,
      "loss": 0.0302,
      "step": 5265
    },
    {
      "epoch": 2.819277108433735,
      "grad_norm": 2.1940622329711914,
      "learning_rate": 1.2098501070663812e-05,
      "loss": 0.0757,
      "step": 5266
    },
    {
      "epoch": 2.8198125836680052,
      "grad_norm": 1.1560516357421875,
      "learning_rate": 1.206281227694504e-05,
      "loss": 0.05,
      "step": 5267
    },
    {
      "epoch": 2.820348058902276,
      "grad_norm": 1.6976619958877563,
      "learning_rate": 1.2027123483226267e-05,
      "loss": 0.0572,
      "step": 5268
    },
    {
      "epoch": 2.820883534136546,
      "grad_norm": 2.296539783477783,
      "learning_rate": 1.1991434689507496e-05,
      "loss": 0.0616,
      "step": 5269
    },
    {
      "epoch": 2.8214190093708167,
      "grad_norm": 1.589365005493164,
      "learning_rate": 1.1955745895788722e-05,
      "loss": 0.0645,
      "step": 5270
    },
    {
      "epoch": 2.821954484605087,
      "grad_norm": 0.7822439074516296,
      "learning_rate": 1.192005710206995e-05,
      "loss": 0.032,
      "step": 5271
    },
    {
      "epoch": 2.8224899598393574,
      "grad_norm": 1.093705654144287,
      "learning_rate": 1.1884368308351177e-05,
      "loss": 0.0412,
      "step": 5272
    },
    {
      "epoch": 2.8230254350736277,
      "grad_norm": 1.9829022884368896,
      "learning_rate": 1.1848679514632406e-05,
      "loss": 0.0532,
      "step": 5273
    },
    {
      "epoch": 2.823560910307898,
      "grad_norm": 2.3860855102539062,
      "learning_rate": 1.1812990720913634e-05,
      "loss": 0.0995,
      "step": 5274
    },
    {
      "epoch": 2.824096385542169,
      "grad_norm": 1.6279487609863281,
      "learning_rate": 1.1777301927194862e-05,
      "loss": 0.0525,
      "step": 5275
    },
    {
      "epoch": 2.824631860776439,
      "grad_norm": 2.2649011611938477,
      "learning_rate": 1.1741613133476089e-05,
      "loss": 0.0942,
      "step": 5276
    },
    {
      "epoch": 2.8251673360107095,
      "grad_norm": 5.1075215339660645,
      "learning_rate": 1.1705924339757317e-05,
      "loss": 0.1104,
      "step": 5277
    },
    {
      "epoch": 2.82570281124498,
      "grad_norm": 1.1387604475021362,
      "learning_rate": 1.1670235546038546e-05,
      "loss": 0.0482,
      "step": 5278
    },
    {
      "epoch": 2.82623828647925,
      "grad_norm": 1.7638121843338013,
      "learning_rate": 1.1634546752319772e-05,
      "loss": 0.1204,
      "step": 5279
    },
    {
      "epoch": 2.826773761713521,
      "grad_norm": 1.9759516716003418,
      "learning_rate": 1.1598857958601e-05,
      "loss": 0.0983,
      "step": 5280
    },
    {
      "epoch": 2.8273092369477912,
      "grad_norm": 1.3193187713623047,
      "learning_rate": 1.1563169164882227e-05,
      "loss": 0.0552,
      "step": 5281
    },
    {
      "epoch": 2.8278447121820616,
      "grad_norm": 2.287212371826172,
      "learning_rate": 1.1527480371163456e-05,
      "loss": 0.053,
      "step": 5282
    },
    {
      "epoch": 2.828380187416332,
      "grad_norm": 0.47637325525283813,
      "learning_rate": 1.1491791577444682e-05,
      "loss": 0.029,
      "step": 5283
    },
    {
      "epoch": 2.8289156626506022,
      "grad_norm": 1.9583159685134888,
      "learning_rate": 1.145610278372591e-05,
      "loss": 0.0417,
      "step": 5284
    },
    {
      "epoch": 2.829451137884873,
      "grad_norm": 2.089521884918213,
      "learning_rate": 1.1420413990007137e-05,
      "loss": 0.0575,
      "step": 5285
    },
    {
      "epoch": 2.8299866131191433,
      "grad_norm": 0.8777409195899963,
      "learning_rate": 1.1384725196288366e-05,
      "loss": 0.031,
      "step": 5286
    },
    {
      "epoch": 2.8305220883534137,
      "grad_norm": 2.00475811958313,
      "learning_rate": 1.1349036402569594e-05,
      "loss": 0.0997,
      "step": 5287
    },
    {
      "epoch": 2.831057563587684,
      "grad_norm": 1.273728609085083,
      "learning_rate": 1.1313347608850822e-05,
      "loss": 0.0492,
      "step": 5288
    },
    {
      "epoch": 2.8315930388219543,
      "grad_norm": 4.328562259674072,
      "learning_rate": 1.1277658815132049e-05,
      "loss": 0.1159,
      "step": 5289
    },
    {
      "epoch": 2.832128514056225,
      "grad_norm": 1.5327539443969727,
      "learning_rate": 1.1241970021413277e-05,
      "loss": 0.0694,
      "step": 5290
    },
    {
      "epoch": 2.8326639892904955,
      "grad_norm": 0.6064994931221008,
      "learning_rate": 1.1206281227694504e-05,
      "loss": 0.0323,
      "step": 5291
    },
    {
      "epoch": 2.833199464524766,
      "grad_norm": 0.9042661190032959,
      "learning_rate": 1.1170592433975732e-05,
      "loss": 0.0628,
      "step": 5292
    },
    {
      "epoch": 2.833734939759036,
      "grad_norm": 1.842327356338501,
      "learning_rate": 1.1134903640256959e-05,
      "loss": 0.1003,
      "step": 5293
    },
    {
      "epoch": 2.8342704149933065,
      "grad_norm": 0.5194900035858154,
      "learning_rate": 1.1099214846538187e-05,
      "loss": 0.0152,
      "step": 5294
    },
    {
      "epoch": 2.8348058902275772,
      "grad_norm": 3.6078031063079834,
      "learning_rate": 1.1063526052819414e-05,
      "loss": 0.1168,
      "step": 5295
    },
    {
      "epoch": 2.835341365461847,
      "grad_norm": 1.6834405660629272,
      "learning_rate": 1.1027837259100642e-05,
      "loss": 0.0828,
      "step": 5296
    },
    {
      "epoch": 2.835876840696118,
      "grad_norm": 3.555980682373047,
      "learning_rate": 1.099214846538187e-05,
      "loss": 0.1611,
      "step": 5297
    },
    {
      "epoch": 2.8364123159303882,
      "grad_norm": 1.3556830883026123,
      "learning_rate": 1.0956459671663099e-05,
      "loss": 0.0479,
      "step": 5298
    },
    {
      "epoch": 2.8369477911646586,
      "grad_norm": 1.0718886852264404,
      "learning_rate": 1.0920770877944327e-05,
      "loss": 0.041,
      "step": 5299
    },
    {
      "epoch": 2.837483266398929,
      "grad_norm": 0.9816491007804871,
      "learning_rate": 1.0885082084225554e-05,
      "loss": 0.0517,
      "step": 5300
    },
    {
      "epoch": 2.8380187416331992,
      "grad_norm": 1.078944206237793,
      "learning_rate": 1.0849393290506782e-05,
      "loss": 0.0832,
      "step": 5301
    },
    {
      "epoch": 2.83855421686747,
      "grad_norm": 4.036587715148926,
      "learning_rate": 1.0813704496788009e-05,
      "loss": 0.1324,
      "step": 5302
    },
    {
      "epoch": 2.8390896921017403,
      "grad_norm": 0.5013474225997925,
      "learning_rate": 1.0778015703069237e-05,
      "loss": 0.0168,
      "step": 5303
    },
    {
      "epoch": 2.8396251673360107,
      "grad_norm": 0.49878039956092834,
      "learning_rate": 1.0742326909350464e-05,
      "loss": 0.0192,
      "step": 5304
    },
    {
      "epoch": 2.840160642570281,
      "grad_norm": 1.5242609977722168,
      "learning_rate": 1.0706638115631692e-05,
      "loss": 0.0837,
      "step": 5305
    },
    {
      "epoch": 2.8406961178045513,
      "grad_norm": 1.8726861476898193,
      "learning_rate": 1.0670949321912919e-05,
      "loss": 0.0956,
      "step": 5306
    },
    {
      "epoch": 2.841231593038822,
      "grad_norm": 1.1827412843704224,
      "learning_rate": 1.0635260528194147e-05,
      "loss": 0.0838,
      "step": 5307
    },
    {
      "epoch": 2.8417670682730924,
      "grad_norm": 1.0914722681045532,
      "learning_rate": 1.0599571734475376e-05,
      "loss": 0.0403,
      "step": 5308
    },
    {
      "epoch": 2.8423025435073628,
      "grad_norm": 1.1127644777297974,
      "learning_rate": 1.0563882940756604e-05,
      "loss": 0.049,
      "step": 5309
    },
    {
      "epoch": 2.842838018741633,
      "grad_norm": 1.2411307096481323,
      "learning_rate": 1.052819414703783e-05,
      "loss": 0.0828,
      "step": 5310
    },
    {
      "epoch": 2.8433734939759034,
      "grad_norm": 0.3735894560813904,
      "learning_rate": 1.0492505353319059e-05,
      "loss": 0.0112,
      "step": 5311
    },
    {
      "epoch": 2.843908969210174,
      "grad_norm": 1.8530291318893433,
      "learning_rate": 1.0456816559600285e-05,
      "loss": 0.0912,
      "step": 5312
    },
    {
      "epoch": 2.8444444444444446,
      "grad_norm": 1.2089768648147583,
      "learning_rate": 1.0421127765881514e-05,
      "loss": 0.0387,
      "step": 5313
    },
    {
      "epoch": 2.844979919678715,
      "grad_norm": 0.5246081948280334,
      "learning_rate": 1.0385438972162742e-05,
      "loss": 0.0278,
      "step": 5314
    },
    {
      "epoch": 2.845515394912985,
      "grad_norm": 4.635632038116455,
      "learning_rate": 1.0349750178443969e-05,
      "loss": 0.0708,
      "step": 5315
    },
    {
      "epoch": 2.8460508701472556,
      "grad_norm": 0.703392744064331,
      "learning_rate": 1.0314061384725197e-05,
      "loss": 0.0344,
      "step": 5316
    },
    {
      "epoch": 2.8465863453815263,
      "grad_norm": 0.9545527100563049,
      "learning_rate": 1.0278372591006424e-05,
      "loss": 0.049,
      "step": 5317
    },
    {
      "epoch": 2.8471218206157967,
      "grad_norm": 1.0839918851852417,
      "learning_rate": 1.0242683797287652e-05,
      "loss": 0.0312,
      "step": 5318
    },
    {
      "epoch": 2.847657295850067,
      "grad_norm": 1.2396085262298584,
      "learning_rate": 1.020699500356888e-05,
      "loss": 0.0398,
      "step": 5319
    },
    {
      "epoch": 2.8481927710843373,
      "grad_norm": 2.7679076194763184,
      "learning_rate": 1.0171306209850109e-05,
      "loss": 0.0883,
      "step": 5320
    },
    {
      "epoch": 2.8487282463186077,
      "grad_norm": 1.936388611793518,
      "learning_rate": 1.0135617416131335e-05,
      "loss": 0.0667,
      "step": 5321
    },
    {
      "epoch": 2.8492637215528784,
      "grad_norm": 2.126495599746704,
      "learning_rate": 1.0099928622412564e-05,
      "loss": 0.0937,
      "step": 5322
    },
    {
      "epoch": 2.8497991967871483,
      "grad_norm": 1.3451619148254395,
      "learning_rate": 1.006423982869379e-05,
      "loss": 0.0597,
      "step": 5323
    },
    {
      "epoch": 2.850334672021419,
      "grad_norm": 1.6992180347442627,
      "learning_rate": 1.0028551034975019e-05,
      "loss": 0.0659,
      "step": 5324
    },
    {
      "epoch": 2.8508701472556894,
      "grad_norm": 0.7494288682937622,
      "learning_rate": 9.992862241256245e-06,
      "loss": 0.0345,
      "step": 5325
    },
    {
      "epoch": 2.8514056224899598,
      "grad_norm": 0.6255915760993958,
      "learning_rate": 9.957173447537474e-06,
      "loss": 0.025,
      "step": 5326
    },
    {
      "epoch": 2.85194109772423,
      "grad_norm": 1.201261043548584,
      "learning_rate": 9.9214846538187e-06,
      "loss": 0.0637,
      "step": 5327
    },
    {
      "epoch": 2.8524765729585004,
      "grad_norm": 1.5312913656234741,
      "learning_rate": 9.885795860099929e-06,
      "loss": 0.039,
      "step": 5328
    },
    {
      "epoch": 2.853012048192771,
      "grad_norm": 2.749329090118408,
      "learning_rate": 9.850107066381155e-06,
      "loss": 0.1055,
      "step": 5329
    },
    {
      "epoch": 2.8535475234270415,
      "grad_norm": 1.8944026231765747,
      "learning_rate": 9.814418272662384e-06,
      "loss": 0.0922,
      "step": 5330
    },
    {
      "epoch": 2.854082998661312,
      "grad_norm": 2.508660316467285,
      "learning_rate": 9.778729478943612e-06,
      "loss": 0.0502,
      "step": 5331
    },
    {
      "epoch": 2.854618473895582,
      "grad_norm": 2.120476007461548,
      "learning_rate": 9.74304068522484e-06,
      "loss": 0.1047,
      "step": 5332
    },
    {
      "epoch": 2.8551539491298525,
      "grad_norm": 2.472687244415283,
      "learning_rate": 9.707351891506069e-06,
      "loss": 0.0604,
      "step": 5333
    },
    {
      "epoch": 2.8556894243641233,
      "grad_norm": 1.4765625,
      "learning_rate": 9.671663097787295e-06,
      "loss": 0.0943,
      "step": 5334
    },
    {
      "epoch": 2.8562248995983937,
      "grad_norm": 2.710712432861328,
      "learning_rate": 9.635974304068524e-06,
      "loss": 0.0668,
      "step": 5335
    },
    {
      "epoch": 2.856760374832664,
      "grad_norm": 1.3103922605514526,
      "learning_rate": 9.60028551034975e-06,
      "loss": 0.0375,
      "step": 5336
    },
    {
      "epoch": 2.8572958500669343,
      "grad_norm": 1.2447307109832764,
      "learning_rate": 9.564596716630979e-06,
      "loss": 0.0406,
      "step": 5337
    },
    {
      "epoch": 2.8578313253012047,
      "grad_norm": 5.605571746826172,
      "learning_rate": 9.528907922912205e-06,
      "loss": 0.1292,
      "step": 5338
    },
    {
      "epoch": 2.8583668005354754,
      "grad_norm": 0.6599816083908081,
      "learning_rate": 9.493219129193434e-06,
      "loss": 0.0328,
      "step": 5339
    },
    {
      "epoch": 2.8589022757697458,
      "grad_norm": 2.1301631927490234,
      "learning_rate": 9.45753033547466e-06,
      "loss": 0.0944,
      "step": 5340
    },
    {
      "epoch": 2.859437751004016,
      "grad_norm": 1.0336049795150757,
      "learning_rate": 9.421841541755889e-06,
      "loss": 0.0286,
      "step": 5341
    },
    {
      "epoch": 2.8599732262382864,
      "grad_norm": 0.9483011364936829,
      "learning_rate": 9.386152748037117e-06,
      "loss": 0.0541,
      "step": 5342
    },
    {
      "epoch": 2.8605087014725568,
      "grad_norm": 3.4995954036712646,
      "learning_rate": 9.350463954318345e-06,
      "loss": 0.0664,
      "step": 5343
    },
    {
      "epoch": 2.8610441767068275,
      "grad_norm": 1.1657861471176147,
      "learning_rate": 9.314775160599572e-06,
      "loss": 0.0433,
      "step": 5344
    },
    {
      "epoch": 2.861579651941098,
      "grad_norm": 2.2079057693481445,
      "learning_rate": 9.2790863668808e-06,
      "loss": 0.0694,
      "step": 5345
    },
    {
      "epoch": 2.862115127175368,
      "grad_norm": 2.23494291305542,
      "learning_rate": 9.243397573162027e-06,
      "loss": 0.1218,
      "step": 5346
    },
    {
      "epoch": 2.8626506024096385,
      "grad_norm": 0.08217328786849976,
      "learning_rate": 9.207708779443255e-06,
      "loss": 0.0032,
      "step": 5347
    },
    {
      "epoch": 2.863186077643909,
      "grad_norm": 0.6602566838264465,
      "learning_rate": 9.172019985724482e-06,
      "loss": 0.0176,
      "step": 5348
    },
    {
      "epoch": 2.8637215528781796,
      "grad_norm": 0.9747609496116638,
      "learning_rate": 9.13633119200571e-06,
      "loss": 0.0487,
      "step": 5349
    },
    {
      "epoch": 2.8642570281124495,
      "grad_norm": 0.6236645579338074,
      "learning_rate": 9.100642398286937e-06,
      "loss": 0.0415,
      "step": 5350
    },
    {
      "epoch": 2.8647925033467203,
      "grad_norm": 1.35537588596344,
      "learning_rate": 9.064953604568165e-06,
      "loss": 0.0854,
      "step": 5351
    },
    {
      "epoch": 2.8653279785809906,
      "grad_norm": 0.7876244187355042,
      "learning_rate": 9.029264810849394e-06,
      "loss": 0.0459,
      "step": 5352
    },
    {
      "epoch": 2.865863453815261,
      "grad_norm": 0.7579373717308044,
      "learning_rate": 8.993576017130622e-06,
      "loss": 0.0223,
      "step": 5353
    },
    {
      "epoch": 2.8663989290495313,
      "grad_norm": 0.9125344157218933,
      "learning_rate": 8.95788722341185e-06,
      "loss": 0.0473,
      "step": 5354
    },
    {
      "epoch": 2.8669344042838016,
      "grad_norm": 1.5392420291900635,
      "learning_rate": 8.922198429693077e-06,
      "loss": 0.1071,
      "step": 5355
    },
    {
      "epoch": 2.8674698795180724,
      "grad_norm": 0.9084643125534058,
      "learning_rate": 8.886509635974305e-06,
      "loss": 0.0221,
      "step": 5356
    },
    {
      "epoch": 2.8680053547523428,
      "grad_norm": 1.4599519968032837,
      "learning_rate": 8.850820842255532e-06,
      "loss": 0.0529,
      "step": 5357
    },
    {
      "epoch": 2.868540829986613,
      "grad_norm": 0.8094931244850159,
      "learning_rate": 8.81513204853676e-06,
      "loss": 0.0147,
      "step": 5358
    },
    {
      "epoch": 2.8690763052208834,
      "grad_norm": 1.6443417072296143,
      "learning_rate": 8.779443254817987e-06,
      "loss": 0.0651,
      "step": 5359
    },
    {
      "epoch": 2.8696117804551537,
      "grad_norm": 2.0839831829071045,
      "learning_rate": 8.743754461099215e-06,
      "loss": 0.0574,
      "step": 5360
    },
    {
      "epoch": 2.8701472556894245,
      "grad_norm": 1.811203956604004,
      "learning_rate": 8.708065667380442e-06,
      "loss": 0.0472,
      "step": 5361
    },
    {
      "epoch": 2.870682730923695,
      "grad_norm": 2.277132987976074,
      "learning_rate": 8.67237687366167e-06,
      "loss": 0.0897,
      "step": 5362
    },
    {
      "epoch": 2.871218206157965,
      "grad_norm": 0.5686620473861694,
      "learning_rate": 8.636688079942897e-06,
      "loss": 0.0405,
      "step": 5363
    },
    {
      "epoch": 2.8717536813922355,
      "grad_norm": 1.126598596572876,
      "learning_rate": 8.600999286224125e-06,
      "loss": 0.0419,
      "step": 5364
    },
    {
      "epoch": 2.872289156626506,
      "grad_norm": 0.45898282527923584,
      "learning_rate": 8.565310492505354e-06,
      "loss": 0.0145,
      "step": 5365
    },
    {
      "epoch": 2.8728246318607766,
      "grad_norm": 2.184818983078003,
      "learning_rate": 8.529621698786582e-06,
      "loss": 0.0423,
      "step": 5366
    },
    {
      "epoch": 2.873360107095047,
      "grad_norm": 1.674519658088684,
      "learning_rate": 8.493932905067809e-06,
      "loss": 0.0684,
      "step": 5367
    },
    {
      "epoch": 2.8738955823293173,
      "grad_norm": 2.252223253250122,
      "learning_rate": 8.458244111349037e-06,
      "loss": 0.1016,
      "step": 5368
    },
    {
      "epoch": 2.8744310575635876,
      "grad_norm": 5.4776716232299805,
      "learning_rate": 8.422555317630265e-06,
      "loss": 0.086,
      "step": 5369
    },
    {
      "epoch": 2.874966532797858,
      "grad_norm": 1.3007736206054688,
      "learning_rate": 8.386866523911492e-06,
      "loss": 0.0617,
      "step": 5370
    },
    {
      "epoch": 2.8755020080321287,
      "grad_norm": 1.5056180953979492,
      "learning_rate": 8.35117773019272e-06,
      "loss": 0.115,
      "step": 5371
    },
    {
      "epoch": 2.876037483266399,
      "grad_norm": 2.1604955196380615,
      "learning_rate": 8.315488936473947e-06,
      "loss": 0.0689,
      "step": 5372
    },
    {
      "epoch": 2.8765729585006694,
      "grad_norm": 1.0912076234817505,
      "learning_rate": 8.279800142755175e-06,
      "loss": 0.0711,
      "step": 5373
    },
    {
      "epoch": 2.8771084337349397,
      "grad_norm": 2.2636818885803223,
      "learning_rate": 8.244111349036402e-06,
      "loss": 0.063,
      "step": 5374
    },
    {
      "epoch": 2.87764390896921,
      "grad_norm": 0.8022005558013916,
      "learning_rate": 8.20842255531763e-06,
      "loss": 0.0279,
      "step": 5375
    },
    {
      "epoch": 2.878179384203481,
      "grad_norm": 0.9678698182106018,
      "learning_rate": 8.172733761598859e-06,
      "loss": 0.0589,
      "step": 5376
    },
    {
      "epoch": 2.8787148594377507,
      "grad_norm": 2.1157925128936768,
      "learning_rate": 8.137044967880087e-06,
      "loss": 0.0473,
      "step": 5377
    },
    {
      "epoch": 2.8792503346720215,
      "grad_norm": 1.0443216562271118,
      "learning_rate": 8.101356174161314e-06,
      "loss": 0.0395,
      "step": 5378
    },
    {
      "epoch": 2.879785809906292,
      "grad_norm": 2.912109136581421,
      "learning_rate": 8.065667380442542e-06,
      "loss": 0.0528,
      "step": 5379
    },
    {
      "epoch": 2.880321285140562,
      "grad_norm": 3.246838331222534,
      "learning_rate": 8.029978586723769e-06,
      "loss": 0.13,
      "step": 5380
    },
    {
      "epoch": 2.8808567603748325,
      "grad_norm": 1.3044615983963013,
      "learning_rate": 7.994289793004997e-06,
      "loss": 0.0602,
      "step": 5381
    },
    {
      "epoch": 2.881392235609103,
      "grad_norm": 0.6588127613067627,
      "learning_rate": 7.958600999286224e-06,
      "loss": 0.0376,
      "step": 5382
    },
    {
      "epoch": 2.8819277108433736,
      "grad_norm": 5.701961040496826,
      "learning_rate": 7.922912205567452e-06,
      "loss": 0.0687,
      "step": 5383
    },
    {
      "epoch": 2.882463186077644,
      "grad_norm": 2.1395132541656494,
      "learning_rate": 7.887223411848679e-06,
      "loss": 0.0773,
      "step": 5384
    },
    {
      "epoch": 2.8829986613119143,
      "grad_norm": 2.7259883880615234,
      "learning_rate": 7.851534618129907e-06,
      "loss": 0.1096,
      "step": 5385
    },
    {
      "epoch": 2.8835341365461846,
      "grad_norm": 2.3335976600646973,
      "learning_rate": 7.815845824411135e-06,
      "loss": 0.0948,
      "step": 5386
    },
    {
      "epoch": 2.884069611780455,
      "grad_norm": 1.7348357439041138,
      "learning_rate": 7.780157030692364e-06,
      "loss": 0.0752,
      "step": 5387
    },
    {
      "epoch": 2.8846050870147257,
      "grad_norm": 2.4593608379364014,
      "learning_rate": 7.744468236973592e-06,
      "loss": 0.0859,
      "step": 5388
    },
    {
      "epoch": 2.885140562248996,
      "grad_norm": 0.3614712953567505,
      "learning_rate": 7.708779443254819e-06,
      "loss": 0.0136,
      "step": 5389
    },
    {
      "epoch": 2.8856760374832664,
      "grad_norm": 0.7012799382209778,
      "learning_rate": 7.673090649536047e-06,
      "loss": 0.039,
      "step": 5390
    },
    {
      "epoch": 2.8862115127175367,
      "grad_norm": 1.0374618768692017,
      "learning_rate": 7.637401855817274e-06,
      "loss": 0.0576,
      "step": 5391
    },
    {
      "epoch": 2.886746987951807,
      "grad_norm": 2.029128074645996,
      "learning_rate": 7.601713062098502e-06,
      "loss": 0.097,
      "step": 5392
    },
    {
      "epoch": 2.887282463186078,
      "grad_norm": 0.7419127225875854,
      "learning_rate": 7.566024268379729e-06,
      "loss": 0.0244,
      "step": 5393
    },
    {
      "epoch": 2.887817938420348,
      "grad_norm": 1.010061264038086,
      "learning_rate": 7.530335474660957e-06,
      "loss": 0.0497,
      "step": 5394
    },
    {
      "epoch": 2.8883534136546185,
      "grad_norm": 0.8485628962516785,
      "learning_rate": 7.4946466809421845e-06,
      "loss": 0.0432,
      "step": 5395
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 3.7582039833068848,
      "learning_rate": 7.458957887223413e-06,
      "loss": 0.1498,
      "step": 5396
    },
    {
      "epoch": 2.889424364123159,
      "grad_norm": 0.769744336605072,
      "learning_rate": 7.4232690935046395e-06,
      "loss": 0.035,
      "step": 5397
    },
    {
      "epoch": 2.88995983935743,
      "grad_norm": 0.5169569253921509,
      "learning_rate": 7.387580299785868e-06,
      "loss": 0.017,
      "step": 5398
    },
    {
      "epoch": 2.8904953145917003,
      "grad_norm": 0.7438579797744751,
      "learning_rate": 7.3518915060670945e-06,
      "loss": 0.0276,
      "step": 5399
    },
    {
      "epoch": 2.8910307898259706,
      "grad_norm": 4.819512367248535,
      "learning_rate": 7.316202712348323e-06,
      "loss": 0.0684,
      "step": 5400
    },
    {
      "epoch": 2.891566265060241,
      "grad_norm": 1.5482077598571777,
      "learning_rate": 7.28051391862955e-06,
      "loss": 0.1025,
      "step": 5401
    },
    {
      "epoch": 2.8921017402945113,
      "grad_norm": 1.0520918369293213,
      "learning_rate": 7.244825124910779e-06,
      "loss": 0.0628,
      "step": 5402
    },
    {
      "epoch": 2.892637215528782,
      "grad_norm": 1.092089295387268,
      "learning_rate": 7.209136331192005e-06,
      "loss": 0.0838,
      "step": 5403
    },
    {
      "epoch": 2.893172690763052,
      "grad_norm": 0.9084806442260742,
      "learning_rate": 7.173447537473234e-06,
      "loss": 0.0432,
      "step": 5404
    },
    {
      "epoch": 2.8937081659973227,
      "grad_norm": 3.9031875133514404,
      "learning_rate": 7.137758743754462e-06,
      "loss": 0.1115,
      "step": 5405
    },
    {
      "epoch": 2.894243641231593,
      "grad_norm": 1.489161729812622,
      "learning_rate": 7.1020699500356895e-06,
      "loss": 0.0832,
      "step": 5406
    },
    {
      "epoch": 2.8947791164658634,
      "grad_norm": 0.5960556268692017,
      "learning_rate": 7.066381156316918e-06,
      "loss": 0.0206,
      "step": 5407
    },
    {
      "epoch": 2.8953145917001337,
      "grad_norm": 1.1846562623977661,
      "learning_rate": 7.0306923625981445e-06,
      "loss": 0.0473,
      "step": 5408
    },
    {
      "epoch": 2.895850066934404,
      "grad_norm": 5.2607855796813965,
      "learning_rate": 6.995003568879373e-06,
      "loss": 0.117,
      "step": 5409
    },
    {
      "epoch": 2.896385542168675,
      "grad_norm": 1.5903109312057495,
      "learning_rate": 6.9593147751605995e-06,
      "loss": 0.0891,
      "step": 5410
    },
    {
      "epoch": 2.896921017402945,
      "grad_norm": 1.1758460998535156,
      "learning_rate": 6.923625981441828e-06,
      "loss": 0.0348,
      "step": 5411
    },
    {
      "epoch": 2.8974564926372155,
      "grad_norm": 3.300302028656006,
      "learning_rate": 6.887937187723055e-06,
      "loss": 0.0824,
      "step": 5412
    },
    {
      "epoch": 2.897991967871486,
      "grad_norm": 0.6969805359840393,
      "learning_rate": 6.852248394004284e-06,
      "loss": 0.0249,
      "step": 5413
    },
    {
      "epoch": 2.898527443105756,
      "grad_norm": 0.34944671392440796,
      "learning_rate": 6.81655960028551e-06,
      "loss": 0.0112,
      "step": 5414
    },
    {
      "epoch": 2.899062918340027,
      "grad_norm": 0.41424983739852905,
      "learning_rate": 6.780870806566739e-06,
      "loss": 0.0213,
      "step": 5415
    },
    {
      "epoch": 2.8995983935742973,
      "grad_norm": 2.4731109142303467,
      "learning_rate": 6.745182012847965e-06,
      "loss": 0.1006,
      "step": 5416
    },
    {
      "epoch": 2.9001338688085676,
      "grad_norm": 1.2249078750610352,
      "learning_rate": 6.709493219129194e-06,
      "loss": 0.0498,
      "step": 5417
    },
    {
      "epoch": 2.900669344042838,
      "grad_norm": 1.7343767881393433,
      "learning_rate": 6.673804425410421e-06,
      "loss": 0.0868,
      "step": 5418
    },
    {
      "epoch": 2.9012048192771083,
      "grad_norm": 1.8103829622268677,
      "learning_rate": 6.6381156316916495e-06,
      "loss": 0.0511,
      "step": 5419
    },
    {
      "epoch": 2.901740294511379,
      "grad_norm": 2.050379991531372,
      "learning_rate": 6.602426837972876e-06,
      "loss": 0.0994,
      "step": 5420
    },
    {
      "epoch": 2.9022757697456494,
      "grad_norm": 2.608884572982788,
      "learning_rate": 6.5667380442541045e-06,
      "loss": 0.0549,
      "step": 5421
    },
    {
      "epoch": 2.9028112449799197,
      "grad_norm": 0.5902350544929504,
      "learning_rate": 6.531049250535332e-06,
      "loss": 0.0246,
      "step": 5422
    },
    {
      "epoch": 2.90334672021419,
      "grad_norm": 1.0239349603652954,
      "learning_rate": 6.49536045681656e-06,
      "loss": 0.059,
      "step": 5423
    },
    {
      "epoch": 2.9038821954484604,
      "grad_norm": 0.8752514123916626,
      "learning_rate": 6.459671663097789e-06,
      "loss": 0.0374,
      "step": 5424
    },
    {
      "epoch": 2.904417670682731,
      "grad_norm": 0.9848639965057373,
      "learning_rate": 6.423982869379015e-06,
      "loss": 0.055,
      "step": 5425
    },
    {
      "epoch": 2.9049531459170015,
      "grad_norm": 1.5014145374298096,
      "learning_rate": 6.388294075660244e-06,
      "loss": 0.0666,
      "step": 5426
    },
    {
      "epoch": 2.905488621151272,
      "grad_norm": 1.5660560131072998,
      "learning_rate": 6.35260528194147e-06,
      "loss": 0.0914,
      "step": 5427
    },
    {
      "epoch": 2.906024096385542,
      "grad_norm": 1.7295299768447876,
      "learning_rate": 6.316916488222699e-06,
      "loss": 0.0673,
      "step": 5428
    },
    {
      "epoch": 2.9065595716198125,
      "grad_norm": 0.8898885250091553,
      "learning_rate": 6.281227694503926e-06,
      "loss": 0.0708,
      "step": 5429
    },
    {
      "epoch": 2.9070950468540833,
      "grad_norm": 3.7532365322113037,
      "learning_rate": 6.245538900785154e-06,
      "loss": 0.0793,
      "step": 5430
    },
    {
      "epoch": 2.907630522088353,
      "grad_norm": 1.6598397493362427,
      "learning_rate": 6.209850107066381e-06,
      "loss": 0.0986,
      "step": 5431
    },
    {
      "epoch": 2.908165997322624,
      "grad_norm": 0.19351926445960999,
      "learning_rate": 6.174161313347609e-06,
      "loss": 0.0063,
      "step": 5432
    },
    {
      "epoch": 2.9087014725568943,
      "grad_norm": 2.301079750061035,
      "learning_rate": 6.138472519628837e-06,
      "loss": 0.0552,
      "step": 5433
    },
    {
      "epoch": 2.9092369477911646,
      "grad_norm": 1.1405043601989746,
      "learning_rate": 6.1027837259100645e-06,
      "loss": 0.0938,
      "step": 5434
    },
    {
      "epoch": 2.909772423025435,
      "grad_norm": 0.6418469548225403,
      "learning_rate": 6.067094932191293e-06,
      "loss": 0.025,
      "step": 5435
    },
    {
      "epoch": 2.9103078982597053,
      "grad_norm": 2.3295161724090576,
      "learning_rate": 6.03140613847252e-06,
      "loss": 0.0507,
      "step": 5436
    },
    {
      "epoch": 2.910843373493976,
      "grad_norm": 1.1824750900268555,
      "learning_rate": 5.995717344753748e-06,
      "loss": 0.0536,
      "step": 5437
    },
    {
      "epoch": 2.9113788487282464,
      "grad_norm": 2.493607759475708,
      "learning_rate": 5.960028551034975e-06,
      "loss": 0.0617,
      "step": 5438
    },
    {
      "epoch": 2.9119143239625167,
      "grad_norm": 1.6338837146759033,
      "learning_rate": 5.924339757316203e-06,
      "loss": 0.0735,
      "step": 5439
    },
    {
      "epoch": 2.912449799196787,
      "grad_norm": 1.2837270498275757,
      "learning_rate": 5.888650963597431e-06,
      "loss": 0.0581,
      "step": 5440
    },
    {
      "epoch": 2.9129852744310574,
      "grad_norm": 1.5680148601531982,
      "learning_rate": 5.852962169878659e-06,
      "loss": 0.1183,
      "step": 5441
    },
    {
      "epoch": 2.913520749665328,
      "grad_norm": 0.7029776573181152,
      "learning_rate": 5.817273376159886e-06,
      "loss": 0.0244,
      "step": 5442
    },
    {
      "epoch": 2.9140562248995985,
      "grad_norm": 1.1840455532073975,
      "learning_rate": 5.781584582441114e-06,
      "loss": 0.0293,
      "step": 5443
    },
    {
      "epoch": 2.914591700133869,
      "grad_norm": 1.5160455703735352,
      "learning_rate": 5.745895788722341e-06,
      "loss": 0.0961,
      "step": 5444
    },
    {
      "epoch": 2.915127175368139,
      "grad_norm": 0.7380189299583435,
      "learning_rate": 5.710206995003569e-06,
      "loss": 0.0401,
      "step": 5445
    },
    {
      "epoch": 2.9156626506024095,
      "grad_norm": 2.130241870880127,
      "learning_rate": 5.674518201284797e-06,
      "loss": 0.0808,
      "step": 5446
    },
    {
      "epoch": 2.9161981258366803,
      "grad_norm": 0.6906110644340515,
      "learning_rate": 5.6388294075660244e-06,
      "loss": 0.0321,
      "step": 5447
    },
    {
      "epoch": 2.9167336010709506,
      "grad_norm": 1.9576534032821655,
      "learning_rate": 5.603140613847252e-06,
      "loss": 0.0556,
      "step": 5448
    },
    {
      "epoch": 2.917269076305221,
      "grad_norm": 0.85219806432724,
      "learning_rate": 5.5674518201284794e-06,
      "loss": 0.025,
      "step": 5449
    },
    {
      "epoch": 2.9178045515394913,
      "grad_norm": 0.953703761100769,
      "learning_rate": 5.531763026409707e-06,
      "loss": 0.0375,
      "step": 5450
    },
    {
      "epoch": 2.9183400267737616,
      "grad_norm": 2.027778387069702,
      "learning_rate": 5.496074232690935e-06,
      "loss": 0.193,
      "step": 5451
    },
    {
      "epoch": 2.9188755020080324,
      "grad_norm": 2.9591939449310303,
      "learning_rate": 5.460385438972164e-06,
      "loss": 0.0569,
      "step": 5452
    },
    {
      "epoch": 2.9194109772423027,
      "grad_norm": 0.8881900310516357,
      "learning_rate": 5.424696645253391e-06,
      "loss": 0.0285,
      "step": 5453
    },
    {
      "epoch": 2.919946452476573,
      "grad_norm": 1.0812408924102783,
      "learning_rate": 5.389007851534619e-06,
      "loss": 0.0788,
      "step": 5454
    },
    {
      "epoch": 2.9204819277108434,
      "grad_norm": 1.0276904106140137,
      "learning_rate": 5.353319057815846e-06,
      "loss": 0.0612,
      "step": 5455
    },
    {
      "epoch": 2.9210174029451137,
      "grad_norm": 1.2057069540023804,
      "learning_rate": 5.317630264097074e-06,
      "loss": 0.0287,
      "step": 5456
    },
    {
      "epoch": 2.9215528781793845,
      "grad_norm": 2.6194660663604736,
      "learning_rate": 5.281941470378302e-06,
      "loss": 0.0936,
      "step": 5457
    },
    {
      "epoch": 2.9220883534136544,
      "grad_norm": 1.675801157951355,
      "learning_rate": 5.246252676659529e-06,
      "loss": 0.0662,
      "step": 5458
    },
    {
      "epoch": 2.922623828647925,
      "grad_norm": 0.9821330308914185,
      "learning_rate": 5.210563882940757e-06,
      "loss": 0.0598,
      "step": 5459
    },
    {
      "epoch": 2.9231593038821955,
      "grad_norm": 2.7860159873962402,
      "learning_rate": 5.174875089221984e-06,
      "loss": 0.1037,
      "step": 5460
    },
    {
      "epoch": 2.923694779116466,
      "grad_norm": 1.6628918647766113,
      "learning_rate": 5.139186295503212e-06,
      "loss": 0.0739,
      "step": 5461
    },
    {
      "epoch": 2.924230254350736,
      "grad_norm": 0.7964974045753479,
      "learning_rate": 5.10349750178444e-06,
      "loss": 0.0458,
      "step": 5462
    },
    {
      "epoch": 2.9247657295850065,
      "grad_norm": 0.791198194026947,
      "learning_rate": 5.067808708065668e-06,
      "loss": 0.0473,
      "step": 5463
    },
    {
      "epoch": 2.9253012048192772,
      "grad_norm": 0.5048666596412659,
      "learning_rate": 5.032119914346895e-06,
      "loss": 0.0287,
      "step": 5464
    },
    {
      "epoch": 2.9258366800535476,
      "grad_norm": 0.3405514061450958,
      "learning_rate": 4.996431120628123e-06,
      "loss": 0.0088,
      "step": 5465
    },
    {
      "epoch": 2.926372155287818,
      "grad_norm": 1.0179110765457153,
      "learning_rate": 4.96074232690935e-06,
      "loss": 0.0437,
      "step": 5466
    },
    {
      "epoch": 2.9269076305220882,
      "grad_norm": 1.9236769676208496,
      "learning_rate": 4.925053533190578e-06,
      "loss": 0.0807,
      "step": 5467
    },
    {
      "epoch": 2.9274431057563586,
      "grad_norm": 1.1752204895019531,
      "learning_rate": 4.889364739471806e-06,
      "loss": 0.06,
      "step": 5468
    },
    {
      "epoch": 2.9279785809906294,
      "grad_norm": 0.38833457231521606,
      "learning_rate": 4.853675945753034e-06,
      "loss": 0.0068,
      "step": 5469
    },
    {
      "epoch": 2.9285140562248997,
      "grad_norm": 1.1483458280563354,
      "learning_rate": 4.817987152034262e-06,
      "loss": 0.043,
      "step": 5470
    },
    {
      "epoch": 2.92904953145917,
      "grad_norm": 1.9579209089279175,
      "learning_rate": 4.782298358315489e-06,
      "loss": 0.0865,
      "step": 5471
    },
    {
      "epoch": 2.9295850066934404,
      "grad_norm": 1.885150671005249,
      "learning_rate": 4.746609564596717e-06,
      "loss": 0.079,
      "step": 5472
    },
    {
      "epoch": 2.9301204819277107,
      "grad_norm": 1.1663719415664673,
      "learning_rate": 4.710920770877944e-06,
      "loss": 0.0583,
      "step": 5473
    },
    {
      "epoch": 2.9306559571619815,
      "grad_norm": 0.5081614255905151,
      "learning_rate": 4.675231977159173e-06,
      "loss": 0.0191,
      "step": 5474
    },
    {
      "epoch": 2.931191432396252,
      "grad_norm": 3.1058084964752197,
      "learning_rate": 4.6395431834404e-06,
      "loss": 0.0585,
      "step": 5475
    },
    {
      "epoch": 2.931726907630522,
      "grad_norm": 1.7509340047836304,
      "learning_rate": 4.603854389721628e-06,
      "loss": 0.0406,
      "step": 5476
    },
    {
      "epoch": 2.9322623828647925,
      "grad_norm": 2.5366857051849365,
      "learning_rate": 4.568165596002855e-06,
      "loss": 0.0825,
      "step": 5477
    },
    {
      "epoch": 2.932797858099063,
      "grad_norm": 1.0954195261001587,
      "learning_rate": 4.532476802284083e-06,
      "loss": 0.0578,
      "step": 5478
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 1.1742630004882812,
      "learning_rate": 4.496788008565311e-06,
      "loss": 0.0172,
      "step": 5479
    },
    {
      "epoch": 2.933868808567604,
      "grad_norm": 2.2046356201171875,
      "learning_rate": 4.4610992148465385e-06,
      "loss": 0.1189,
      "step": 5480
    },
    {
      "epoch": 2.9344042838018742,
      "grad_norm": 0.5332408547401428,
      "learning_rate": 4.425410421127766e-06,
      "loss": 0.0205,
      "step": 5481
    },
    {
      "epoch": 2.9349397590361446,
      "grad_norm": 0.5280796885490417,
      "learning_rate": 4.3897216274089935e-06,
      "loss": 0.0216,
      "step": 5482
    },
    {
      "epoch": 2.935475234270415,
      "grad_norm": 3.185931444168091,
      "learning_rate": 4.354032833690221e-06,
      "loss": 0.0857,
      "step": 5483
    },
    {
      "epoch": 2.9360107095046857,
      "grad_norm": 1.7810561656951904,
      "learning_rate": 4.3183440399714485e-06,
      "loss": 0.1011,
      "step": 5484
    },
    {
      "epoch": 2.9365461847389556,
      "grad_norm": 1.235133171081543,
      "learning_rate": 4.282655246252677e-06,
      "loss": 0.0427,
      "step": 5485
    },
    {
      "epoch": 2.9370816599732263,
      "grad_norm": 1.0642187595367432,
      "learning_rate": 4.246966452533904e-06,
      "loss": 0.0293,
      "step": 5486
    },
    {
      "epoch": 2.9376171352074967,
      "grad_norm": 1.3683165311813354,
      "learning_rate": 4.211277658815133e-06,
      "loss": 0.0567,
      "step": 5487
    },
    {
      "epoch": 2.938152610441767,
      "grad_norm": 2.219083070755005,
      "learning_rate": 4.17558886509636e-06,
      "loss": 0.0655,
      "step": 5488
    },
    {
      "epoch": 2.9386880856760373,
      "grad_norm": 1.969699501991272,
      "learning_rate": 4.139900071377588e-06,
      "loss": 0.0752,
      "step": 5489
    },
    {
      "epoch": 2.9392235609103077,
      "grad_norm": 2.7102997303009033,
      "learning_rate": 4.104211277658815e-06,
      "loss": 0.0745,
      "step": 5490
    },
    {
      "epoch": 2.9397590361445785,
      "grad_norm": 1.7151178121566772,
      "learning_rate": 4.0685224839400435e-06,
      "loss": 0.0538,
      "step": 5491
    },
    {
      "epoch": 2.940294511378849,
      "grad_norm": 0.2508358359336853,
      "learning_rate": 4.032833690221271e-06,
      "loss": 0.0067,
      "step": 5492
    },
    {
      "epoch": 2.940829986613119,
      "grad_norm": 2.6827898025512695,
      "learning_rate": 3.9971448965024985e-06,
      "loss": 0.0709,
      "step": 5493
    },
    {
      "epoch": 2.9413654618473895,
      "grad_norm": 0.23280642926692963,
      "learning_rate": 3.961456102783726e-06,
      "loss": 0.0055,
      "step": 5494
    },
    {
      "epoch": 2.94190093708166,
      "grad_norm": 0.9268703460693359,
      "learning_rate": 3.9257673090649535e-06,
      "loss": 0.0268,
      "step": 5495
    },
    {
      "epoch": 2.9424364123159306,
      "grad_norm": 1.962907075881958,
      "learning_rate": 3.890078515346182e-06,
      "loss": 0.1167,
      "step": 5496
    },
    {
      "epoch": 2.942971887550201,
      "grad_norm": 1.895861029624939,
      "learning_rate": 3.854389721627409e-06,
      "loss": 0.0551,
      "step": 5497
    },
    {
      "epoch": 2.9435073627844712,
      "grad_norm": 1.0247440338134766,
      "learning_rate": 3.818700927908637e-06,
      "loss": 0.033,
      "step": 5498
    },
    {
      "epoch": 2.9440428380187416,
      "grad_norm": 0.43702569603919983,
      "learning_rate": 3.7830121341898643e-06,
      "loss": 0.0166,
      "step": 5499
    },
    {
      "epoch": 2.944578313253012,
      "grad_norm": 1.0323739051818848,
      "learning_rate": 3.7473233404710923e-06,
      "loss": 0.0372,
      "step": 5500
    },
    {
      "epoch": 2.9451137884872827,
      "grad_norm": 0.6096463203430176,
      "learning_rate": 3.7116345467523198e-06,
      "loss": 0.0268,
      "step": 5501
    },
    {
      "epoch": 2.945649263721553,
      "grad_norm": 2.829245090484619,
      "learning_rate": 3.6759457530335473e-06,
      "loss": 0.0766,
      "step": 5502
    },
    {
      "epoch": 2.9461847389558233,
      "grad_norm": 3.0447685718536377,
      "learning_rate": 3.640256959314775e-06,
      "loss": 0.0851,
      "step": 5503
    },
    {
      "epoch": 2.9467202141900937,
      "grad_norm": 0.6488328576087952,
      "learning_rate": 3.6045681655960027e-06,
      "loss": 0.0414,
      "step": 5504
    },
    {
      "epoch": 2.947255689424364,
      "grad_norm": 2.204665422439575,
      "learning_rate": 3.568879371877231e-06,
      "loss": 0.1088,
      "step": 5505
    },
    {
      "epoch": 2.9477911646586348,
      "grad_norm": 1.7230514287948608,
      "learning_rate": 3.533190578158459e-06,
      "loss": 0.039,
      "step": 5506
    },
    {
      "epoch": 2.948326639892905,
      "grad_norm": 1.7353734970092773,
      "learning_rate": 3.4975017844396864e-06,
      "loss": 0.0646,
      "step": 5507
    },
    {
      "epoch": 2.9488621151271754,
      "grad_norm": 3.018953323364258,
      "learning_rate": 3.461812990720914e-06,
      "loss": 0.1433,
      "step": 5508
    },
    {
      "epoch": 2.9493975903614458,
      "grad_norm": 1.2887630462646484,
      "learning_rate": 3.426124197002142e-06,
      "loss": 0.0717,
      "step": 5509
    },
    {
      "epoch": 2.949933065595716,
      "grad_norm": 1.6914469003677368,
      "learning_rate": 3.3904354032833693e-06,
      "loss": 0.0637,
      "step": 5510
    },
    {
      "epoch": 2.950468540829987,
      "grad_norm": 0.4891537129878998,
      "learning_rate": 3.354746609564597e-06,
      "loss": 0.0106,
      "step": 5511
    },
    {
      "epoch": 2.9510040160642568,
      "grad_norm": 0.9398607611656189,
      "learning_rate": 3.3190578158458247e-06,
      "loss": 0.0239,
      "step": 5512
    },
    {
      "epoch": 2.9515394912985276,
      "grad_norm": 0.6545121669769287,
      "learning_rate": 3.2833690221270522e-06,
      "loss": 0.0165,
      "step": 5513
    },
    {
      "epoch": 2.952074966532798,
      "grad_norm": 1.311347484588623,
      "learning_rate": 3.24768022840828e-06,
      "loss": 0.0872,
      "step": 5514
    },
    {
      "epoch": 2.952610441767068,
      "grad_norm": 1.9578274488449097,
      "learning_rate": 3.2119914346895077e-06,
      "loss": 0.136,
      "step": 5515
    },
    {
      "epoch": 2.9531459170013385,
      "grad_norm": 1.7524923086166382,
      "learning_rate": 3.176302640970735e-06,
      "loss": 0.113,
      "step": 5516
    },
    {
      "epoch": 2.953681392235609,
      "grad_norm": 0.4296885132789612,
      "learning_rate": 3.140613847251963e-06,
      "loss": 0.0191,
      "step": 5517
    },
    {
      "epoch": 2.9542168674698797,
      "grad_norm": 1.2014700174331665,
      "learning_rate": 3.1049250535331906e-06,
      "loss": 0.0355,
      "step": 5518
    },
    {
      "epoch": 2.95475234270415,
      "grad_norm": 0.7217395901679993,
      "learning_rate": 3.0692362598144185e-06,
      "loss": 0.0256,
      "step": 5519
    },
    {
      "epoch": 2.9552878179384203,
      "grad_norm": 0.6599177718162537,
      "learning_rate": 3.0335474660956464e-06,
      "loss": 0.0308,
      "step": 5520
    },
    {
      "epoch": 2.9558232931726907,
      "grad_norm": 1.1450265645980835,
      "learning_rate": 2.997858672376874e-06,
      "loss": 0.046,
      "step": 5521
    },
    {
      "epoch": 2.956358768406961,
      "grad_norm": 0.7516666650772095,
      "learning_rate": 2.9621698786581014e-06,
      "loss": 0.0396,
      "step": 5522
    },
    {
      "epoch": 2.9568942436412318,
      "grad_norm": 1.3870220184326172,
      "learning_rate": 2.9264810849393293e-06,
      "loss": 0.0676,
      "step": 5523
    },
    {
      "epoch": 2.957429718875502,
      "grad_norm": 0.8432482481002808,
      "learning_rate": 2.890792291220557e-06,
      "loss": 0.0316,
      "step": 5524
    },
    {
      "epoch": 2.9579651941097724,
      "grad_norm": 1.2413514852523804,
      "learning_rate": 2.8551034975017843e-06,
      "loss": 0.0483,
      "step": 5525
    },
    {
      "epoch": 2.9585006693440428,
      "grad_norm": 1.0345304012298584,
      "learning_rate": 2.8194147037830122e-06,
      "loss": 0.0605,
      "step": 5526
    },
    {
      "epoch": 2.959036144578313,
      "grad_norm": 2.2355117797851562,
      "learning_rate": 2.7837259100642397e-06,
      "loss": 0.1771,
      "step": 5527
    },
    {
      "epoch": 2.959571619812584,
      "grad_norm": 0.507875382900238,
      "learning_rate": 2.7480371163454676e-06,
      "loss": 0.0221,
      "step": 5528
    },
    {
      "epoch": 2.960107095046854,
      "grad_norm": 0.9037078619003296,
      "learning_rate": 2.7123483226266955e-06,
      "loss": 0.0233,
      "step": 5529
    },
    {
      "epoch": 2.9606425702811245,
      "grad_norm": 4.40244722366333,
      "learning_rate": 2.676659528907923e-06,
      "loss": 0.0326,
      "step": 5530
    },
    {
      "epoch": 2.961178045515395,
      "grad_norm": 0.6600945591926575,
      "learning_rate": 2.640970735189151e-06,
      "loss": 0.0296,
      "step": 5531
    },
    {
      "epoch": 2.961713520749665,
      "grad_norm": 0.45351362228393555,
      "learning_rate": 2.6052819414703785e-06,
      "loss": 0.0119,
      "step": 5532
    },
    {
      "epoch": 2.962248995983936,
      "grad_norm": 2.4753506183624268,
      "learning_rate": 2.569593147751606e-06,
      "loss": 0.0998,
      "step": 5533
    },
    {
      "epoch": 2.9627844712182063,
      "grad_norm": 2.1820499897003174,
      "learning_rate": 2.533904354032834e-06,
      "loss": 0.0595,
      "step": 5534
    },
    {
      "epoch": 2.9633199464524766,
      "grad_norm": 1.7091141939163208,
      "learning_rate": 2.4982155603140614e-06,
      "loss": 0.0659,
      "step": 5535
    },
    {
      "epoch": 2.963855421686747,
      "grad_norm": 1.0535601377487183,
      "learning_rate": 2.462526766595289e-06,
      "loss": 0.0408,
      "step": 5536
    },
    {
      "epoch": 2.9643908969210173,
      "grad_norm": 0.6957054138183594,
      "learning_rate": 2.426837972876517e-06,
      "loss": 0.0323,
      "step": 5537
    },
    {
      "epoch": 2.964926372155288,
      "grad_norm": 0.9141733050346375,
      "learning_rate": 2.3911491791577447e-06,
      "loss": 0.0461,
      "step": 5538
    },
    {
      "epoch": 2.965461847389558,
      "grad_norm": 2.0715651512145996,
      "learning_rate": 2.355460385438972e-06,
      "loss": 0.0526,
      "step": 5539
    },
    {
      "epoch": 2.9659973226238288,
      "grad_norm": 5.976923942565918,
      "learning_rate": 2.3197715917202e-06,
      "loss": 0.0871,
      "step": 5540
    },
    {
      "epoch": 2.966532797858099,
      "grad_norm": 1.7618966102600098,
      "learning_rate": 2.2840827980014276e-06,
      "loss": 0.056,
      "step": 5541
    },
    {
      "epoch": 2.9670682730923694,
      "grad_norm": 1.5640811920166016,
      "learning_rate": 2.2483940042826555e-06,
      "loss": 0.0308,
      "step": 5542
    },
    {
      "epoch": 2.9676037483266398,
      "grad_norm": 0.9356228709220886,
      "learning_rate": 2.212705210563883e-06,
      "loss": 0.0397,
      "step": 5543
    },
    {
      "epoch": 2.96813922356091,
      "grad_norm": 2.5108108520507812,
      "learning_rate": 2.1770164168451105e-06,
      "loss": 0.0378,
      "step": 5544
    },
    {
      "epoch": 2.968674698795181,
      "grad_norm": 0.5502504706382751,
      "learning_rate": 2.1413276231263384e-06,
      "loss": 0.0262,
      "step": 5545
    },
    {
      "epoch": 2.969210174029451,
      "grad_norm": 1.0705363750457764,
      "learning_rate": 2.1056388294075664e-06,
      "loss": 0.0616,
      "step": 5546
    },
    {
      "epoch": 2.9697456492637215,
      "grad_norm": 0.8801912069320679,
      "learning_rate": 2.069950035688794e-06,
      "loss": 0.0451,
      "step": 5547
    },
    {
      "epoch": 2.970281124497992,
      "grad_norm": 0.9350772500038147,
      "learning_rate": 2.0342612419700218e-06,
      "loss": 0.0292,
      "step": 5548
    },
    {
      "epoch": 2.970816599732262,
      "grad_norm": 1.5297940969467163,
      "learning_rate": 1.9985724482512493e-06,
      "loss": 0.0634,
      "step": 5549
    },
    {
      "epoch": 2.971352074966533,
      "grad_norm": 1.643736720085144,
      "learning_rate": 1.9628836545324768e-06,
      "loss": 0.0951,
      "step": 5550
    },
    {
      "epoch": 2.9718875502008033,
      "grad_norm": 0.35065364837646484,
      "learning_rate": 1.9271948608137047e-06,
      "loss": 0.0173,
      "step": 5551
    },
    {
      "epoch": 2.9724230254350736,
      "grad_norm": 1.401437759399414,
      "learning_rate": 1.8915060670949322e-06,
      "loss": 0.0827,
      "step": 5552
    },
    {
      "epoch": 2.972958500669344,
      "grad_norm": 1.0501248836517334,
      "learning_rate": 1.8558172733761599e-06,
      "loss": 0.0532,
      "step": 5553
    },
    {
      "epoch": 2.9734939759036143,
      "grad_norm": 0.3578185737133026,
      "learning_rate": 1.8201284796573876e-06,
      "loss": 0.0168,
      "step": 5554
    },
    {
      "epoch": 2.974029451137885,
      "grad_norm": 1.7509287595748901,
      "learning_rate": 1.7844396859386155e-06,
      "loss": 0.07,
      "step": 5555
    },
    {
      "epoch": 2.9745649263721554,
      "grad_norm": 1.6807414293289185,
      "learning_rate": 1.7487508922198432e-06,
      "loss": 0.0388,
      "step": 5556
    },
    {
      "epoch": 2.9751004016064257,
      "grad_norm": 0.7849979996681213,
      "learning_rate": 1.713062098501071e-06,
      "loss": 0.0251,
      "step": 5557
    },
    {
      "epoch": 2.975635876840696,
      "grad_norm": 1.2717550992965698,
      "learning_rate": 1.6773733047822984e-06,
      "loss": 0.0567,
      "step": 5558
    },
    {
      "epoch": 2.9761713520749664,
      "grad_norm": 1.337429404258728,
      "learning_rate": 1.6416845110635261e-06,
      "loss": 0.0517,
      "step": 5559
    },
    {
      "epoch": 2.976706827309237,
      "grad_norm": 2.396291494369507,
      "learning_rate": 1.6059957173447538e-06,
      "loss": 0.0629,
      "step": 5560
    },
    {
      "epoch": 2.9772423025435075,
      "grad_norm": 0.7964057922363281,
      "learning_rate": 1.5703069236259815e-06,
      "loss": 0.0488,
      "step": 5561
    },
    {
      "epoch": 2.977777777777778,
      "grad_norm": 1.0225164890289307,
      "learning_rate": 1.5346181299072092e-06,
      "loss": 0.0266,
      "step": 5562
    },
    {
      "epoch": 2.978313253012048,
      "grad_norm": 2.8548173904418945,
      "learning_rate": 1.498929336188437e-06,
      "loss": 0.1024,
      "step": 5563
    },
    {
      "epoch": 2.9788487282463185,
      "grad_norm": 1.3307621479034424,
      "learning_rate": 1.4632405424696647e-06,
      "loss": 0.1137,
      "step": 5564
    },
    {
      "epoch": 2.9793842034805893,
      "grad_norm": 0.9538798332214355,
      "learning_rate": 1.4275517487508921e-06,
      "loss": 0.0744,
      "step": 5565
    },
    {
      "epoch": 2.979919678714859,
      "grad_norm": 1.1656920909881592,
      "learning_rate": 1.3918629550321199e-06,
      "loss": 0.0448,
      "step": 5566
    },
    {
      "epoch": 2.98045515394913,
      "grad_norm": 2.0537002086639404,
      "learning_rate": 1.3561741613133478e-06,
      "loss": 0.0396,
      "step": 5567
    },
    {
      "epoch": 2.9809906291834003,
      "grad_norm": 4.012321472167969,
      "learning_rate": 1.3204853675945755e-06,
      "loss": 0.1157,
      "step": 5568
    },
    {
      "epoch": 2.9815261044176706,
      "grad_norm": 0.5099947452545166,
      "learning_rate": 1.284796573875803e-06,
      "loss": 0.0199,
      "step": 5569
    },
    {
      "epoch": 2.982061579651941,
      "grad_norm": 0.8199028372764587,
      "learning_rate": 1.2491077801570307e-06,
      "loss": 0.0498,
      "step": 5570
    },
    {
      "epoch": 2.9825970548862113,
      "grad_norm": 1.0264027118682861,
      "learning_rate": 1.2134189864382586e-06,
      "loss": 0.0441,
      "step": 5571
    },
    {
      "epoch": 2.983132530120482,
      "grad_norm": 1.3152806758880615,
      "learning_rate": 1.177730192719486e-06,
      "loss": 0.0827,
      "step": 5572
    },
    {
      "epoch": 2.9836680053547524,
      "grad_norm": 0.718692421913147,
      "learning_rate": 1.1420413990007138e-06,
      "loss": 0.0236,
      "step": 5573
    },
    {
      "epoch": 2.9842034805890227,
      "grad_norm": 6.929449081420898,
      "learning_rate": 1.1063526052819415e-06,
      "loss": 0.0949,
      "step": 5574
    },
    {
      "epoch": 2.984738955823293,
      "grad_norm": 1.0005565881729126,
      "learning_rate": 1.0706638115631692e-06,
      "loss": 0.0401,
      "step": 5575
    },
    {
      "epoch": 2.9852744310575634,
      "grad_norm": 2.375211477279663,
      "learning_rate": 1.034975017844397e-06,
      "loss": 0.1211,
      "step": 5576
    },
    {
      "epoch": 2.985809906291834,
      "grad_norm": 0.7250838279724121,
      "learning_rate": 9.992862241256246e-07,
      "loss": 0.0249,
      "step": 5577
    },
    {
      "epoch": 2.9863453815261045,
      "grad_norm": 2.7726292610168457,
      "learning_rate": 9.635974304068523e-07,
      "loss": 0.1251,
      "step": 5578
    },
    {
      "epoch": 2.986880856760375,
      "grad_norm": 2.4158408641815186,
      "learning_rate": 9.279086366880799e-07,
      "loss": 0.0489,
      "step": 5579
    },
    {
      "epoch": 2.987416331994645,
      "grad_norm": 0.661584734916687,
      "learning_rate": 8.922198429693078e-07,
      "loss": 0.0245,
      "step": 5580
    },
    {
      "epoch": 2.9879518072289155,
      "grad_norm": 1.258638858795166,
      "learning_rate": 8.565310492505355e-07,
      "loss": 0.0502,
      "step": 5581
    },
    {
      "epoch": 2.9884872824631863,
      "grad_norm": 0.9020060896873474,
      "learning_rate": 8.208422555317631e-07,
      "loss": 0.029,
      "step": 5582
    },
    {
      "epoch": 2.9890227576974566,
      "grad_norm": 1.2431862354278564,
      "learning_rate": 7.851534618129908e-07,
      "loss": 0.0539,
      "step": 5583
    },
    {
      "epoch": 2.989558232931727,
      "grad_norm": 5.68815803527832,
      "learning_rate": 7.494646680942185e-07,
      "loss": 0.1052,
      "step": 5584
    },
    {
      "epoch": 2.9900937081659973,
      "grad_norm": 1.1202508211135864,
      "learning_rate": 7.137758743754461e-07,
      "loss": 0.0327,
      "step": 5585
    },
    {
      "epoch": 2.9906291834002676,
      "grad_norm": 0.5578492283821106,
      "learning_rate": 6.780870806566739e-07,
      "loss": 0.0193,
      "step": 5586
    },
    {
      "epoch": 2.9911646586345384,
      "grad_norm": 3.1421384811401367,
      "learning_rate": 6.423982869379015e-07,
      "loss": 0.1021,
      "step": 5587
    },
    {
      "epoch": 2.9917001338688087,
      "grad_norm": 3.8135483264923096,
      "learning_rate": 6.067094932191293e-07,
      "loss": 0.1593,
      "step": 5588
    },
    {
      "epoch": 2.992235609103079,
      "grad_norm": 1.3879514932632446,
      "learning_rate": 5.710206995003569e-07,
      "loss": 0.0689,
      "step": 5589
    },
    {
      "epoch": 2.9927710843373494,
      "grad_norm": 0.8662025332450867,
      "learning_rate": 5.353319057815846e-07,
      "loss": 0.0251,
      "step": 5590
    },
    {
      "epoch": 2.9933065595716197,
      "grad_norm": 1.8042389154434204,
      "learning_rate": 4.996431120628123e-07,
      "loss": 0.0445,
      "step": 5591
    },
    {
      "epoch": 2.99384203480589,
      "grad_norm": 1.4214228391647339,
      "learning_rate": 4.6395431834403997e-07,
      "loss": 0.079,
      "step": 5592
    },
    {
      "epoch": 2.9943775100401604,
      "grad_norm": 2.0604758262634277,
      "learning_rate": 4.2826552462526773e-07,
      "loss": 0.1182,
      "step": 5593
    },
    {
      "epoch": 2.994912985274431,
      "grad_norm": 1.0872451066970825,
      "learning_rate": 3.925767309064954e-07,
      "loss": 0.0681,
      "step": 5594
    },
    {
      "epoch": 2.9954484605087015,
      "grad_norm": 0.7662766575813293,
      "learning_rate": 3.5688793718772304e-07,
      "loss": 0.0399,
      "step": 5595
    },
    {
      "epoch": 2.995983935742972,
      "grad_norm": 0.8293559551239014,
      "learning_rate": 3.2119914346895074e-07,
      "loss": 0.0354,
      "step": 5596
    },
    {
      "epoch": 2.996519410977242,
      "grad_norm": 1.363957166671753,
      "learning_rate": 2.8551034975017845e-07,
      "loss": 0.072,
      "step": 5597
    },
    {
      "epoch": 2.9970548862115125,
      "grad_norm": 1.4903631210327148,
      "learning_rate": 2.4982155603140616e-07,
      "loss": 0.0449,
      "step": 5598
    },
    {
      "epoch": 2.9975903614457833,
      "grad_norm": 1.164300560951233,
      "learning_rate": 2.1413276231263386e-07,
      "loss": 0.0482,
      "step": 5599
    },
    {
      "epoch": 2.9981258366800536,
      "grad_norm": 1.1627970933914185,
      "learning_rate": 1.7844396859386152e-07,
      "loss": 0.0458,
      "step": 5600
    },
    {
      "epoch": 2.998661311914324,
      "grad_norm": 1.5396891832351685,
      "learning_rate": 1.4275517487508923e-07,
      "loss": 0.0452,
      "step": 5601
    },
    {
      "epoch": 2.9991967871485943,
      "grad_norm": 0.7561702728271484,
      "learning_rate": 1.0706638115631693e-07,
      "loss": 0.0486,
      "step": 5602
    },
    {
      "epoch": 2.9997322623828646,
      "grad_norm": 1.6356106996536255,
      "learning_rate": 7.137758743754461e-08,
      "loss": 0.1285,
      "step": 5603
    },
    {
      "epoch": 3.0,
      "grad_norm": 4.638979911804199,
      "learning_rate": 3.5688793718772306e-08,
      "loss": 0.113,
      "step": 5604
    }
  ],
  "logging_steps": 1,
  "max_steps": 5604,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.3458138821536256e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
